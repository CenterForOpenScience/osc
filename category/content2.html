<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>A Pelican Blog &middot; articles in the "content" category</title>
<!--        <link rel="shortcut icon" href="http://centerforopenscience.github.io/osc/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://centerforopenscience.github.io/osc/category/content.html">content</a></li>
                <li><a href="http://centerforopenscience.github.io/osc">Home</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/about.html">About</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/authors.html">Authors</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/policy.html">Policy</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://centerforopenscience.github.io/osc"><img src="http://centerforopenscience.github.io/osc/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">May 20,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/20/clinical-trials-day/" rel="bookmark" title="Permanent Link to &quot;Support Publication of Clinical Trials for International Clinical Trials Day&quot;">Support Publication of Clinical Trials for International Clinical Trials Day</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p>Today is International Clinical Trials Day, held on May 20th in honor of George Lind, the famous Scottish physician who began one of the world's first clinical trials on May 20th, 1747.  This trial discovered that vitamin C deficiency was the cause of scurvy.  While it and the other life-saving trials that have been conducted in the last two hundred and sixty seven years are surely worth celebration, International Clinical Trials Day is also a time to reflect on the problems that plague the clinical trials system.  In particular, the lack of reporting on nearly half of all clinical trials has potentially deadly consequences.</p>
<p>The AllTrials campaign, launched in January 2013, aims to have all past and present clinical trials registered and reported.  From the AllTrials campaign website:</p>
<blockquote>
<p>Doctors and regulators need the results of clinical trials to make informed decisions about treatments.</p>
<p>But companies and researchers can withhold the results of clinical trials even when asked for them. The best available evidence shows that about half of all clinical trials have never been published, and trials with negative results about a treatment are much more likely to be brushed under the carpet.</p>
<p>This is a serious problem for evidence based medicine because we need all the evidence about a treatment to understand its risks and benefits. If you tossed a coin 50 times, but only shared the outcome when it came up heads and you didn’t tell people how many times you had tossed it, you could make it look as if your coin always came up heads. This is very similar to the absurd situation that we permit in medicine, a situation that distorts the evidence and exposes patients to unnecessary risk that the wrong treatment may be prescribed.</p>
<p>It also affects some very expensive drugs. Governments around the world have spent billions on a drug called Tamiflu: the UK alone spent £500 million on this one drug in 2009, which is 5% of the total £10bn NHS drugs budget. But Roche, the drug’s manufacturer, published fewer than half of the clinical trials conducted on it, and continues to withhold important information about these trials from doctors and researchers. So we don’t know if Tamiflu is any better than paracetamol.  (<em>Author's note: in April 2014 <a href="http://www.cochrane.org/features/tamiflu-relenza-how-effective-are-they">a review based on full clinical trial data</a> determined that Tamiflu was almost entirely ineffective.</em>)</p>
<p>Initiatives have been introduced to try to fix this problem, but they have all failed. Since 2008 in the US the FDA has required results of all trials to be posted within a year of completion of the trial. However an audit published in 2012 has shown that 80% of trials failed to comply with this law. Despite this fact, no fines have ever been issued for non-compliance. In any case, since most currently used drugs came on the market before 2008, the trial results that are most important for current medical practice would not have been released even if the FDA’s law was fully enforced.</p>
<p>We believe that this situation cannot go on. The AllTrials initiative is campaigning for the publication of the results (that is, full clinical study reports) from all clinical trials – past, present and future – on all treatments currently being used.</p>
<p>We are calling on governments, regulators and research bodies to implement measure to achieve this.</p>
<p>And we are calling for all universities, ethics committees and medical bodies to enact a change of culture, recognise that underreporting of trials is misconduct and police their own members to ensure compliance.</p>
</blockquote>
<p>You can learn more about the problem of missing clinical trial data in <a href="http://www.alltrials.net/wp-content/uploads/2013/01/Missing-trials-briefing-note.pdf">this brief</a>.  AllTrials also provides slides on this issue to incorporate into talks and presentations as well as a <a href="http://www.alltrials.net/2013/get-involved/">petition you can sign</a>.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/20/clinical-trials-day/#disqus_thread" data-disqus-identifier="2014/05/20/clinical-trials-day/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/20/clinical-trials-day/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 15,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/" rel="bookmark" title="Permanent Link to &quot;How anonymous peer review fails to do its job and damages science.&quot;">How anonymous peer review fails to do its job and damages science.</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jp-de-ruiter-bielefeld-university.html" rel="author">JP de Ruiter, Bielefeld University</a>

		           	

<p>Churchill believed that democracy was the “worst form of government except all those other forms that have been tried from time to time.” Something analogous is often said about anonymous peer review (APR) in science: “it may have its flaws, but it’s the ‘least bad’ of all possible systems.” In this contribution, I present some arguments to the contrary. I believe that APR is threatening scientific progress, and therefore that it urgently needs to be fixed.  </p>
<p>The reason we have a review system in the first place is to uphold basic standards of scientific quality. The two main goals of a review system are to minimize both the number of bad studies that are accepted for publication and the number of good studies that are rejected for publication. Borrowing terminology of signal detection theory, let’s call these false positives and false negatives respectively.  </p>
<p>It is often implicitly assumed that minimizing the number of false positives is the primary goal of APR. However, signal detection theory tells us that reducing the number of false positives inevitably leads to an increase in the rate of false negatives. I want to draw attention here to the fact that the cost of false negatives is both invisible and potentially very high. It is invisible, obviously, because we never get to see the good work that was rejected for the wrong reasons. And the cost is high, because it removes not only good papers from our scientific discourse, but also entire scientists. I personally know a number of very talented and promising young scientists who first sent their work to a journal, fully expecting to be scrutinized, but then receiving reviews that were so personal, rude, scathing, and above all, unfair, that they decided to look for another profession and never looked back. I also know a large number of talented young scientists who are still in the game, but who suffer intensely every time they attempt to publish something and get trashed by anonymous reviewers. I would not be surprised if they also leave academia soon. The inherent conservatism in APR means that people with new, original approaches to old problems run the risk of being shut out, humiliated, and consequently chased away from academia. In the short term, this is to the advantage of the established scientists who do not like their work to be challenged. In the long run, this is obviously very damaging for science. This is especially true of the many journals that will only accept papers that receive unanimously positive reviews. These journals are not facilitating scientific progress, because work with even the faintest hint of controversy is almost automatically rejected.  </p>
<p>With all this in mind, it is somewhat surprising that APR <em>also</em> fails to keep out many obviously bad papers.  </p>
 <a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/#disqus_thread" data-disqus-identifier="2014/05/15/anonymous-peer-review/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/">Posted at 11:30 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May  7,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/07/selecting-for-fraud/" rel="bookmark" title="Permanent Link to &quot;When Science Selects for Fraud&quot;">When Science Selects for Fraud</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This post is in response to <a href="http://osc.centerforopenscience.org/2014/05/02/avoiding-a-witch-hunt/">Jon Grahe's recent article</a> in which he invited readers to propose metaphors that might help us understand why fraud occurs and how to prevent it.</em></p>
<p>Natural selection is the process by which populations change as individual organisms succeed or fail to adapt to their environments.  It is also an apt metaphor for how human cultures form and thrive.  The scientific community, broadly speaking, selects for a number of personality traits, and those traits are more common among scientists than in the general population.  In some cases, this is necessary and beneficial.  In other cases, it is tragic.  </p>
<p>The scientific community selects for curiosity.  Not every scientist is driven by a deep desire to understand the natural world, but so many are.  How boring would endless conferences, lab meetings, and lectures be if one didn’t delight in asking questions and figuring out answers.  It also selects for a certain kind of analytical thinking.  Those who can spot a confound or design a carefully controlled experiment are more likely to succeed.  And it selects for perseverance.  Just ask the researchers who work late into the night running gels, observing mice, or analyzing data.  </p>
<p>The scientific community, like the broader culture of which it is a part, sometimes selects unjustly.  It selects for the well-off: those who can afford the kind of schools where a love of science is cultivated rather than ignored or squashed, those who can volunteer in labs because they don’t need to work to support themselves and others, those who can pay $30 to read a journal article.  It selects for white men: those who don’t have to face conscious and unconscious discrimination, cultural stereotyping, and microaggressions.  </p>
<p>Of particular relevance right now is the way the scientific community selects for fraud.  If asked, most scientists would say that the ideal scientist is honest, open-minded, and able to accept being wrong.  But we do not directly reward these attributes.  Instead, success - publication of papers, grant funding, academic positions and tenure, the approbation of our peers - is too often based on a specific kind of result.  We reward those who can produce novel and positive results.  We don’t reward based on how they produce them.  </p>
<p>This does give an advantage to those with good scientific intuitions, which is a reasonable thing to select for.  It also gives an advantage to risk-takers, those willing to risk their careers on being right.  The risk averse?  They have two options: to drop out of scientific research, as I did, or to commit fraud in order to ensure positive results, as Diederik Stapel, Marc Hauser and Jens Foster did.  Among the risk-averse, those who are unwilling to do shoddy or unethical science are selected against.  Those who are willing are selected for, and often reach the tops of their fields.  </p>
<p>One of the more famous examples of natural selection is the peppered moth of England.  Before the Industrial Revolution, these moths were lightly colored, allowing them to blend in with the light gray bark of the average tree.  During the Industrial Revolution, extreme pollution painted the trees of England black with soot.  To adapt, peppered moths evolved dark, soot-colored wings.  </p>
<p>We can censure the individuals who commit fraud, but this is like punishing the peppered moth for its dirty wings.  As long as success in the scientific community is measured by results and not process, we will continue to select for those willing to violate process in order to ensure results.  Our species, the scientists, need to change our environment if we want to evolve past fraud.  </p>
<p><img src="images/moth.jpg" alt="Photo of Jon Grahe" align="center" width="600px" />
<a href="https://www.flickr.com/photos/dhobern/7522055588/in/photolist-bwwkzi-a2zqz8-a2CkHA-4RecUF-6uwtBr-a9tjNF-dyGUvV-6Mhm2U-2tw4Qf-cTZ46w-fzqdc-fzqdj-9QBTbx-f5YrkT-csGu1y-csGv4A-bV36Wk-c4U7JS-fpNHoh">Biston betularia by Donald Hobern</a>, CC BY 2.0</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/07/selecting-for-fraud/#disqus_thread" data-disqus-identifier="2014/05/07/selecting-for-fraud/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/07/selecting-for-fraud/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May  2,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/" rel="bookmark" title="Permanent Link to &quot;Avoiding a Witch Hunt: What is the Next Phase of our Scientific Inquisition?&quot;">Avoiding a Witch Hunt: What is the Next Phase of our Scientific Inquisition?</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jon-grahe.html" rel="author">Jon Grahe</a>

		           	

<p>Earlier this week, I learned about another case of fraud in psychological science (<a href="http://retractionwatch.com/2014/04/29/new-dutch-psychology-scandal-inquiry-cites-data-manipulation-calls-for-retraction/">Retraction Watch, 4.29.2014</a>). The conclusions from the evidence in the case against him after an extended investigation are hard to ignore. The probability that the findings could have occurred by chance are so minute that it is hard to believe that they didn’t result from falsified data. In an email to the scientific community (<a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/">Retraction Watch, 4.30.2014</a>), the target of this investigation strongly asserted that he never faked any data, while assuring us that the coauthor target never worked on the data, it was all his. Some comments from the Retraction Watch post use the term “witch hunt.” It was the first term I used in response as well, suggesting caution before judgment. A colleague pointed out that the difference was that there were no witches, and that there are clearly dishonest scientists. I have no choice but to agree; I think a better analogy is that of the Inquisition. We are entering the era of the Scientific Inquisition. A body of experts (LOWI in this case) will use a battery of sophisticated tools to examine the likelihood that the findings’ irregularities occurred by chance. In this case it is hard to believe his denial, but thankfully I am not a judge in the Scientific Inquisition.  </p>
 <a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/#disqus_thread" data-disqus-identifier="2014/05/02/avoiding-a-witch-hunt/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/">Posted at  5:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr 23,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/" rel="bookmark" title="Permanent Link to &quot;Memo From the Office of Open Science&quot;">Memo From the Office of Open Science</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

		           	

<p>Dear Professor Lucky,</p>
<p>Congratulations on your new position as assistant professor at Utopia University. We look forward to your joining our community and are eager to aid you in your transition from Antiquated Academy. It’s our understanding that Antiquated Academy does not have an Office of Open Science, so you may be unfamiliar with who we are and what we do.</p>
<p>The Office of Open Science was created to provide faculty, staff and students with the technical, educational, social and logistical support they need to do their research openly. We recognize that the fast pace of research and the demands placed on scientists to be productive make it difficult to prioritize open science. We collaborate with researchers at all levels to make it easier to do this work.</p>
<p>Listed below are some of the services we offer.</p>
 <a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/#disqus_thread" data-disqus-identifier="2014/04/23/memo-from-the-office-of-open-science/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/">Posted at  2:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr 16,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/04/16/expectations-2/" rel="bookmark" title="Permanent Link to &quot;Expectations of replicability and variability in priming effects, Part II: When should we expect replication, how does this relate to variability, and what do we do when we fail to replicate?&quot;">Expectations of replicability and variability in priming effects, Part II: When should we expect replication, how does this relate to variability, and what do we do when we fail to replicate?</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/joseph-cesario-kai-jonas.html" rel="author">Joseph Cesario, Kai Jonas</a>

		           	

<p><em>Continued from <a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/">Part 1</a>.</em></p>
<p>Now that some initial points and clarifications have been offered, we can move to the meat of the argument. Direct replication is essential to science. What does it mean to replicate an effect? All effects require a set of contingencies to be in place. To replicate an effect is to set up those same contingencies that were present in the initial investigation and observe the same effect, whereas to fail to replicate an effect is to set up those same contingencies and fail to observe the same effect. Putting aside what we mean by "same effect" (i.e., directional consistency versus magnitude), we don't see any way in which people can reasonably disagree on this point. This is a general point true of all domains of scientific inquiry.</p>
<p>The real question becomes, <em>how can we know what contingencies produced the effect in the original investigation</em>? Or more specifically, <em>how can we separate the important contingencies from the unimportant contingencies</em>? There are innumerable contingencies present in a scientific investigation that are totally irrelevant to obtaining the effect: the brand of the light bulb in the room, the sock color of the experimenter, whether the participant got a haircut last Friday morning or Friday afternoon. Common sense can provide some guidance, but in the end <em>the theory used to explain the effect</em> specifies the necessary contingencies and, by omission, the unnecessary contingencies. Therefore, if one is operating under the wrong theory, one might think some contingencies are important when really they are unimportant, and more interestingly, one might <em>miss</em> some necessary contingencies because the theory did not mention them as being important.</p>
<p>Before providing an example, it might be useful to note that, as far as we can tell, no one has offered any criticism of the logic outlined above. Many sarcastic comments have been made along the lines of, "apparently we can never learn anything because of all these mysterious moderators." And it is true that the argument can be misused to defend poor research practices. But at core, there is no criticism about the basic point that contingencies are necessary for all effects and a theory establishes those contingencies.</p>
 <a href="http://centerforopenscience.github.io/osc/2014/04/16/expectations-2/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/04/16/expectations-2/#disqus_thread" data-disqus-identifier="2014/04/16/expectations-2/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/04/16/expectations-2/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr  9,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/04/09/expectations-1/" rel="bookmark" title="Permanent Link to &quot;Expectations of replicability and variability in priming effects, Part I: Setting the scope and some basic definitions&quot;">Expectations of replicability and variability in priming effects, Part I: Setting the scope and some basic definitions</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/joseph-cesario-kai-jonas.html" rel="author">Joseph Cesario, Kai Jonas</a>

		           	

<p>We are probably thought of as "defenders" of priming effects and along with that comes the expectation that we will provide some convincing argument for why priming effects are real. We will do no such thing. The kinds of priming effects under consideration (priming of social categories which result in behavioral priming effects) is a field with relatively few direct replications<sup>1</sup> and we therefore lack good estimates of the effect size of any specific effect. Judgments about the nature of such effects can only be made after thorough, systematic research, which will take some years still (assuming priming researchers change their research practices). And of course, we must be open to the possibility that further data will show any given effect to be small or non-existent.</p>
<p>One really important thing we could do to advance the field to that future ideal state is to <strong>stop calling everything priming</strong>. It appears now, especially with the introduction of the awful term "social priming," that any manipulation used by a social cognition researcher can be called priming and, if such a manipulation fails to have an effect, it is cheerfully linked to this nebulous, poorly-defined class of research called "social priming." <strong>There is no such thing as "social priming."</strong> There is priming of social categories (elderly, professor) and priming of motivational terms (achievement) and priming of objects (flags, money) and so on. And there are priming <em>effects</em> at the level of cognition (increased activation of concepts) or affect (valence, arousal, or emotions) or behavior (walking, trivial pursuit performance) or physiology, and some of these priming <em>effects</em> will be automatic and some not (and even then recognizing the different varieties of automaticity; Bargh, 1989). These are all different things and need to be treated separately.</p>
 <a href="http://centerforopenscience.github.io/osc/2014/04/09/expectations-1/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/04/09/expectations-1/#disqus_thread" data-disqus-identifier="2014/04/09/expectations-1/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/04/09/expectations-1/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr  2,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/04/02/deathly-hallows/" rel="bookmark" title="Permanent Link to &quot;The Deathly Hallows of Psychological Science&quot;">The Deathly Hallows of Psychological Science</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/brent-w-roberts.html" rel="author">Brent W. Roberts</a>

		           	

<p><em>This piece was <a href="http://pigee.wordpress.com/2014/03/10/the-deathly-hallows-of-psychological-science/">originally posted</a> to the Personality Interest Group and Espresso (PIG-E) web blog at the University of Illinois.</em></p>
<p>As of late, psychological science has arguably done more to address the ongoing <a href="http://wp.me/p1b8ZP-3l">believability crisis</a> than most other areas of science.  Many notable efforts have been put forward to improve our methods.  From the Open Science Framework (<a href="https://osf.io/">OSF</a>), to changes in <a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/JRPEditorial2013.pdf">journal reporting practices</a>, to <a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Cumming%20-%20The%20New%20Statistics%20Why%20and%20How%20-%20PS2014.pdf">new statistics</a>, psychologists are doing more than any other science to rectify practices that allow far too many unbelievable findings to populate our journal pages.</p>
<p>The efforts in psychology to improve the believability of our science can be boiled down to some relatively simple changes.  We need to replace/supplement the typical reporting practices and statistical approaches by:</p>
<ol>
<li>Providing more information with each paper so others can double-check our work, such as the study materials, hypotheses, data, and syntax (through the OSF or journal reporting practices).</li>
<li>Designing our studies so they have adequate power or precision to evaluate the theories we are purporting to test (i.e., use larger sample sizes).</li>
<li>Providing more information about effect sizes in each report, such as what the effect sizes are for each analysis and their respective confidence intervals.</li>
<li>Valuing direct replication.</li>
</ol>
<p>It seems pretty simple.  Actually, the proposed changes are simple, even mundane.</p>
<p>What has been most surprising is the consistent push back and protests against these seemingly innocuous recommendations.  When confronted with these recommendations it seems many psychological researchers balk. Despite calls for transparency, most researchers avoid platforms like the OSF.  A striking number of individuals argue against and are quite disdainful of reporting effect sizes. Direct replications are disparaged. In response to the various recommendations outlined above, prototypical protests are:</p>
<ol>
<li>Effect sizes are unimportant because we are “testing theory” and effect sizes are only for “applied research.”</li>
<li>Reporting effect sizes is nonsensical because our research is on constructs and ideas that have no natural metric, so that documenting effect sizes is meaningless.</li>
<li>Having highly powered studies is cheating because it allows you to lay claim to effects that are so small as to be uninteresting.</li>
<li>Direct replications are uninteresting and uninformative.</li>
<li>Conceptual replications are to be preferred because we are testing theories, not confirming techniques.</li>
</ol>
<p>While these protestations seem reasonable, the passion with which they are provided is disproportionate to the changes being recommended.  After all, if you’ve run a t-test, it is little trouble to estimate an effect size too. Furthermore, running a direct replication is hardly a serious burden, especially when the typical study only examines 50 to 60 odd subjects in a 2×2 design. Writing entire <a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Stroebe%20%26%20Strack%202014.pdf">treatises</a> arguing against direct replication when direct replication is so easy to do falls into the category of “the lady doth protest too much, methinks.” Maybe it is a reflection of my repressed Freudian predilections, but it is hard not to take a Depth Psychology stance on these protests.  If smart people balk at seemingly benign changes, then there must be something psychologically big lurking behind those protests.  What might that big thing be?  I believe the reason for the passion behind the protests lies in the fact that, though mundane, the changes that are being recommended to improve the believability of psychological science undermine the incentive structure on which the field is built.</p>
<p>I think this confrontation needs to be more closely examined because we need to consider the challenges and consequences of deconstructing our incentive system and status structure.  This, then begs the question, what is our incentive system and just what are we proposing to do to it?  For this, I believe a good analogy is the dilemma faced by Harry Potter in the last book of the eponymously titled book series.</p>
 <a href="http://centerforopenscience.github.io/osc/2014/04/02/deathly-hallows/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/04/02/deathly-hallows/#disqus_thread" data-disqus-identifier="2014/04/02/deathly-hallows/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/04/02/deathly-hallows/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar 26,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/03/26/behavioral-priming/" rel="bookmark" title="Permanent Link to &quot;Behavioral Priming: Time to Nut Up or Shut Up&quot;">Behavioral Priming: Time to Nut Up or Shut Up</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/ej-wagenmakers.html" rel="author">EJ Wagenmakers</a>

		           	

<p>In the epic movie "Zombieland", one of the main protagonists –Tallahassee, played by Woody Harrelson– is about to enter a zombie-infested supermarket in search of Twinkies. Armed with a banjo, a baseball bat, and a pair of hedge shears, he tells his companion it is "time to nut up or shut up". In other words, the pursuit of happiness sometimes requires that you expose yourself to grave danger. Tallahasee could have walked away from that supermarket and its zombie occupants, but then he would never have discovered whether or not it contained the Twinkies he so desired.   </p>
<p>At its not-so-serious core, Zombieland is about leaving one's comfort zone and facing up to your fears. This I believe is exactly the challenge that confronts the proponents of behavioral priming today. To recap, the phenomenon of behavioral priming refers to unconscious, indirect influences of prior experiences on actual behavior. For instance, presenting people with words associated with old age ("Florida", "grey", etc.) primes the elderly stereotype and supposedly makes people walk more slowly; in the same vein, having people list the attributes of a typical professor ("confused", "nerdy", etc.) primes the concept of intelligence and supposedly makes people answer more Trivia questions correctly.   </p>
<p>In recent years, the phenomenon of behavioral priming has been scrutinized with increasing intensity. Crucial to the debate is that many (if not all) of the behavioral priming effects appear to vanish like thin air in the hands of other researchers. Many of these researchers –from now on, the skeptics– have reached the conclusion that behavioral priming effects are elusive, brought about mostly by confirmation bias, the use of questionable research practices, and selective reporting.   </p>
 <a href="http://centerforopenscience.github.io/osc/2014/03/26/behavioral-priming/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/03/26/behavioral-priming/#disqus_thread" data-disqus-identifier="2014/03/26/behavioral-priming/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/03/26/behavioral-priming/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar 19,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/03/19/if-you-have-data/" rel="bookmark" title="Permanent Link to &quot;If You Have Data, Use It When Theorizing&quot;">If You Have Data, Use It When Theorizing</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/daniel-lakens.html" rel="author">Daniel Lakens</a>

		           	

<p>There is a reason data collection is part of the empirical cycle. If you have a good theory that allows for what Platt (1964) called ‘strong inferences’, then statistical inferences from empirical data can be used to test theoretical predictions. In psychology, as in most sciences, this testing is not done in a Popperian fashion (where we consider a theory falsified if the data does not support our prediction), but we test ideas in Lakatosian lines of research, which can either be progressive or degenerative (e.g., Meehl, 1990). In (meta-scientific) <em>theory</em>, we judge (scientific) theories based on whether they have something going for them.  </p>
<p>In scientific <em>practice</em>, this means we need to evaluate research lines. One really flawed way to do this is to use ‘vote-counting’ procedures, where you examine the literature, and say: "Look at all these significant findings! And there are almost no non-significant findings! This theory is the best!” Read Borenstein, Hedges, Higgins, &amp; Rothstein (2006) who explain “Why Vote-Counting Is Wrong” (p. 252 – but read the rest of the book while you’re at it).  </p>
 <a href="http://centerforopenscience.github.io/osc/2014/03/19/if-you-have-data/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/03/19/if-you-have-data/#disqus_thread" data-disqus-identifier="2014/03/19/if-you-have-data/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/03/19/if-you-have-data/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://centerforopenscience.github.io/osc/category/content.html" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="http://centerforopenscience.github.io/osc/category/content3.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 2 of 5</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>