<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>A Pelican Blog &middot; articles in the "content" category</title>
<!--        <link rel="shortcut icon" href="http://osc.centerforopenscience.org/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://osc.centerforopenscience.org/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <link rel="stylesheet" href="http://osc.centerforopenscience.org/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://osc.centerforopenscience.org/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://osc.centerforopenscience.org/category/content.html">content</a></li>
                <li><a href="http://osc.centerforopenscience.org">Home</a></li>
<li><a href="http://osc.centerforopenscience.org/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://osc.centerforopenscience.org"><img src="http://osc.centerforopenscience.org/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Mar 12,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/" rel="bookmark" title="Permanent Link to &quot;In the Previous Episodes of the Tale of Social Priming and Reproducibility&quot;">In the Previous Episodes of the Tale of Social Priming and Reproducibility</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/ase-innes-ker.html" rel="author">Åse Innes-Ker</a>

   			   <p>We have lined up a nice set of posts responding to the recent special section in PoPS on social priming and replication/reproducibility, which we will publish in the coming weeks. It has proven easier to find critics of social priming than to find defenders of the phenomenon, and if there are primers out there who want to chime in they are most welcome and may contact us at oscblog@googlegroups.com.</p>
<p>The special section in PoPS was immediately prompted by this wonderful <a href="http://pps.sagepub.com/content/7/6.toc">November 2012 issue</a> from PoPS on replicability in psychology  (open access!), but the Problems with Priming started prior to this. For those of you who didn’t seat yourself in front of the screen with a tub of well-buttered pop-corn every time behavioral priming made it outside the trade journals, I’ll provide some back-story, and links to posts and articles that frames the current response.</p>
<p>The mitochondrial Eve of behavioral priming is Bargh’s <a href="http://psycnet.apa.org/journals/psp/71/2/230/">Elderly Prime</a><sup>1</sup>. The unsuspecting participants were given scrambled sentences, and were asked to create proper sentences out of four of the five words in each. Some of the sentences included words like Bingo or Flordia – words that may have made you think of the elderly, if you were a student in New York in the mid nineties. Then, they measured the speed with which the participant walked down the corridor to return their work, and, surprising to many, those that unscrambled sentences that included “Bingo” and “Florida” walked slower than those that did not. Conclusion: the construct of “elderly” had been primed, causing participants to adjust their behavior (slower walk) accordingly. You can check out sample sentences in <a href="http://marginalrevolution.com/marginalrevolution/2012/03/walking-fast-and-slow.html">this Marginal Revolution post</a> – yes, priming made it to this high-traffic economy blog.</p>
<p>This paper has been cited 2571 times, so far (according to Google Scholar). It even appears in Kahneman’s <a href="http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555">Thinking, Fast and Slow</a>, and has been high on the wish-list for replication on Pashler’s <a href="http://www.psychfiledrawer.org/view_article_list.php">PsychFile Drawer</a>. (No longer in the top 20, though).</p>
<p>Finally, in January 2012, Doyen, Klein, Pichon &amp; Cleeremans (a Belgian group) <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029081.">published a replication attempt</a> in PLOSone where they suggest the effect was due to demand.  Ed Yong did <a href="http://blogs.discovermagazine.com/notrocketscience/2012/01/18/primed-by-expectations-why-a-classic-psychology-experiment-isnt-what-it-seemed/#.UuimfLRwG70">this nice write-up</a> of the research.</p>
<p>Bargh was not amused, and wrote a scathing rebuttal on his blog in the Psychology Today domain.  He took it down after some time (for good reason – I think it can be found, but I won’t look for it.). <a href="http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/#.UuimZ7RwG70">Ed commented on this too</a>.</p>
<p>A number of good posts from blogging psychological scientists also commented on the story. A sampling are <a href="http://hardsci.wordpress.com/2012/03/12/some-reflections-on-the-bargh-doyen-elderly-walking-priming-brouhaha/">Sanjay Srivastava</a> on his blog Hardest Science, <a href="http://neurochambers.blogspot.se/2012/03/you-cant-replicate-concept.html">Chris Chambers</a> on NeuroChambers, and <a href="http://cedarsdigest.wordpress.com/2012/03/21/put-your-head-up-to-the-meta-a-peer-reviews-post-post-publication-peer-review-a-bargh-full-of-links/">Cedar Riener</a> on his Cedarsdigest. </p>
<p>The British Psychological Society published <a href="http://www.thepsychologist.org.uk/blog/11/blogpost.cfm?threadid=2196&amp;catid=48">a notice about it</a> in The Psychologist which links to additional commentary.  In May, Ed Yong had <a href="http://www.nature.com/news/replication-studies-bad-copy-1.10634">an article in Nature</a> discussing the status of non-replication in psychology in general, but where he also brings up the Doyen/Bargh controversy. On January 13, the Chronicle published <a href="http://chronicle.com/article/Power-of-Suggestion/136907/">a summary of what had happened</a>.</p>
<p>But, prior to that, Daniel Kahneman made a call for psychologists to clean up their act as far as behavioral priming goes. Ed Yong (again) published two pieces about it. One in <a href="http://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535">Nature</a> and one on <a href="http://blogs.discovermagazine.com/notrocketscience/2012/10/04/daniel-kahneman-daisy-chain-replications-priming-psychology/#.UuTRArRwHIU">his blog</a>.</p>
<p>The controversies surrounding priming continued in the spring of 2013. This time it was David Shanks who, as a hobby (<a href="http://www.ucl.ac.uk/psychlangsci/">from his video</a> - scroll down below the fold) had taken to attempting to replicate priming of intelligence, work originally done by <a href="http://psycnet.apa.org/index.cfm?fa=search.displayrecord&amp;uid=1998-01060-003">Dijksterhuis and van Knippenberg</a> in 1998. He had his students perform a series of replications, all of which showed no effect, and was then collected in <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0056515">this PLOSone paper</a>. </p>
<p>Dijksterhuis retorted in <a href="http://www.plosone.org/annotation/listThread.action?root=64751">the comment section</a><sup>2</sup>. Rolf Zwaan <a href="http://rolfzwaan.blogspot.nl/2013/04/social-priming-in-theory.html">blogged about it</a>. Then, Nature posted <a href="http://www.nature.com/news/disputed-results-a-fresh-blow-for-social-psychology-1.12902#/correction1">a breathless article</a> suggesting that this was a fresh blow for us who are Social Psychologists.</p>
<p>Now, most of us who do science thought instead that this was science working just like it ought to be working, and blogged up a storm about it – with some of the posts (including one of mine) linked in Ed Yong’s <a href="http://phenomena.nationalgeographic.com/2013/05/04/ive-got-your-missing-links-right-here-4-may-2013/">“Missing links” feature</a>. The links are all in the fourth paragraph, above the scroll, and includes additional links to discussions on replicability, and the damage done by a certain Dutch fraudster.</p>
<p>So here you are, ready for the next set of installments.</p>
<p><sup>1</sup> Ancestral to this is <a href="http://psycnet.apa.org/index.cfm?fa=search.displayRecord&amp;uid=1981-01290-001">Srull &amp; Wyer’s</a> (1979) story of Donald, who is either hostile or kind, depending on which set of sentences the participant unscrambled in that earlier experiment that had nothing to do with judging Donald.</p>
<p><sup>2</sup> A nice feature.  No waiting years for the retorts to be published in the dead tree variant we all get as PDF’s anyway.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/#disqus_thread" data-disqus-identifier="2014/03/12/previous-episodes/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar  6,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/03/06/confidence intervals/" rel="bookmark" title="Permanent Link to &quot;Confidence Intervals for Effect Sizes from Noncentral Distributions&quot;">Confidence Intervals for Effect Sizes from Noncentral Distributions</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/russ-clay.html" rel="author">Russ Clay</a>

   			   <p><sup>(Thanks to Shauna Gordon-McKeon, Fred Hasselman, Daniël Lakens, Sean Mackinnon, and Sheila Miguez for their contributions and feedback to this post.)</sup></p>
<p><em>I recently took on the task of calculating a confidence interval around an effect size stemming from a noncentral statistical distribution (the F-distribution to be precise).  This was new to me, and as I am of the view that such statistical procedures would add value to the work being done in the social and behavioral sciences, but that they are not common in practice at the present time, potentially due to lack of awareness, I wanted to pass along some of the things that I found.</em><br />
In an effort to estimate the replicability of psychological science, an important first step is to determine the criteria for declaring a given replication attempt as successful.  Lacking clear consensus around this criteria, the OpenScience group determined that rather than settling on a single set of criteria by which the replicability of psychological research would be assessed, multiple methods would be employed, all which provide a measure of valuable insight regarding the reproducibility of published findings in psychology (OpenScience Collaboration, 2012).  One such method is to examine the confidence interval around the original target effect and to see if this confidence interval overlaps with the confidence interval from the replication effect.  However, estimating the confidence interval around many effects in social science research requires the use of  non-central probability distributions, and most mainstream statistical packages (e.g. SAS, SPSS) do not provide off the shelf capabilities for deriving confidence intervals from these distributions (Kelley, 2007).  </p>
<p>Most of us probably picture common statistical distributions such as the t-distribution, the F-distribution, and the χ2 distribution as being two dimensional, with the x-axis representing the value of the test statistic and the area under the curve representing the likelihood of observing such a value in a sample population.  When first learning to conduct these statistical tests, such visual representations likely provided a helpful way to convey the concept that more extreme values of the test statistic were less likely.  In the realm of null hypothesis statistical testing (NHST), this provides a tool for visualizing how extreme the test statistic would need to be before we would be willing to reject a null hypothesis.  However, it is important to remember that these distributions vary along a third parameter as well: the noncentrality parameter.  The distribution that we use to determine the cut-off points for rejecting a null hypothesis is a special, central case of the distribution when the noncentrality parameter is zero.  This special-case distribution gives the probabilities of test statistic values when the null hypothesis is true (i.e., when the population effect is zero).  As the noncentrality parameter changes (i.e., when we assume that an effect does exist), the shape of the distribution which defines the probabilities of obtaining various values of the parameter in our statistical tests changes as well.  The following figure (copied from the Wikipedia page for the noncentral t-distribution) might help provide a sense of how the shape of the t-distribution changes as the noncentrality parameter varies.  </p>
<p><img src="/images/CIs.png" alt="non-central T distribution" align="center" style="padding-right: 20px;" width="360px" /><br />
<sup>Figure by <a href="http://en.wikipedia.org/wiki/File:Nc_student_t_pdf.svg">Skbkekas</a>, licensed <a href="http://creativecommons.org/licenses/by/3.0/deed.en">CC BY 3.0.</a></sup></p>
<p>The first two plots (orange and purple) illustrate the different shapes of the distribution under the assumption that the true population parameter (the difference in means) is zero.  The value of v indicates the degrees of freedom used to determine the probabilities under the curve.  The difference between these first two curves stems from the fact that the purple curve has more degrees of freedom (a larger sample), and thus there will be a higher probability of observing values near the mean.  These distributions are central (and symmetrical), and as such, values of x that are equally higher or lower than the mean are equally probable.  The second two plots (blue and green) illustrate the shapes of the distribution under the assumption that the true population parameter is two.  Notice that both of these curves are positively skewed, and that this skewness is particularly pronounced in the blue curve as it is based on fewer degrees of freedom (smaller sample size).  The important thing to note is that for these plots, values of x that are equally higher or lower than the mean are NOT equally probable.  Observing a value of x = 4 under the assumption that the true value of x is two is considerably more probable than observing a value of x = 0.  Because of this, a confidence interval around an effect that is anything other than zero will be asymmetrical and will require a bit of work to calculate.  </p>
<p>Because the shape (and thus the degree of symmetry) of many statistical distributions depends on the size of the effect that is present in the population, we need a noncentrality parameter to aid in determining the shape of the distribution and the boundaries of any confidence interval of the population effect.  As mentioned previously, these complexities do not arise as often as we might expect in everyday research because when we use these distributions in the context of null-hypothesis statistical testing (NHST), we can assume a special, ‘centralized’ case of the distributions that occurs when the true population effect of interest is zero (the typical null hypothesis).  However, confidence intervals can provide different information than what can be obtained through NHST.  When testing a null hypothesis, what we glean from our statistics is the probability of obtaining the effect observed in our sample if the true population effect is zero.  The p-value represents this probability, and is derived from a probability curve with a noncentrality parameter of zero.  As mentioned above, these special cases of statistical distributions such as the t, F, and χ2 are ‘central’ distributions.  On the other hand, when we wish to construct a confidence interval of a population effect, we are no longer in the NHST world, and we no longer operate under the assumption of ‘no effect’.  In fact, when we build a confidence interval, we are not necessarily making assumptions at all about the existence or non-existence of an effect.  Instead, when we build a confidence interval, we want a range of values that is likely to contain the true population effect with some degree of confidence.  To be crystal clear, when we construct a 95% confidence interval around a test statistic, what we are saying is that if we repeatedly tested random samples of the same size from the target population under identical conditions, the true population parameter will be bounded by the 95% confidence interval derived from these samples 95% of the time.  </p>
<p>From a practical standpoint, a confidence interval can tell us everything that NHST can, and then some.  If the 95% confidence interval of a given effect contains the value of zero, then there is a good chance that there is a negligible effect in the relationship you are testing.  In this case, as a researcher, the conclusion that you would reach is conceptually similar to declaring that you are not willing to reject a null hypothesis of zero effect on the grounds that there is greater than a 5% chance that the effect is actually zero.  However, a confidence interval allows the researcher to say a bit more about the potential size of a population effect as well as the degree of variability that exists in it’s estimate, whereas NHST only permits the researcher to state, with a specified level of confidence, the likelihood that an effect exists at all.  </p>
<p>Why, then, is NHST the overwhelming choice of statisticians in the social sciences?  The likely answer has to do with the idea of non-centrality stated above.  When we build a confidence interval around an effect size, we generally do not build the confidence interval around an effect of zero.  Instead, we build the confidence interval around the effect that we find in our sample.  As such, we are unable to build the confidence interval using the symmetrical, special case instances of many of our statistical distributions.  We have to build it using an asymmetrical distribution that has a shape (a degree of non-centrality) that depends on the effect that we found in our sample.  This gets messy, complicated, and requires a lot of computation.  As such, the calculation of these confidence intervals was not practical until it became commonplace for researchers to have at their disposal the computational power available in modern computing systems.  However, research in the social sciences has been around much longer than your everyday, affordable, quad-core laptop, and because building confidence intervals around effects from non-central distributions was impractical for much of the history of the social sciences, these statistical techniques were not often taught, and their lack of use is likely to be an artifact of institutional history (Steiger &amp; Fouladi, 1997).
All of this to say that in today’s world, researchers generally have more than enough computational power at their disposal to easily and efficiently construct a confidence interval around an effect from a non-central distribution.  The barriers to these statistical techniques have been largely removed, and as the value of the information obtained from a confidence interval exceeds the value of the information that can be obtained from NHST, it is useful to spread the word about resources that can help in the computation of confidence intervals around common effect size metrics in the social and behavioral sciences.  </p>
<p>One resource that I found to be particularly useful is the MBESS (Methods for the Behavioral, Educational, and Social Sciences) package for the <a href="http://www.r-project.org/">R statistical software platform</a>.  For those unfamiliar with R, it is a free, open-source statistical software package which can be run on Unix, Mac, and Windows platforms.  The standard R software contains basic statistics functionality, but also provides the capability for contributors to develop their own functionality (typically referred to as ‘packages’) which can be made available to the larger user community for download.   MBESS is one such package which provides ninety-seven different functions for statistical procedures that are readily applicable to statistical analysis techniques in the behavioral, educational, and social sciences.  Twenty-five of these functions involve the calculation of confidence intervals or confidence limits, mostly for statistics stemming from noncentral distributions.  </p>
<p>For example, I used the ci.pvaf (confidence interval of the proportion of variance accounted for) function from the MBESS package to obtain a 95% confidence interval around an η2 effect of 0.11 from a one-way between groups analysis of variance.  In order to do this, I only needed to supply the function with several relevant arguments:  </p>
<blockquote>
<p><strong>F-value:</strong>  This is the F-value from a fixed-effects ANOVA<br />
<strong>df:</strong>  The numerator and denominator degrees of freedom from the analysis<br />
<strong>N:</strong>  The sample size<br />
<strong>Confidence Level:</strong>  The confidence level coverage that you desire (i.e. 95%)  </p>
</blockquote>
<p>No more information is required.  Based on this, the function can calculate the desired confidence interval around the effect. Here is a copy of the code that I entered and what was produced (with comments in italics to explain what is going on in each step):  </p>
<blockquote>
<p><strong>library(MBESS);</strong>  </p>
<p><em>once you have installed the MBESS package, this command makes it available for your current session of R</em>  </p>
<p><strong>ci.pvaf(F.value=4.97, df.1=2, df.2=81, N=84, conf.level=.95)</strong>  </p>
<p><em>this uses the ci.pvaf function in the MBESS package to calculate the confidence interval.  I have given # the function an F-value (F.value) of 4.97, with 2 degrees of freedom between groups (df.1), and 81 # degrees of freedom within groups (df.2), a sample size (N) of 84, and have asked it to produce a 95% confidence interval (conf.level).  Executing the above command produces the following output:</em>  </p>
<p><strong>$Lower.Limit.Proportion.of.Variance.Accounted.for</strong><br />
<strong>[1] 0.007611619</strong>  </p>
<p><strong>$Probability.Less.Lower.Limit</strong><br />
<strong>[1] 0.025</strong>  </p>
<p><strong>$Upper.Limit.Proportion.of.Variance.Accounted.for</strong><br />
<strong>[1] 0.2320935</strong>  </p>
<p><strong>$Probability.Greater.Upper.Limit</strong><br />
<strong>[1] 0.025</strong>  </p>
<p><strong>$Actual.Coverage</strong><br />
<strong>[1] 0.95</strong>  </p>
<p><em>Thus, the 95% confidence interval around my η2 effect is <strong>[0.01 - 0.23]</strong>.</em>  </p>
</blockquote>
<p>Similar functions are available in the MBESS package for calculating confidence intervals around a contrast in a fixed-effects ANOVA, multiple correlation coefficient, squared multiple correlation coefficient, regression coefficient, reliability coefficient, RMSEA, standardized mean difference, signal-to-noise ratio, and χ2 parameters, among others.  </p>
<h5>Additional Resources</h5>
<ul>
<li>
<p>Fred Hasselman has created <a href="http://osc.centerforopenscience.org/static/CIs_in_r.html">a brief tutorial</a> for computing effect size confidence intervals using R.  </p>
</li>
<li>
<p>For those more familiar with conducting statistics in an SPSS environment, Dr. Karl Wuensch at East Carolina University provides links to several SPSS programs on his Web Page.  <a href="http://core.ecu.edu/psyc/wuenschk/SPSS/SPSS-Programs.htm">This program</a> is for calculating confidence intervals for a standardized mean difference (Cohen’s d).  </p>
</li>
<li>
<p>In addition, I came across several publications that I found useful in providing background information regarding non-central distributions (a few of which are cited above).  I’m sure there are more, but I found these to be a good place to start:</p>
</li>
</ul>
<blockquote>
<p>Cumming, G. (2006).  <em>How the noncentral t distribution got its hump.</em>  Paper presented at the seventh International Conference on Teaching Statistics, Salvador, Bahia, Brazil.  </p>
<p>Cumming, G. (2014).  <em>The new statistics: Why and how.</em>  Psychological Science, 25, 7-29.  DOI: 10.1177/0956797613504966  </p>
<p>Kelley, K. (2007).  <em>Confidence intervals for standardized effect sizes: Theory, application, and implementation.</em>  Journal of Statistical Software, 20, 1-24.  </p>
<p>Smithson, M. (2001). <em>Correct confidence intervals for various regression effect sizes and parameters: The importance of noncentral distributions in computing intervals.</em> Educational And Psychological Measurement, 61(4), 605-632. doi:10.1177/00131640121971392  </p>
<p>Steiger, J. H., &amp; Fouladi, R. T. (1997).  <em>Noncentrality interval estimation and the evaluation of statistical models.</em>  In L. Harlow, S. &gt; Mulaik, &amp; J. Steiger (Eds.), What if there were no significance tests? (pp. 221-256).  Mahwah, NJ: Erlbaum.  </p>
</blockquote>
<ul>
<li>There is also <a href="http://cran.r-project.org/web/packages/MBESS/MBESS.pdf">thorough documentation of the MBESS package itself</a>.</li>
</ul>
<p>Hopefully others find this information as useful as I did!</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/03/06/confidence intervals/#disqus_thread" data-disqus-identifier="2014/03/06/confidence intervals/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/03/06/confidence intervals/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Feb 27,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/" rel="bookmark" title="Permanent Link to &quot;Data trawling and bycatch – using it well&quot;">Data trawling and bycatch – using it well</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/ruben-arslan.html" rel="author">Ruben Arslan</a>

		           	

<p>Pre-registration is starting to outgrow its old home, <a href="http://neuroskeptic.blogspot.co.uk/2012/04/fixing-science-systems-and-politics.html">clinical trials</a>. Because it is a good way to (a) show that your theory can make viable predictions and (b) that your empirical finding is not vulnerable to hypothesising after the results are known (HARKing) and some other questionable research practices, more and more scientists endorse and actually do pre-registration. <a href="http://funderstorms.wordpress.com/2014/02/25/nsf-gets-an-earful-about-replication/">Many</a> remain wary though and <a href="http://andrewgelman.com/2014/01/23/discussion-preregistration-research-studies/">some</a> simply think pre-registration cannot work for their kind of research. A recent amendment (October 2013) to the Declaration of Helsinki mandates public registration of all research on humans before recruiting the first subject and the publication of all results, positive, negative and inconclusive. </p>
<p>For some of  science the widespread “fishing for significance” metaphor illustrates the problem well: Like an experimental scientist the fisherman casts out the rod many times, tinkering with a variety of baits and bobbers, one at a time, trying to make a good catch, but possibly developing a superstition about the best bobber. And, like an experimental scientist, if he returns the next day to the same spot, it would be easy to check whether the success of the bobber replicates. If he prefers to tell fishing lore and enshrine his bobber in a display at his home, other fishermen can evaluate his lore by doing as he did in his stories.</p>
<p>Some disciplines (epidemiology, economics, developmental and personality psychology come to mind) proceed, quite legitimately, more like fishing trawlers – that is to say data collection is a laborious, time-consuming, collaborative endeavour. Because these operations are so large and complex, some data bycatch will inevitably end up in the dragnet. </p>
 <a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/#disqus_thread" data-disqus-identifier="2014/02/27/data-trawling/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Feb  5,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/" rel="bookmark" title="Permanent Link to &quot;Open Data and IRBs&quot;">Open Data and IRBs</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/bryan-burnham.html" rel="author">Bryan Burnham</a>

   			   <p>Among other things the open science movement encourages “open data” practices, that is, researchers making data freely available on personal/lab websites or institutional repositories for others to use. For some, open data is a necessity as the <a href="http://grants.nih.gov/grants/policy/data_sharing/data_sharing_guidance.htm#goals">NIH</a> and <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a> have adopted data-sharing policies and require some grant applications to include data management and dissemination plans. According to the NIH:</p>
<blockquote>
<p>“...all data should be considered for data sharing. <strong>Data should be made as widely and freely available as possible while safeguarding the privacy of participants, and protecting confidential and proprietary data.</strong>” (emphasis theirs)</p>
</blockquote>
<p>Before making human subject data open several issues must be considered. First, data should be de-identified to maintain subject confidentiality so responses cannot be linked to identities and data are seemingly anonymous. Second, researchers should consider Institutional Review Board’s (IRB) policies about data sharing. (Disclosure: I have been a member of my university's IRB for 6 years and chair of my Departmental Review Board, DRB, for 7 years.)</p>
<p>Unfortunately, while the policies and procedure of all IRBs require researchers to obtain consent, disclose study procedures to subjects, and maintain confidentiality, it is unknown how many IRBs have policies and procedures for open data dissemination. Thus, a conflict may arise between researchers who want to adopt open data practices or <em>need</em> to disseminate data (those with NIH or NSF grants) and judgements of IRBs.</p>
<p>This is an especially important issue for those who want to share data that are already collected: can use data be openly disseminated without IRB review? (I address this below when I offer recommendations.) What can researchers do when they want or need to share data freely, but their IRB does not have a clear policy? And what say does an IRB have in open data practices?</p>
<p>While IRBs should be consulted and informed about open data, as I delineate below IRBs are not now and were never intended to be data-monitoring groups (Bankert &amp; Amdur, 2000). IRBs are regulated and have little say in whether a researcher can share data, based on the purview, scope, and responsibilities of IRBs.</p>
<p>IRBs in the United States are regulated under <a href="http://www.hhs.gov/ohrp/humansubjects/guidance/45cfr46.html">US Health and Human Services (HHS)</a> guidelines for Protection of Human Subjects. The guidelines describe the composition of IRBs, record keeping, define levels of risk, and list specific duties of IRBs and hint at their limits.</p>
<p>When they function appropriately IRBs review research protocols to (1) evaluate risks; (2) determine whether subject confidentiality is maintained, that is, whether responses are linked to identities (‘confidentiality’ differs from ‘privacy’, which means others will not know a person participated in a study); and (3) evaluate whether subjects are given sufficient information about risks, procedures, privacy, and confidentiality. HHS Regulations Part 46, Subpart A, Section 111 ("Criteria for IRB Approval of Research") (a)(2), is very specific on the purview of IRBs in evaluating protocols:</p>
<blockquote>
<p>"In evaluating risks and benefits, <strong>the IRB should consider only those risks and benefits that may result from the research</strong> (as distinguished from risks and benefits of therapies subjects would receive even if not participating in the research). <strong>The IRB should not consider possible long-range effects of applying knowledge gained in the research (for example, the possible effects of the research on public policy) as among those research risks that fall within the purview of its responsibility.</strong>" [emphasis added]</p>
</blockquote>
<p>And regulations §46.111 (a)(6) and (a)(7) state that IRBs are to evaluate the safety, privacy, and confidentiality of subjects in proposed research:</p>
<blockquote>
<p>(a)(6)  "When appropriate, the research plan makes adequate provision for monitoring the data collected to ensure the safety of subjects.”
(a)(7) “When appropriate, there are adequate provisions to protect the privacy of subjects and to maintain the confidentiality of data." </p>
</blockquote>
<p>The regulations make it clear that IRBs should consider only risks directly related to the study, and explicitly forbid IRBs from evaluating potential long-range effects of new knowledge gained from the study, as in new knowledge resulting from data sharing. Thus, IRBs should concern themselves with evaluating a study for safety, confidentiality, and that information is disclosed; reviewing existing data for dissemination is <em>not</em> under the purview of the IRB. The <em>only</em> issue that should concern IRBs about open data is whether the data are de-identified to “...protect the privacy of subjects and to maintain the confidentiality of data." It is not the responsibility of the IRB to monitor data, that responsibility falls to the researcher.</p>
<p>Nonetheless, IRBs may take the position that they are data monitors and deny a researcher’s request to openly disseminate data. In denying a request an IRB may use the argument ‘<em>subjects would not have participated if they knew the data would be openly shared.</em>’ In this case, IRBs would be playing mind-readers; there is no way an IRB can assume subjects would not have participated if they knew data would be openly shared. However, whether a person would decline to participate if they were informed about a researcher’s intent to openly disseminate data is an empirical question.</p>
<p>Also, with this argument the IRB is implicitly suggesting subjects would need to have been informed about open data dissemination in the consent form. But, such a requirement for consent forms neglects other federal guidelines. The <a href="http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html%20">Belmont Report</a> provides responsibilities for human researchers, much like the <a href="http://www.apa.org/ethics/code/index.aspx">APA's ethical principles</a>, and describes what information should be included in the consent process:</p>
<blockquote>
<p>“Most codes of research establish specific items for disclosure intended to assure that subjects are given sufficient information. These items generally include: the research procedure, their purposes, risks and anticipated benefits, alternative procedures (where therapy is involved), and a statement offering the subject the opportunity to ask questions and to withdraw at any time from the research.”</p>
</blockquote>
<p>The Belmont Report does not even mention that subjects should be informed about the potential long-range plans or uses of the data they provide. Indeed, researchers do not have to tell subjects what analyses will be used, and for good reason. All the Belmont requires is for subjects be informed about the purpose of the study, the procedures, and be informed about their privacy and confidentiality of responses.</p>
<p>Another argument an IRB could make is the data could be used maliciously. For example, a researcher could make a data set open that included ethnicity and test scores and someone else could use that data to show certain ethnic groups are smarter than others. (This example is based on a recent <a href="https://groups.google.com/forum/#!topic/openscienceframework/JHucNxN19hc">Open Science Framework post</a> that is the basis for this post.) </p>
<p>Although it is more likely that open data would be used as intended, someone could use data as they were not intended and may find a relationship between ethnicity and test scores. So what? The data are not malicious or problematic, it is the person using (misusing?) the data, and IRBs should not be in the habit of allowing only politically correct research to proceed (Lilienfeld, 2010). Also, by considering what others <em>might</em> do with open data, IRBs would be mind-reading and overstepping its purview by considering “...long-range effects of applying knowledge gained in the research (for example, the possible effects of the research on public policy).”</p>
<p>The bottom line is IRBs cannot know whether subjects would not have participated in a project if they knew the data would be openly disseminated, or potential findings by others. Federal regulations inform IRBs of their specific duties, which do <em>not</em> include data monitoring or making judgments on open data dissemination; those duties are the responsibilities of the researcher.</p>
<p>So what should you do if you want to make your data open? First, don't fear the IRB, but don’t forget the IRB. Perhaps re-examine IRB policies any time you plan a new project to remind yourself of the IRB requirements.</p>
<p>Second, making your data open does depend on what subjects agree to on the consent form, and this is especially important if you want to make existing data open. If subjects are told their participation will remain private (identities not disclosed) and responses will remain confidential (identities not linked to responses), openly disseminating de-identified data would not violate the agreement. However, if subjects were told the data would ‘not be disseminated’, the researcher <em>may</em> violate the agreement if they openly share data. In this case the IRB would need to be involved, subjects may need to re-consent to allow their responses to be disseminated, and new IRB approval may be needed as the original consent agreement may change.</p>
<p>Third, <a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/">de-identify data sets you plan to make open</a>. This includes removing names, student IDs, the subject numbers, timestamps, and anything else that could be used to uniquely identify a person.</p>
<p>Fourth, inform your IRB and department of your intentions. Describe your de-identification process and that you are engaging in open data practices as you see appropriate while maintaining subject confidentiality and privacy. (If someone objects, direct them toward federal IRB regulations.)</p>
<p>Finally, work with your IRB to develop guidelines and policies for data sharing. Given the speed and recency of the open science and open data movements, it is unlikely many IRBs have considered such policies.</p>
<p>We want greater transparency in science, and open data is one practice the can help. The IRB should not be seen as a hurdle or barrier to disseminating data, but as a reminder that one of the best practices in science is to ensure the integrity of our data and information communications by responsibly maintaining the confidence and privacy of our research subjects.</p>
<p><strong>References</strong></p>
<p>Bankert, E., &amp; Amdur, R. (2000). <a href="http://www.jstor.org/stable/3563586">The IRB is not a data and safety monitoring board.</a> IRB: Ethics and Human Research, 22(6), 9-11. </p>
<p>De Wolfe, V. A., Sieber, J. E., Steel, P. M., &amp; Zarate, A. O. (2005). <a href="http://www.jstor.org/stable/3563537">Part I: What is the requirement for data sharing?</a> IRB: Ethics and Human Research, 27(6), 12-16. </p>
<p>De Wolfe, V. A., Sieber, J. E., Steel, P. M., &amp; Zarate, A. O. (2006). <a href="http://www.jstor.org/stable/30033191">Part III: Meeting the challenge when data sharing is required.</a> IRB: Ethics and Human Research, 28(2), 10-15. </p>
<p>Lilienfeld, S.O. (2010).  <em>Can psychology become a science?</em>  Personality and Individual Differences, 49, 281-288.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/#disqus_thread" data-disqus-identifier="2014/02/05/open-data-and-IRBs/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jan 29,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/" rel="bookmark" title="Permanent Link to &quot;Privacy in the Age of Open Data&quot;">Privacy in the Age of Open Data</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/sean-mackinnon.html" rel="author">Sean Mackinnon</a>

   			   <p>Nothing is really private anymore.  Corporations like Facebook and Google have been collecting our information for some time, and selling it in aggregate to the highest bidder. People have been raising concerns over these invasions of privacy, but generally only technically-savvy, highly motivated people can really be successful at remaining anonymous in this new digital world.</p>
<p>For a variety of incredibly important reasons, we are moving towards open research data as a scientific norm – that is, micro datasets and statistical syntax openly available to anyone who wants it. However, some people are uncomfortable with open research data, because they have concerns about privacy and confidentiality violations.  Some of these violations are even making the news: <a href="http://wi.mit.edu/news/archive/2013/scientists-expose-new-vulnerabilities-security-personal-genetic-information">A high profile case</a> about people being identified from their publicly shared genetic information comes to mind.</p>
<p>With open data comes increased responsibility. As researchers, we need to take particular care to balance the advantages of data-sharing with the need to protect research participants from harm.  I’m particularly primed for this issue because my own research often intersects with clinical psychology. I ask questions about things like depression, anxiety, eating disorders, substance use and conflict with romantic partners.  The data collected in many of my studies has the potential to seriously harm the reputation – and potentially the mental health – of participants if linked to their identity by a malicious person.  This said, I believe in the value of open data sharing. In this post, I’m going to discuss a few core issues as it pertains to de-identification – that is, ensuring the anonymity of participants in an openly shared dataset.  Violations of privacy will always be a risk: However, some relatively simple steps on the part of the researcher can make re-identification of individual participants much more challenging.</p>
<h3>Who are we protecting the data from?</h3>
<p>Throughout the process, it’s helpful to imagine yourself as a person trying to get dirt on a potential participant. Of course, this is ignoring the fact that very few people are likely to use data for malicious purposes … but for now, let’s just consider the rare cases where this might happen. It only takes one high-profile incident to be a public relations and ethics nightmare for your research! There are two possibilities for malicious users that I can think of:</p>
<ol>
<li>
<p>Identity thieves who don’t know the participant directly, but are looking for enough personal information to duplicate someone’s identity for criminal activities, such as credit card fraud. These users are unlikely to know anything about participants ahead of time, so they have a much more challenging job because they have to be able to identify people exclusively using publicly available information.</p>
</li>
<li>
<p>People who know the participant in real-life and want to find out private information about someone for some unpleasant purpose (e.g., stalkers, jealous romantic partners, a fired employee, etc.). In this case, the party likely knows (a) that the person of interest is in your dataset; (b) basic demographic information on the person such as sex, age, occupation, and the city they live in.  Whether or not this user is successful in identifying individuals in an open dataset depends on what exactly the researcher has shared.  For fine-grained data, it could be very easy; however, for properly de-identified data, it should be virtually impossible.</p>
</li>
</ol>
<h3>Key Identifiers to Consider when De-Identifying Data</h3>
<p>The primary way to safeguard privacy in publicly shared data is to avoid identifiers; that is, pieces of information that can be used directly or indirectly to determine a person’s identity. A useful starting point for this is the list of 18 identifiers indicated in the <a href="http://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act">Health Insurance Portability and Accountability Act</a> that are to be used with Protected Health Information. <a href="http://www.oshpd.ca.gov/Boards/CPHS/HIPAAIdentifiers.pdf">A full list of these identifiers can be found here.</a> Many of these identifiers are obvious (e.g., no names, phone numbers, SIN numbers, etc.), but some identifiers are worth discussing more specifically in the context of psychological research paradigm which shares data openly.</p>
<p><strong>Demographic variables</strong>. Most of the variables that psychologists are interested in are not going to be very informative for identifying individuals.  For example, reaction time data (even if unique to an individual) is very unlikely to identify participants – and in any event, most people are unlikely to care if other people know that they respond 50ms faster to certain types of visual stimuli. The type of data that are generally problematic are what I’ll call “demographic variables.” So things like sex, ethnicity, age, occupation, university major, etc.  These data are sometimes used in analyses, but most often are just used to characterize the sample in the participants section of manuscripts. Most of the time, demographic variables can’t be used in isolation to identify people; instead, combinations of variables are used (e.g., a 27-year old, Mexican woman who works as a nurse may be the only person with that combination of traits in the data, leaving her vulnerable to loss of privacy). Because the combination of several demographic characteristics can potentially produce identifiable profiles, a common rule of thumb I picked up when working with Statistics Canada is to require a minimum of 5 participants per cell. In other words, if a particular combination of demographic features yields less than 5 individuals, the group will be collapsed into a larger, more anonymous, aggregate group. The most common example of this would be using age ranges (e.g., ages 18-25) instead of exact ages; similar logic could apply to most demographic variables. This rule can get restrictive fast (but also demonstrates how little data can be required to identify individual people!) so ideally, share only the demographic information that is theoretically and empirically important to your research area.</p>
<p><strong>Outliers and rare values</strong>. Another major issue are outliers and other rare values. Outliers are variably defined depending on the statistical text you read, but generally refer to extreme values when variables are using continuous, interval, or ordinal measurement (e.g., someone has an IQ of 150 in your sample, and the next highest person is 120). Rare values refer to categorical data that very few people endorse (e.g., the only physics professor in a sample). There are lots of different ways you can deal with outliers, and there’s not necessarily a lot of agreement on which is the best – indeed, it’s one of those <a href="http://osc.centerforopenscience.org/2013/12/18/researcher-degrees-of-freedom/">researcher degrees of freedom</a> you might have heard about. Though this may depend on the sensitivity of the data in question, outliers often have the potential to be a privacy risk. From a privacy standpoint, it may be best for the researcher to deal with outliers by deleting or transforming them before sharing the data. For rare values, you can collapse response options together until there are no more unique values (e.g., perhaps classify the physics professor as a “teaching professional” if there are other teachers in the sample). In the worst case scenario, you may need to report the value as missing data (e.g., a single intersex person in your sample that doesn’t identify as male or female). Whatever you decide, you should disclose to readers what your strategy was for dealing with outliers and rare values in the accompanying documentation so it is clear for everyone using the data.</p>
<p><strong>Dates</strong>. Though it might not be immediately obvious, any exact dates in the dataset place participants at risk for re-identification. For example, if someone knew what day the participant took part in a study (e.g., they mention it to a friend; they’re seen in a participant waiting area) then their data would be easily identifiable by this date.  To minimize privacy risks, no exact dates should be included in the shared dataset. If dates are necessary for certain analyses, transforming the data into some less identifiable format that is still useful for analyses is preferable (e.g., have variables for “day of week” or “number of days in between measurement occasions” if these are important).</p>
<p><strong>Geographic Locations</strong>. The rule of having “no geographic subdivisions smaller than a state” from the <a href="http://www.oshpd.ca.gov/Boards/CPHS/HIPAAIdentifiers.pdf">HIPAA guidelines</a> is immediately problematic for many studies. Most researchers collect data from their surrounding community. Thus, it will be impossible to blind the geographic location in many circumstances (e.g., if I recruit psychology students for my study, it will be easy for others to infer that I did so from my place of employment at Dalhousie University). So at a minimum, people will know that participants are probably living relatively close to my place of employment. This is going to be unavoidable in many circumstances, but in most cases it should not be enough to identify participants. However, you will need to consider if this geographical information can be combined with other demographic information to potentially identify people, since it will not be possible to suppress this information in many cases. Aside from that, you’ll just have to do your best to avoid more finely grained geographical information. For example, in Canada, <a href="http://www.canadapost.ca/cpotools/apps/fpc/personal/findAnAddress?execution=e3s1">a reverse lookup of postal codes</a> can identify some locations with a surprising degree of accuracy, sometimes down to a particular street!</p>
<p><strong>Participant ID numbers</strong>. Almost every dataset will (and should) have a unique identification number for each participant. If this is just a randomly selected number, there are no major issues. However, most researchers I know generate ID numbers in non-random ways. For example, in my own research on romantic couples we assign ID numbers chronologically, with a suffix number of “1” indicating men and “2” indicating women. So ID 003-2 would be the third couple that participated, and the male within that couple. In this kind of research, the most likely person to snoop would probably be the other romantic partner. If I were to leave the ID numbers as originally entered, the romantic partner would easily be able to find their own partner’s data (assuming a heterosexual relationship and that participants remember their own ID number). There are many other algorithms researchers might use to create ID numbers, many of which do not provide helpful information to other researchers, but could be used to identify people. Before freely sharing data, you might consider scrambling the unique ID numbers so that they cannot be a privacy risk (you can, of course, keep a record of the original ID numbers in your own files if needed for administrative purposes).</p>
<h3>Some Final Thoughts</h3>
<p>Risk of re-identification is never zero. Especially when data are shared openly online, there will always be a risk for participants. Making sure participants are fully informed about the risks involved during the consent process is essential. Careless sharing of data could result in a breach of privacy, which could have extremely negative consequences both for the participants and for your own research program. However, with proper safeguards, the risk of re-identification is low, in part due to some naturally occurring features of research. The slow, plodding pace of scientific research inadvertently protects the privacy of participants: Databases are likely to be 1-3 years old by the time they are posted, and people can change considerably within that time, making them harder to identify. Naturally occurring noise (e.g., missing data, imputation, errors by participants) also impedes the ability to identify people, and the variables psychologists are usually most interested in are often not likely candidates to re-identify someone.</p>
<p>As a community of scientists devoted to making science more transparent and open, we also carry the responsibility of protecting the privacy and rights of participants as much as is possible. I don’t think we have all the answers yet, and there’s a lot more to consider when moving forward.  Ethical principles are not static; there are no single “right” answers that will be appropriate for all research, and standards will change as technology and social mores change with each generation. Still, by moving forward with an open mind, and a strong ethical conscience to protect the privacy of participants, I believe that data can really be both open and private. </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/#disqus_thread" data-disqus-identifier="2014/01/29/privacy-and-open-data/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jan 22,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/01/22/op-wiki-project-med/" rel="bookmark" title="Permanent Link to &quot;Open Projects - Wikipedia Project Medicine&quot;">Open Projects - Wikipedia Project Medicine</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This article is the first in <a href="http://osc.centerforopenscience.org/tag/open-projects.html">a series highlighting open science projects</a> around the community.  You can read the interview this article was based on: <a href="https://docs.google.com/document/d/1xSHIF3hqqqF_iyb5olwj5NvVIVwIZX794rAINq93q_s/edit?usp=sharing">edited for clarity</a>, <a href="https://docs.google.com/document/d/1Ci9pYkjbignjuir9Tjcx2j4TMTS_iFJ8PAcdXhtlIlA/edit?usp=sharing">unedited</a>.</em></p>
<p>Six years ago, Doctor James Heilman was working a night shift in the ER when he came across an error-ridden article on Wikipedia.  Someone else might have used the article to dismiss the online encyclopedia, which was then less than half the size it is now.  Instead, Heilman decided to improve the article.  “I noticed an edit button and realized that I could fix it.  Sort of got hooked from there.  I’m still finding lots of articles that need a great deal of work before they reflect the best available medical evidence.”</p>
<p>Heilman, who goes by the username <a href="http://en.wikipedia.org/wiki/User:Jmh649">Jmh649</a> on Wikipedia, is now the president of the board of <a href="http://meta.wikimedia.org/wiki/Wiki_Project_Med">Wiki Project Med</a>.  A non-profit corporation created to promote medical content on Wikipedia, WPM contains over a dozen different initiatives aimed at adding and improving articles, building relationships with schools, journals and other medical organizations, and increasing access to research.</p>
<p>One of the initiatives closest to Heilman’s heart is the Translation Task Force, an effort to identify key medical articles and translate them into as many languages as they can.  These articles cover common and potentially deadly medical circumstances, such as gastroenteritis (diarrhea), birth control, HIV/AIDS, and burns.  With the help of Translators Without Borders, over 3 million words have been translated into about 60 languages.  One of these languages is Yoruba, a West African language.  Although Yoruba is spoken by nearly 30 million people, there are only a few editors working to translate medical articles into it.  </p>
<p>“The first two billion people online by and large speak/understand at least one of the wealthy languages of the world.  With more and more people getting online via cellphones that is not going to be true for the next 5 billion coming online.  Many of them will find little that they can understand.”  <a href="http://wikimediafoundation.org/wiki/Wikipedia_Zero">Wikipedia Zero</a>, a program which provides users in some developing countries access to Wikipedia without mobile data charges, is increasing access to the site.  </p>
<p>“People are, for better or worse, learning about life and death issues through Wikipedia.  So we need to make sure that content is accurate, up to date, well-sourced, comprehensive, and accessible.  For readers with no native medical literature, Wikipedia may well be the only option they have to learn about health and disease.” </p>
<p>That’s Jake Orlowitz (<a href="http://en.wikipedia.org/wiki/User:Ocaasi/About">Ocaasi</a>), WPM’s outreach coordinator.  He and Heilman stress that there’s a lot of need for volunteer help, and not just with translating.  Of the 80+ <a href="http://en.wikipedia.org/wiki/Book:Health_care">articles identified as key</a>, only 31 are ready to be translated.  The rest need citations verified, jargon simplified, content updated and restructured, and more.  </p>
<p>In an effort to find more expert contributors, WPM has launched a number of initiatives to partner with medical schools and other research organizations.  Orlowitz was recently a course ambassador to the UCSF medical school, where students edited Wikipedia articles for credit.  He also set up a partnership with the <a href="http://www.cochrane.org/">Cochrane Collaboration</a> a non-profit made up of over 30,000 volunteers, mostly medical professionals, who conduct reviews of medical interventions.  “We arranged a donation of 100 full access accounts to The Cochrane Library, and we are currently coordinating a Wikipedian in Residence position with them.  That person will teach dozens of Cochrane authors how to incorporate their findings into Wikipedia,” explains Orlowitz.</p>
<p>Those who are familiar with how Wikipedia is edited might balk at the thought of contributing.  Won’t they be drawn in to “edit wars”, endless battles with people who don’t believe in evolution or who just enjoy conflict?  “There are edit wars,” admits Heilman.  “They are not that common though.  99% of articles can be easily edited without problems.”</p>
<p>Orlowitz elaborates on some of the problems that arise.  “We have a lot of new editors who don't understand evidence quality.”  The medical experts they recruit face a different set of challenges.  “One difficulty many experts have is that they wish to reference their own primary sources.  Or write about themselves.  Both those are frowned upon.  We also have some drug and device companies that edit articles in their area of business--we discourage this strongly and it's something we keep an eye on.”</p>
<p>And what about legitimate differences of opinion about as yet unsettled medical theories, facts and treatments?  </p>
<p>“Wikipedia 'describes debates rather than engaging in them'.  We don't take sides, we just summarize the evidence on all sides--in proportion to the quality and quantity of that evidence,” says Orlowitz.  Heilman continues: “For example Cochrane reviews state it is unclear if the risk versus benefits of breast cancer screening are positive or negative.  The USPSTF is supportive.  We state both.”  Wikipedia provides <a href="http://en.wikipedia.org/wiki/Wikipedia:Identifying_reliable_sources_(medicine)">detailed guidelines</a> for evaluating sources and dealing with conflicting evidence.  </p>
<p>Another reason academics might hesitate before contributing is the poor reputation Wikipedia has in academic circles.  Another initiative, the Wikipedia-journal collaboration, 
states: "One reason some academics express for not contributing to Wikipedia is that they are unable to get the recognition they require for their current professional position. A number of medical journals have agreed in principle to publishing high quality Wikipedia articles under authors' real names following formal peer review.”  A pilot paper, adapted from the Wikipedia article on Dengue Fever, is to be published in the <a href="http://www.openmedicine.ca/">Journal of Open Medicine</a>, with more publications hopefully to come.</p>
<p>The stigma against Wikipedia itself is also decreasing.  “The usage stats for the lay public, medical students, junior physicians, and doctors, and pharmacists are just mindbogglingly high.  It's in the range of 50-90%, even for clinical professionals.  We hear a lot that doctors 'jog their memory' with Wikipedia, or use it as a starting point,” says Orlowitz.  One <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3713956/">2013 study</a> found that a third or more of general practitioners, specialists and medical professors had used Wikipedia, with over half of physicians in training accessing it.  As more diverse kinds of scientific contributions begin to be recognized, Wikipedia edits may make their way onto CVs.  </p>
<p>Open science activists may be disappointed to learn that Wikipedia doesn’t require or even prefer open access sources for its articles.  “<a href="http://enwp.org/WP:PAYWALL">Our policy</a> simply states that our primary concern is article content, and verifi_ability_.  That standard is irrespective of how hard or easy it is to verify,” explains Orlowitz.  Both Wikipedians personally support open access, and would welcome efforts to supplement closed access citations with open ones.  “If there are multiple sources of equal quality that come to the same conclusions we support using the open source ones,” says Heilman.  A new project, the <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Open_Access/Signalling_OA-ness">Open Access Signalling project</a> aims to help readers quickly distinguish what sources they’ll be able to access.</p>
<p>So what are the best ways for newcomers to get involved?  Heilman stresses that editing articles remains one of the most important tasks of the project.  This is especially true of people affiliated with universities.  “Ironically, since these folks have access to high quality paywalled sources, one great thing they could do would be to update articles with them. We also could explore affiliating a Wikipedia editor with a university as a Visiting Scholar, so they'd have access to the library's catalogue to improve Wikipedia, in the spirit of research affiliates,” says Orlowitz.   </p>
<p>Adds Heilman, “If there are institution who would be willing to donate library accounts to Wikipedia's we would appreciate it.  This would require having the Wikipedian register in some manner with the university.  There are also a number of us who may be willing / able to speak to Universities that wish to learn more about the place of Wikipedia in Medicine.”  The two also speak at conferences and other events.  </p>
<p>Wiki Project Med, like Wikipedia itself, is an open community - a “do-ocracy”, as Orlowitz calls it.  If you’re interested in learning more, or in getting involved, you can check out their <a href="http://meta.wikimedia.org/wiki/Wiki_Project_Med">project page</a>, which details their many initiatives, or reach out to Orlowitz or the project as a whole on Twitter (<a href="https://twitter.com/JakeOrlowitz">@JakeOrlowitz</a>, <a href="https://twitter.com/WikiProjectMed">@WikiProjectMed</a>) or via email (jorlowitz@gmail.com, wikiprojectmed@gmail.com).</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/01/22/op-wiki-project-med/#disqus_thread" data-disqus-identifier="2014/01/22/op-wiki-project-med/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/01/22/op-wiki-project-med/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/open-projects.html">open-projects</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jan 15,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/01/15/apa-and-open-data/" rel="bookmark" title="Permanent Link to &quot;The APA and Open Data: one step forward, two steps back?&quot;">The APA and Open Data: one step forward, two steps back?</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/denny-borsboom.html" rel="author">Denny Borsboom</a>

   			   <p><img src="/images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /></p>
<p>I was pleasantly surprised when, last year, I was approached with the request to become Consulting Editor for a new APA journal called <em>Archives of Scientific Psychology</em>. The journal, as advertised on its website upon launch, had a distinct Open Science signature. As its motto said, it was an “Open Methodology, Open Data, Open Access journal”.  That’s a lot of openness indeed.</p>
<p>When the journal started, the website not only boosted the Open Access feature of the journal, but went on to say that "[t]he authors have made available for use by others the data that underlie the analyses presented in the paper". This was an incredibly daunting move by APA - or so it seemed. Of course, I happily accepted the position.</p>
<p>After a few months, the first papers in <em>Archives</em> were published. Open Data enthusiast Jelte Wicherts of Tilburg University immediately tried to retrieve data for reanalysis. Then it turned out that the APA holds a quite ideosyncratic definition of the word “open”: upon his request, Wicherts was referred to a website that presented a <a href="http://www.apa.org/pubs/journals/arc/data-access.aspx">daunting list of requirements</a> for data-requests to fulfill. That was quite a bit more intimidating than the positive tone struck in <a href="http://www.apa.org/pubs/journals/features/arc-1-1-1.pdf">the editorial</a> that accompanied the launch of the journal.</p>
<p>This didn’t seem open to me at all. So: I approached the editors and said that I could not subscribe to this procedure, given the fact that the journal is supposed to have open data. The editors then informed me that their choice to implement these procedures was an entirely conscious one, and that they stood by it. Their point of view is articulated in their data sharing guidelines. For instance, "next-users of data must formally agree to offer co-authorship to the generator(s) of the data on any subsequent publications" since "[i]t is the opinion of the <em>Archives</em> editors that designing and conducting the original data collection is a scientific contribution that cannot be exhausted after one use of the data; it resides in the data permanently."</p>
<p>Well, that's not my opinion at all. In fact it's quite directly opposed to virtually everything I think is important about openness in scientific research. So I chose to resign my position.</p>
<p>In October 2013, I learned that Wicherts had taken the initiative of exposing the <em>Archives</em>’ policy in an open letter to the editorial board, in which he says:</p>
<blockquote>
<p>“[…] I recently learned that data from empirical articles published in the <em>Archives</em> are not even close to being “open”.</p>
<p>In fact, a request for data published in the <em>Archives</em> involves not only a full-blown review committee but also the filling in and signing of an extensive form: http://www.apa.org/pubs/journals/features/arc-data-access-request-form.pdf</p>
<p>This 15-page form asks for the sending of professional resumes, descriptions of the policies concerning academic integrity at one’s institution, explicit research plans including hypotheses and societal relevance, specification of the types of analyses, full ethics approval of the reanalysis by the IRB, descriptions of the background of the research environment, an indication of the primary source of revenue of one’s institution, dissemination plans of the work to be done with the data, a justification for the data request, manners of storage, types of computers and storage media being used, ways of transmitting data between research team members,  whether data will be encrypted, and signatures of institutional heads.</p>
<p>The requester of the data also has to sign that (s)he provides an “Offer [of] co-authorship to the data generators on any subsequent publications” and the (s)he will offer to the review committee an “annual data use report that outlines what has been done, that the investigator remains in compliance with the original research proposal, and provide references of any resulting publications.”</p>
<p>In case of non-compliance of any of these stipulations, the requester can face up to a $10,000 fine as well a future prohibition of data access from work published in the <em>Archives</em>.”</p>
</blockquote>
<p>A fine?  Seriously? Kafkaesque!</p>
<p>Wicherts also notes that “the guidelines with respect to data sharing in the <em>Archives</em> considerably exceed APA’s Ethical Standard 8.14”. <a href="http://www.apa.org/ethics/code/index.aspx?item=11">Ethical Standard 8.14</a> is a default that applies to all APA journals, and says:</p>
<blockquote>
<p>“After research results are published, psychologists do not withhold the data on which their conclusions are based from other competent professionals who seek to verify the substantive claims through reanalysis and who intend to use such data only for that purpose, provided that the confidentiality of the participants can be protected and unless legal rights concerning proprietary data preclude their release.” </p>
</blockquote>
<p>Since this guideline says nothing about fines and co-authorship requirements, we indeed have to conclude that it’s <em>harder</em> to get data from APA’s open science journal, than it is to get data from its regular journals. Picture that!</p>
<p>In response to my resignation and Wicherts' letter, the editors have taken an interesting course of action. Rather than change their policy such that their deeds match their name, they have changed their name to match their deeds. The journal is now no longer an "Open Methodology, Open Data, Open Access Journal" but an "Open Methodology, Collaborative Data Sharing, Open Access Journal".</p>
<p>The APA and open data. One step forward, two steps back.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/01/15/apa-and-open-data/#disqus_thread" data-disqus-identifier="2014/01/15/apa-and-open-data/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/01/15/apa-and-open-data/">Posted at 11:20 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jan  8,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/01/08/hard-science/" rel="bookmark" title="Permanent Link to &quot;When Open Science is Hard Science&quot;">When Open Science is Hard Science</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p>When it comes to opening up your work there is, ironically, a bit of a secret.  Here it is: being open - in open science, open source software, or any other open community - can be hard.  Sometimes it can be harder than being closed.</p>
<p>In an effort to attract more people to the cause, advocates of openness tend to tout its benefits.  Said benefits are bountiful: increased collaboration and dissemination of ideas, transparency leading to more frequent error checking, improved reproducibility, easier meta-analysis, and greater diversity in participation, just to name a few.</p>
<p>But there are downsides, too.  One of those is that it can be difficult to do your research openly.  (Note here that I mean well and openly.  Taking the full contents of your hard drive and dumping it on a server somewhere might be technically open, but it’s not much use to anyone.)</p>
<p>How is it hard to open up your work?  And why?</p>
<p><strong>Closed means privacy.</strong></p>
<p>In the privacy of my own home, I seldom brush my hair.  Sometimes I spend all day in my pajamas.  I leave my dirty dishes on the table and eat ice cream straight out of the tub.  But when I have visitors, or when I’m going out, I make sure to clean up.</p>
<p>In the privacy of a closed access project, you might take shortcuts.  You might recruit participants from your own 101 class, or process your data without carefully documenting which steps you took.  You’d never intentionally do something unethical, but you might get sloppy.</p>
<p>Humans are social animals.  We try to be more perfect for each other than we do for ourselves.  This makes openness better, but it also makes it harder.</p>
<p><strong>Two heads need more explanation than one.</strong></p>
<p>As I mentioned above, taking all your work and throwing it online without organization or documentation is not very helpful.  There’s a difference between access and accessibility.  To create a truly open project, you need to be willing to explain your research to those trying to understand it. </p>
<p>There are numerous routes towards sharing your work, and the most open projects take more than one.  You can create stellar documentation of your project.  You can point people towards background material, finding good explanations of the way your research methodology was developed or the math behind your data analysis or how the code that runs your stimulus presentation works.  You can design tutorials or trainings for people who want to run your study.  You can encourage people to ask questions about the project, and reply publicly.  You can make sure to do all the above for people at all levels - laypeople, students, and participants as well as colleagues. </p>
<p>Even closed science is usually collaborative, so hopefully your project is decently well documented.  But making it accessible to everyone is a project in itself.</p>
<p><strong>New ideas and tools need to be learned.</strong></p>
<p>As long as closed is the default, we’ll need to learn new skills and tools in the process of becoming open, such as version control, format conversion and database management.</p>
<p>These skills aren’t unique to working openly.  And if you have a good network of friends and colleagues, you can lean on them to supplement your own expertise.  But the fact remains that “going open” isn’t as easy as flipping a switch.  Unless you’re already well-connected and well-informed, you’ll have a lot to learn.</p>
<p><strong>People can be exhausting.</strong></p>
<p>Making your work open often means dealing with other people - and not always the people you want to deal with.  There are the people who mean well, but end up confusing, misleading, or offending you.  There are the people who don’t mean well at all.  There are the discussions that go off in unproductive directions, the conversations that turn into conflicts, the promises that get forgotten.</p>
<p>Other people are both a joy and a frustration, in many areas of life beyond open science.  But the nature of openness assures you’ll get your fair share.  This is especially true of open science projects that are explicitly trying to build community.</p>
<p>It can be all too easy to overlook this <a href="http://en.wikipedia.org/wiki/Emotional_labor#Gender">emotional labor</a>, but it’s work - hard work, at that.</p>
<p><strong>There are no guarantees.</strong></p>
<p>For all the effort you put into opening up your research, you may find no one else is willing to engage with it.  There are plenty of open source software projects with no forks or new contributors, open science articles that are seldom downloaded or science wikis that remain mostly empty, open government tools or datasets that no one uses.</p>
<p><a href="http://opcit.eprints.org/oacitation-biblio.html">Open access may increase impact</a> on the whole, but there are no promises for any particular project.  It’s a sobering prospect to someone considering opening up their research.</p>
<p><strong>How can we make open science easier?</strong></p>
<p>We can advocate for open science while acknowledging the barriers to achieving it.  And we can do our best to lower those barriers:</p>
<p><em>Forgive imperfections.</em>  We need to create an environment where mistakes are routine and failures are expected - only then will researchers feel comfortable exposing their work to widespread review.  That’s a tall order in the cutthroat world of academia, but we can begin with our own roles as teachers, mentors, reviewers, and internet commentators.  Be a role model: encourage others to review your work and point out your mistakes.</p>
<p><em>Share your skills as well as your research.</em>  Talk about your experiences opening up your research with colleagues.  Host lab meetings, department events, and conference panels to discuss the practical difficulties.  If a training, website, or individual helped you understand some skill or concept, recommend widely.  Talking about the individual steps will help the journey seem less intimidating - and will give others a map for how to get there.</p>
<p><em>Recognize the hard work of others with words and, if you can, financial support.</em>  Organization, documentation, mentorship, community management.  These are areas that often get overlooked when it comes to celebrating scientific achievement - and allocating funding.  Yet many open science projects would fail without leadership in these areas.  Contribute what you can and support others who take on these roles.</p>
<p><em>Collaborate.</em>  Open source advocates have been creating tools to help share the work involved in opening research - there’s <a href="http://software-carpentry.org/">Software Carpentry</a>, the <a href="https://osf.io/">Open Science Framework</a>, <a href="http://sagebase.org/platforms-and-services/">Sage Bionetworks</a>, and <a href="http://researchcompendia.org/">Research Compendia</a>, just to name a few.  But beyond sharing tools, we can share time and resources.  Not every researcher will have the skillset, experience, or personality to quickly and easily open up their work.  Sharing efforts across labs, departments and even schools can lighten the load.  So can open science specialists, if we create a scientific culture where these specialists are trained, utilized and valued.</p>
<p>We can and should demand open scientific practices from our colleagues and our institutions.  But we can also provide guidelines, tools, resources and sympathy.  Open science is hard.  Let’s not make it any harder.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/01/08/hard-science/#disqus_thread" data-disqus-identifier="2014/01/08/hard-science/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/01/08/hard-science/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jan  1,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/01/01/open-science-timeline/" rel="bookmark" title="Permanent Link to &quot;Timeline of Notable Open Science Events in 2013 - Psychology&quot;">Timeline of Notable Open Science Events in 2013 - Psychology</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/jon-grahe-pacific-lutheran-university.html" rel="author">Jon Grahe, Pacific Lutheran University</a>

   			   <p>Happy New Year! New Year’s is a great time for reflection and resolution, and when I reflect on 2013, I view it with an air of excitement and promise. As a social psychologist, I celebrated with my many of my colleagues in Washington, DC. at the 25th anniversary of the Association for Psychological Science. There were many celebrations including a ‘80s themed dance night at the Convention. However, this year was also marred by the “Crisis of Confidence” in psychological and broader sciences that has been percolating since the turn of the 21st century. Our timeline begins the year with the <em>Perspectives on Psychological Science</em>’s special issue dedicated to addressing this Crisis. Rather than focusing on the problems, papers in this issue suggested solutions and many of those suggestions emerged as projects in 2013. This timeline focuses on these many Open Science Collaboration successes and initiatives and offers a glimpse at the activity directed at reaching the Scientific Utopia envisioned by so many in the OSC. </p>
<p>Maybe when APS celebrates its 50th Anniversary, it will also mark the 25th Anniversary of the year that the tide turned on the bad practices that had led to the “Crisis of Confidence”. Perhaps in addition to a ‘13 themed dance band playing Lorde’s “Royals” or Imagine Dragon’s “Demons”, maybe there will be a theme reflecting on changing science practices. With the COS celebrating a 25th anniversary of its own, let us share your memory of the important events from 2013. </p>
<p>These posts reflect a limited list of psychology-related events that one person noticed. We invite you to add other notable events that you feel are missing from this list, particularly in other scientific areas. Add a comment below with information about any research projects aimed at replication across institutions or initiatives directed at making science practices more transparent. </p>
<p><a href="http://cdn.knightlab.com/libs/timeline/latest/embed/index.html?source=0An4eLhySzFmBdFV4Wjh0SkZkajU3dEV6b08tV1p4dmc&amp;font=Bevan-PotanoSans&amp;maptype=TERRAIN&amp;lang=en&amp;height=650">View the timeline!</a></p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/01/01/open-science-timeline/#disqus_thread" data-disqus-identifier="2014/01/01/open-science-timeline/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/01/01/open-science-timeline/">Posted at  1:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 18,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/12/18/researcher-degrees-of-freedom/" rel="bookmark" title="Permanent Link to &quot;Researcher Degrees of Freedom in Data Analysis&quot;">Researcher Degrees of Freedom in Data Analysis</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/sean-mackinnon.html" rel="author">Sean Mackinnon</a>

   			   <p>The enormous amount of options available for modern data analysis is both a blessing and a curse. On one hand, researchers have specialized tools for any number of complex questions. On the other hand, we’re also faced with a staggering number of equally-viable choices, many times without any clear-cut guidelines for deciding between them. For instance, I just popped open SPSS statistical software and counted 18 different ways to conduct post-hoc tests for a one-way ANOVA. Some choices are clearly inferior (e.g., the <a href="http://www.graphpad.com/guides/prism/6/statistics/index.htm?stat_fishers_lsd.htm">LSD test</a> doesn’t adjust p-values for multiple comparisons) but it’s possible to defend the use of many of the available options. These ambiguous choice points are sometimes referred to as researcher degrees of freedom.</p>
<p>In theory, researcher degrees of freedom shouldn’t be a problem. More choice is better, right? The problem arises from two interconnected issues: (a) Ambiguity as to which statistical test is most appropriate and (b) an incentive system where scientists are rewarded with publications, grants, and career stability when their p-values fall below the revered p &lt; .05 criterion. So, perhaps unsurprisingly, when faced with a host of ambiguous options for data analysis, most people settle on the one that achieves statistically significant results. Simmons, Nelson, and Simonsohn (2011) argue that this undisclosed flexibility in data analysis allows people to present almost any data as “significant,” and calls for 10 simple guidelines for reviewers and authors to disclose in every paper – which, if you haven’t read yet <a href="http://pss.sagepub.com/content/22/11/1359.full.pdf+html">are worth checking out</a>. In this post, I will discuss a few guidelines of my own for conducting data analysis in a way that strives to overcome our inherent tendency to be self-serving.</p>
<ol>
<li>
<p>Make as many data analytic decisions as possible before looking at your data. Review the statistical literature and decide on which statistical test(s) will be best before looking at your collected data. Continue to use those tests until enough evidence emerges to change your mind. The important thing is that you make these decisions before looking at your data. Once you start playing with the actual data, your self-serving biases will start to kick in. Do not underestimate your ability for self-deception: Self-serving biases are powerful, pervasive, and apply to virtually everyone. Consider pre-registering your data analysis plan (perhaps using the <a href="https://openscienceframework.org/">Open Science Framework</a> to keep yourself honest and to convince future reviewers that you aren’t exploiting researcher degrees of freedom.</p>
</li>
<li>
<p>When faced with a situation where there are too many equally viable choices, run a small number of the best choices, and report all of them. In this case, decide on 2-5 different tests ahead of time. Report the results of all choices, and make a tentative conclusion based if the majority of these tests agree. For instance, when determining model fit in structural equation modeling, there <a href="http://davidakenny.net/cm/fit.htm">many different methods you might use</a>. If you can’t figure out which method is best by reviewing the statistical literature – it’s not entirely clear, statisticians disagree about as often as any other group of scientists – then report the results of all tests, and make a conclusion if they all converge on the same solution. When they disagree, make a tentative conclusion based on the majority of tests that agree (e.g., 2 of 3 tests come to the same conclusion). For the record, I currently use CFI, TLI, RMSEA, and SRMR in my own work, and use these even if other fit indices provide more favorable results.</p>
</li>
<li>
<p>When deciding on a data analysis plan after you’ve seen the data, keep in mind that most researcher degrees of freedom have minimal impact on strong results. For any number of reasons, you might find yourself deciding on a data analysis plan after you’ve played around with the data for a while. At the end of the day, strong data will not be influenced much by researcher degrees of freedom. For instance, results should look much the same regardless of whether you exclude outliers, transform them, or leave them in the data when you have a study with high statistical power. Simmons et al. (2011) specifically recommend that results should be presented (a) with and without covariates, and (b) with and without specific data points excluded, if any were removed. Again, the general idea is that strong results will not change much when you alter researcher degrees of freedom. Thus, I again recommend analyzing the data in a few different ways and looking for convergence across all methods when you’re developing a data analysis plan after seeing the data. This sets the bar higher to try and combat your natural tendency to report just the one analysis that “works.” When minor data analytic choices drastically change the conclusions, this should be a warning sign that your solution is unstable and the results are probably not trustworthy.  The number one reason why you have an unstable solution is probably because you have <a href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/">low statistical power</a>. Since you hopefully had a strict data collection end date, the only viable alternative when results are unstable is to replicate the results in a second, more highly-powered study using the same data analytic approach.</p>
</li>
</ol>
<p>At the end of the day, there is no “quick-fix” for the problem of self-serving biases during data analysis so long as the incentive system continues to reward novel, statistically significant results. However, by using the tips in this article (and elsewhere) researchers can focus on finding strong, replicable results by minimizing the natural human tendency to be self-serving.</p>
<p><strong>References</strong></p>
<p>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). <a href="http://pss.sagepub.com/content/22/11/1359.full.pdf+html">False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant.</a> Psychological Science, 22, 1359-1366. doi:10.1177/0956797611417632</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/12/18/researcher-degrees-of-freedom/#disqus_thread" data-disqus-identifier="2013/12/18/researcher-degrees-of-freedom/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/12/18/researcher-degrees-of-freedom/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://osc.centerforopenscience.org/category/content2.html" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="http://osc.centerforopenscience.org/category/content4.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 3 of 5</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://osc.centerforopenscience.org/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>