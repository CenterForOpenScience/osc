<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>Open Science Collaboration Blog &middot; articles in the "content" category</title>
<!--        <link rel="shortcut icon" href="http://centerforopenscience.github.io/osc/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Open Science Collaboration Blog Atom Feed" />

        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://centerforopenscience.github.io/osc/category/content.html">content</a></li>
                <li><a href="http://centerforopenscience.github.io/osc">Home</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/about.html">About</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/authors.html">Authors</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/policy.html">Policy</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://centerforopenscience.github.io/osc"><img src="http://centerforopenscience.github.io/osc/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Jun 18,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/" rel="bookmark" title="Permanent Link to &quot;Open Science Initiatives promote Diversity, Social Justice, and Sustainability&quot;">Open Science Initiatives promote Diversity, Social Justice, and Sustainability</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jon-grahe.html" rel="author">Jon Grahe</a>

		           	

<p>As I follow the recent social media ruckus centered on replication science questioning motives and methods, it becomes clear that the open science discussion needs to consider the point made by the title of this blog; maybe repeatedly. For readers who weren’t following this, <a href="https://politicalsciencereplication.wordpress.com/2014/05/25/replication-bullying-who-replicates-the-replicators/">this blog</a> by a political scientist and another post from the <a href="http://www.spspblog.org/psychology-news-round-up-may-23rd/">SPSP Blog</a> might be of interest. I invite you to join me in evaluating this argument as the discussion progresses. I contend that “Open Science Initiatives promote Diversity, Social Justice, and Sustainability.” Replication science and registered reports are two Open Science Initiatives and by extension should also promote these ideals. If this is not true, I will abandon this revolution and go back to the status quo. However, I am confident that when considering all the evidence, you will agree with me that these idealistic principles benefit from openness generally and by open science specifically.  </p>
<p>Before suggesting specific mechanisms by which this occurs, I will briefly note that the definitions of <a href="http://en.wikipedia.org/wiki/Open_science">Open Science</a>, <a href="http://en.wikipedia.org/wiki/Diversity">Diversity</a>, <a href="http://en.wikipedia.org/wiki/Social_justice">Social Justice</a>, and <a href="http://en.wikipedia.org/wiki/Sustainability">Sustainability</a> that are listed on Wikipedia are sufficient for this discussion since Wikipedia itself is an Open Science initiative. Also, I would like to convey the challenge of advancing each of these simultaneously. My own institution, Pacific Lutheran University (PLU), in our recent long range plan, <a href="http://issuu.com/pacific.lutheran.university/docs/plu-2020?e=1067239/2651397">PLU2020</a>, highlighted the importance of uplifting each of these at our own institution as introduced on page 11, “As we discern our commitments for the future, we reaffirm the ongoing commitments to diversity, sustainability, and justice that already shape our contemporary identity, and we resolve to integrate these values ever more intentionally into our mission and institution.” This is easier said than done because at times the goals of these ideals sometimes conflict. For instance, the environmental costs of feeding billions of people and heating their homes are enormous. Sometimes valuing diversity (such as scholarships targeted for people of color) seems unjust because resources are being assigned unevenly. These tensions can be described with many examples across numerous goals in all three dimensions and highlight the need to make balanced decisions.  </p>
<p>PLU has not yet resolved this challenge in uplifting all three simultaneously, but I hope that we succeed as we continue the vigorous discussion. Why each is important should be considered from is a Venn diagram on the sustainability Wikipedia page showing <a href="http://en.wikipedia.org/wiki/File:Sustainable_development.svg">sustainable development</a> as intersections between three pillars of sustainable development, social (people), economic, and environmental because even sustainability itself represents competing interests. Diversity and Social Justice are both core aspects of the social dimension, where uplifting diversity highlights the importance of distinct ideas and cultures and helps us understand why people and their varied ideas, in addition to oceans and forests are important resources of our planet. The ideals of social justice aim to provide mechanisms such that all members of our diverse population receive and contribute our fair share of these resources. Because resources are limited and society complex and flawed, these ideals are often more aspirational rather than practical. However, the basic premise of uplifting all three is that we are better when valuing diversity, providing social justice, and sustainably using the planet’s resources (people, animals, plants, and rocks). Below I provide examples for how OSIs promote each of these principles while illustrating why each is important to science.   </p>
 <a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/#disqus_thread" data-disqus-identifier="2014/06/18/osi-promote-dsjs/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jun 11,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/11/thoughts-on-this-debate/" rel="bookmark" title="Permanent Link to &quot;Thoughts on this debate about social scientific rigor&quot;">Thoughts on this debate about social scientific rigor</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/betsy-levy-paluck.html" rel="author">Betsy Levy Paluck</a>

   			   <p><em>This article was originally posted on Betsy Levy Paluck's <a href="http://www.betsylevypaluck.com/blog/2014/5/25/what-i-stand-for-in-this-discussion-about-scientific-rigor">website</a>.</em></p>
<p>On his terrific <a href="http://hardsci.wordpress.com/">blog</a>, Professor Sanjay Srivastava <a href="http://hardsci.wordpress.com/2014/05/25/does-the-replication-debate-have-a-diversity-problem/">points out</a> that the current (vitriolic) debate about replication in psychology has been "salted with casually sexist language, and historically illiterate" arguments, on both sides. I agree, and thank him for pointing this out.  </p>
<p>I'd like to add that I believe academics participating in this debate should be mindful of co-opting powerful terms like <em>bullying</em> and <em>police</em> (e.g., the "replication police") to describe the replication movement. Why? Bullying behavior describes repeated abuse from a person of higher power and influence. Likewise, many people in the US and throughout the world have a well-grounded terror of police abuse. The terror and power inequality that these terms connote is diminished when we use it to describe the experience of academics replicating one another's studies. Let's keep evocative language in reserve so that we can use it to name and change the experience of truly powerless and oppressed people.   </p>
<p><strong>Back to replication.</strong> Here is the thing: we all believe in the principle of replication. As scientists and as psychologists, we are all here because we wish to contribute to cumulative research that makes progress on important psychological questions. This desire unites us.</p>
<p>So what's up?  </p>
<p>It seems to me that some people oppose the current wave of replication efforts because they do not like the tenor of the recent public discussions. As I already mentioned, neither do I. I'm bewildered by the vitriol. Just a few days ago, one of the most prominent modern economists, currently an internationally bestselling author, had his <a href="http://www.ft.com/cms/s/2/e1f343ca-e281-11e3-89fd-00144feabdc0.html#axzz32mFmDac2">book called into question</a> over alleged data errors in a spreadsheet that he made public. His <a href="http://blogs.ft.com/money-supply/2014/05/23/piketty-response-to-ft-data-concerns/?Authorised=false">response</a> was cordial and curious; his colleagues followed up with <a href="http://www.nytimes.com/2014/05/25/upshot/a-new-critique-of-piketty-has-its-own-shortcomings.html?_r=0">care</a>, <a href="http://www.nytimes.com/2014/05/24/upshot/did-piketty-get-his-math-wrong.html?smid=tw-upshotnyt">equanimity</a>, and respect.   </p>
<p>Are we really being taught a lesson in manners from economists? Is that happening?  </p>
<p>As one of my favorite TV characters said recently ...  </p>
<p>If we don't like the tenor of the discussion about replication, registration, etc., let's change it.  </p>
<p>In this spirit, I offer a brief description of what we are doing in my lab to try to make our social science rigorous, transparent, and replicable. It's one model for your consideration, and we are open to suggestions.  </p>
<p>For the past few years we have <strong>registered analysis plans</strong> for every new project we start. (They can be found <a href="http://e-gap.org/design-registration/registered-designs/">here</a> on the <a href="http://e-gap.org/">EGAP</a> website; this is a group to which I belong. EGAP has had great discussions in partnership with <a href="http://cega.berkeley.edu/programs/BITSS/">BITSS</a> about transparency.) My lab's analysis registrations are accompanied by a <strong>codebook</strong> describing each variable in the dataset.  </p>
<p>I am happy to say that we are just starting to get better at producing <strong>replication code</strong> and <strong>data &amp; file organization that is sharing-ready</strong> as we do the research, rather than trying to reconstruct these things from our messy code files and Dropbox disaster areas following publication (for this, I thank my <a href="http://www.betsylevypaluck.com/collaborators/">brilliant students</a>, who surpass me with their coding skills and help me to keep things organized and in place. See also <a href="http://faculty.chicagobooth.edu/matthew.gentzkow/research/CodeAndData.pdf">this</a>). What a privilege and a learning experience to have graduate students, right? Note that <a href="http://personxsituation.wordpress.com/2014/05/25/im-disappointed-a-graduate-students-perspective/">they are listening to us</a> have this debate.  </p>
<p>Margaret Tankard, Rebecca Littman, Graeme Blair, Sherry Wu, Joan Ricart-Huguet, Andreana Kenrick (awesome grad students), and Robin Gomila and David Mackenzie (awesome lab managers) have all been writing analysis registrations, organizing files, checking data codebooks, and writing replication code for the experiments we've done in the past three years, and colleagues Hana Shepherd, Peter Aronow, Debbie Prentice, and Eldar Shafir are doing the same with me. Thank goodness for all these amazing and dedicated collaborators, because one reason I understand replication to be so difficult is that it is a huge challenge to reconstruct what you thought and did over a long period of time, without careful record keeping (note: analysis registration also serves that purpose for us!).  </p>
<p>Previously, I <strong>posted data</strong> at Yale's ISPS archive, and for other datasets made them available on request if I thought I was going to work more on them. But in future we plan to post all published data plus the dataset's codebook. Economist and political scientists friends often post to their personal websites. Another possibility is posting in digital archives (like Yale's, but there are others: I follow @<a href="https://twitter.com/annthegreen">annthegreen</a> for updates on digital archiving).  </p>
<p>I owe so much of my appreciation for these practices to my advisor <a href="https://sites.google.com/site/donaldpgreen/">Donald Green</a>. I've also learned a lot from <a href="http://www.columbia.edu/~mh2245/">Macartan Humphreys</a>.  </p>
<p>I'm interested in how we can be better. I'm listening to the constructive debates and to the suggestions out there. If anyone has questions about our current process, please leave a comment below! I'd be happy to answer questions, provide examples, and to take suggestions. </p>
<p>It costs nothing to do this--but it slows us down. Slowing down is not a bad thing for research (though I recognize that a bad heuristic of quantity = quality still dominates our discipline). During registration, we can stop to think-- <em>are we sure we want to predict this? With this kind of measurement? Should we go back to the drawing board about this particular secondary prediction?</em> I know that if I personally slow down, I can oversee everything more carefully. I'm learning how to say no to new and shiny projects. </p>
<p>I want to end on the following note. I am now tenured. If good health continues, I'll be on hiring committees for years to come. In a hiring capacity, I will appreciate applicants who, though they do not have a ton of publications, can link their projects to an online analysis registration, or have posted data and replication code. Why? I will infer that they were slowing down to do very careful work, that they are doing their best to build a cumulative science. I will also appreciate candidates who have conducted studies that "failed to replicate" and who responded to those replication results with follow up work and with thoughtful engagement and curiosity (I have read about <a href="http://www.talyarkoni.org/blog/2013/12/27/what-we-can-and-cant-learn-from-the-many-labs-replication-project/">Eugene Caruso's response</a> and thought that he is a great model of this kind of response).</p>
<p>I say this because it's true, and also because some academics report that their graduate students are very nervous about how replication of their lab's studies might <a href="http://www.spspblog.org/simone-schnall-on-her-experience-with-a-registered-replication-project/">ruin their reputations on the job market</a> (see Question 13). I think the concern is understandable, so it's important for those of us in these lucky positions to speak out about what we value and to allay fears of punishment over non-replication (see Funder: <a href="http://funderstorms.wordpress.com/2012/10/31/the-perilous-plight-of-the-non-replicator/">SERIOUSLY NOT OK</a>). </p>
<p>In sum, I am excited by efforts to improve the transparency and cumulative power of our social science. I'll try them myself and support newer academics who engage in these practices. Of course, we need to have good ideas as well as good research practices (ugh--this business is not easy. Tell that to your friends who think that you've chosen grad school as a shelter from the bad job market). </p>
<p>I encourage all of my colleagues, and especially colleagues from diverse positions in academia and from underrepresented groups in science, to comment on what they are doing in their own research and how they are affected by these ideas and practices. Feel free to post below, post on (real) blogs, write letters to the editor, have conversations in your lab and department, or <a href="https://twitter.com/betsylevyp">tweet</a>. I am listening. Thanks for reading.</p>
<p>*</p>
<p><em>A collection of comments I've been reading about the replication debate, in case you haven't been keeping up. Please do post more links below, since this isn't comprehensive.</em></p>
<p><a href="http://personxsituation.wordpress.com/2014/05/25/im-disappointed-a-graduate-students-perspective/">I'm disappointed: a graduate student's perspective</a></p>
<p><a href="http://hardsci.wordpress.com/2014/05/25/does-the-replication-debate-have-a-diversity-problem/">Does the replication debate have a diversity problem?</a></p>
<p><a href="https://osf.io/98tkv/">Replications of Important Results in Social Psychology: Special Issue of Social Psychology</a></p>
<p><a href="http://funderstorms.wordpress.com/2012/10/31/the-perilous-plight-of-the-non-replicator/">The perilous plight of the (non)-replicator</a></p>
<p><a href="http://politicalsciencereplication.wordpress.com/2014/05/25/replication-bullying-who-replicates-the-replicators/">"Replication Bullying": Who replicates the replicators?</a></p>
<p><a href="http://davidjjohnson.wordpress.com/2014/05/25/rejoinder-to-schnall-2014/">Rejoinder to Schnall (2014) in Social Psychology</a></p>
<p><a href="https://docs.google.com/document/d/1ew7X0RaClU5_Ev4Ns3Uyn0I7PmjzP_Z1wKlnza_3Fe0/edit">Context and Correspondence for Special Issue of Social Psychology</a></p>
<p><a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/">Behavioral Priming: Time to Nut Up or Shut Up</a></p>
<p><strong>Tweets:</strong></p>
<ul>
<li><a href="https://twitter.com/DanTGilbert/status/470436673697095680">@DanTGilbert: "Simone Schnall's expose of the replication police..."</a></li>
<li><a href="https://twitter.com/BrianNosek/status/470563826274807808">@BrianNosek: "A remarkable set of public comments..."</a></li>
<li><a href="https://twitter.com/DavidFunder/status/470316176627613696">@DavidFunder: "3 terms I learned in past 24 hrs..."</a></li>
</ul>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/11/thoughts-on-this-debate/#disqus_thread" data-disqus-identifier="2014/06/11/thoughts-on-this-debate/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/11/thoughts-on-this-debate/">Posted at  4:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jun  5,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/05/op-open-humans/" rel="bookmark" title="Permanent Link to &quot;Open Projects - Open Humans&quot;">Open Projects - Open Humans</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This article is the second in <a href="http://osc.centerforopenscience.org/tag/open-projects.html">a series highlighting open science projects</a> around the community. You can read the interview this article was based on: <a href="https://docs.google.com/document/d/1tQuOFme5EQbNkcGBCc1rr7FO9SWEEufPwhhJ59JXBUM/edit">edited for clarity</a>, <a href="https://docs.google.com/document/d/1c2xMBMEr5m8a3wR7Om5mPAUgsz_JBPudlYsnSEbH0lY/edit">unedited</a>.</em>  </p>
<p>While many researchers encounter no privacy-based barriers to releasing data, those working with human participants, such as doctors, psychologists, and geneticists, have a difficult problem to surmount. How do they reconcile their desire to share data, allowing their analyses and conclusions to be verified, with the need to protect participant privacy? It's a dilemma we've talked about before on the blog (see: <a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/">Open Data and IRBs</a>, <a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/">Privacy and Open Data</a>). A new project, Open Humans, seeks to resolve the issue by finding patients who are willing - even eager - to share their personal data.  </p>
<p>Open Humans, which recently won a $500,000 grant <a href="http://blog.personalgenomes.org/2014/01/14/open-humans-network-wins-knight-news-challenge-health-award/">from the Knight Foundation</a>, grew out of the <a href="http://www.personalgenomes.org/">Personal Genome Project</a>. Founded in 2005 by Harvard genetics professor <a href="http://arep.med.harvard.edu/gmc/">George Church</a>, the Personal Genome Project sought to solve a problem that many genetics researchers had yet to recognize. "At the time people didn't really see genomes as inherently identifiable," Madeleine Price Ball explains. Ball is co-founder of OpenHumans, Senior Research Scientist at PersonalGenomes.org, and Director of Research at the Harvard Personal Genome Project.  She quotes from <a href="http://www.1000genomes.org/">1000 Genomes</a>' <a href="http://www.1000genomes.org/sites/1000genomes.org/files/docs/Informed%20Consent%20Form%20Template.pdf">informed consent form</a>: "'Because of these measures, it will be very hard for anyone who looks at any of the scientific databases to know which information came from you, or even that any information in the scientific databases came from you.'"  </p>
<p>"So that's sort of the attitude scientists had towards genomes at the time. Also, the Genetic Information Nondiscrimination Act didn't exist yet. And there was GATTACA. Privacy was still this thing everyone thought they could have, and genomes were this thing people thought would be crazy to share in an identifiable manner. I think the scientific community had a bit of unconscious blindness, because they couldn't imagine an alternative."  </p>
<p>Church found an initial <a href="https://en.wikipedia.org/wiki/Personal_Genome_Project">ten participants</a> - the list includes university professors, health care professionals, and Church himself. The IRB interviewed each of the participants to make sure they truly understood the project and, satisfied, allowed it to move forward. The Personal Genome Project now boasts <a href="https://my.pgp-hms.org/users/">over 3,400 participants</a>, each of whom have passed an entrance exam showing that they understand what will happen to their data, and the risks involved. Most participants are enthusiastic about sharing. One participant described it as "donating my body to science, but I don't have to die first".   </p>
<p>The Personal Genome Project's expansion hasn't been without growing pains. "We've started to try to collect data beyond genomes." Personal health information, including medical history, procedures, test results, prescriptions, has been provided by a subset of participants. "Every time one of these new studies was brought before the IRB they'd be like ‘what? that too?? I don't understand what are you doing???' It wasn't scaling, it was confusing, the PGP was trying to collect samples and sequence genomes <em>and</em> it was trying to let other groups collect samples and do other things."  </p>
<p>Thus, Open Humans was born. "Open Humans is an abstraction that takes part of what the PGP was doing (the second part) and make it scalable," Ball explains. "It's a cohort of participants that demonstrate an interest in public data sharing, and it's researchers that promise to return data to participants."  </p>
<p>Open Humans will start out with a number of participants and an array of public data sets, thanks to collaborating projects <a href="http://humanfoodproject.com/americangut/">American Gut</a>, <a href="https://flunearyou.org/">Flu Near You</a>, and of course, the Harvard Personal Genome Project. Participants share data and, in return, researchers promise to share results. What precisely "sharing results" means has yet to be determined. "We're just starting out and know that figuring out how this will work is a learning process," Ball explains. But she's already seen what can happen when participants are brought into the research process - and brought together:  </p>
<p>"One of the participants made an online forum, another a Facebook group, and another maintains a LinkedIn group… before this happened it hadn't occurred to me that abandoning the privacy-assurance model of research could empower participants in this manner. Think about the typical study - each participant is isolated, they never see each other. Meeting each other could breach confidentiality! Here they can talk to each other and <em>gasp</em> complain about you. That's pretty empowering." Ball and her colleague Jason Bobe, Open Humans co-founder and Executive Director of PersonalGenomes.org, hope to see all sorts of collaborations between participants and researchers. Participants could help researchers refine and test protocols, catch errors, and even provide their own analyses.  </p>
<p>Despite these dreams, Ball is keeping the project grounded. When asked whether Open Humans will require articles published using their datasets to be made open access, she replies that, "stacking up a bunch of ethical mandates can sometimes do more harm than good if it limits adoption". Asked about the effect of participant withdrawals on datasets and reproducibility, she responds, "I don't want to overthink it and implement things to protect researchers at the expense of participant autonomy based on just speculation." (It <em>is</em> mostly speculation. Less than 1% of Personal Genome Project users have withdrawn from the study, and none of the participants who've provided whole genome or exome data have done so.)  </p>
<p>It's clear that Open Humans is focused on the road directly ahead. And what does that road look like?  "Immediately, my biggest concern is building our staff. Now that we won funding, we need to hire a good programmer... so if you are or know someone that seems like a perfect fit for us, please pass along <a href="http://openhumans.org/#now_hiring">our hiring opportunities</a>". She adds that anyone can <a href="http://openhumans.org/">join the project's mailing list</a> to get updates and find out when Open Humans is open to new participants - and new researchers. "And just talk about us. Referring to us is an intangible but important aspect for helping promote awareness of participant-mediated data sharing as a participatory research method and as a method for creating open data."  </p>
<p>In other words: start spreading the news.  Participant mediated data isn't the only solution to privacy issues, but it's an enticing one - and the more people who embrace it, the better a solution it will be.  </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/05/op-open-humans/#disqus_thread" data-disqus-identifier="2014/06/05/op-open-humans/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/05/op-open-humans/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/open-projects.html">open-projects</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 29,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/" rel="bookmark" title="Permanent Link to &quot;Questions and Answers about the Förster case&quot;">Questions and Answers about the Förster case</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/denny-borsboom-han-van-der-maas-eric-jan-wagenmakers-department-of-psychological-methods-uva.html" rel="author">Denny Borsboom, Han van der Maas, Eric-Jan Wagenmakers, Department of Psychological Methods, UvA</a>

		           	

<p>By now, everyone is probably familiar with the recent investigation of the work of Dr. Förster, in which the Landelijk Orgaan Wetenschappelijke Integriteit (LOWI) concluded that data reported in a paper by Dr. Förster had been manipulated. In his reaction to the newspaper article NRC Dr. Förster suggested that our department would be involved in a witch-hunt. This is incorrect.  </p>
<p>However, we have noticed that there are many questions about both the nature of the case and the procedure followed. We have compiled the following list of questions and answers to explain what happened. If any other questions arise, feel free to email them to us so we can add them to this document.  </p>
<p><strong>Q: What was the basis of the allegations against Dr. Förster?</strong><br />
A: In every single one of 40 experiments, reported across three papers, the means of two experimental conditions (“local focus” and “global focus”) showed almost exactly opposite behavior with respect to the control condition. So whenever the local focus condition led to a one-point increase of the mean level of the dependent variable compared to the control condition, the global condition led almost exactly to a one-point decrease. Thus, the samples exhibit an unrealistic level of linearity.  </p>
<p><strong>Q: Couldn’t the effects actually be linear in reality?</strong><br />
A: Yes, that is unlikely but possible. However, in addition to the perfect linearity of the effects themselves, there is far too little variance in the means of the conditions, given the variance that is present within the conditions. In other words: the means across the conditions follow the linear pattern (much) too perfectly. To show this, the whistleblower’s complaint computed the probability of finding this level of linearity (or even more perfect linearity) in the samples researched, under the assumption that, in reality, the effect is linear in the population. That probability equals 1/508,000,000,000,000,000,000.   </p>
 <a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/#disqus_thread" data-disqus-identifier="2014/05/29/forster-case/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/">Posted at  3:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 28,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/28/train-wreck-prevention/" rel="bookmark" title="Permanent Link to &quot;The etiquette of train wreck prevention&quot;">The etiquette of train wreck prevention</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jp-de-ruiter-bielefeld-university.html" rel="author">JP de Ruiter, Bielefeld University</a>

   			   <p>In a famous <a href="http://www.nature.com/polopoly_fs/7.6716.1349271308!/suppinfoFile/Kahneman%20Letter.pdf">open letter</a> to scientists , Daniel Kahneman, seeing “a train wreck looming”, argued that social psychologists (and presumably, especially those who are publishing social priming effects) should engage in systematic and extensive replication studies to avoid a loss of credibility in the field. The fact that a Nobel Prize winning psychologist made such a clear statement gave a strong boost of support to systematic replication efforts in social psychology (see Pashler &amp; Wagenmakers 2012, and their special issue in <em>Psychological Science</em>).  </p>
<p>But in a more recent <a href="http://www.scribd.com/doc/225285909/Kahneman-Commentary">commentary</a>, Kahneman appears to have changed his mind, and argues that “current norms allow replicators too much freedom to define their study as a direct replication of previous research”, and that the “seemingly reasonable demand” of requiring method sections to be so precise that they enable direct replications is “rarely satisfied in psychology, because behavior is easily affected by seemingly irrelevant factors”. A similar argument was put forth by Simone Schnall, who recently <a href="http://www.psychol.cam.ac.uk/cece/blog">wrote</a> that “human nature is complex, and identical materials will not necessarily have the identical effect on all people in all contexts”.  </p>
<p>While I wholeheartedly agree with Kahneman’s original letter on this topic, I strongly disagree with his commentary, for reasons that I will outline here.  </p>
<p>First, he argues (as Schnall did too) that there always are potentially influential differences between the original study and the replication attempt. But this would imply that any replication study, no matter how meticulously performed, would be meaningless. (Note that this also holds for <em>successful</em> replication studies.) This is a clear case of a <em>reductio ad absurdum</em>.  </p>
<p>The main reason why this argument is flawed is that there is a fundamental relationship between the theoretical claim based on a finding and its proper replication, which is the topic of an interesting discussion about the degree to which a replication should be similar to the study it addresses (see Stroebe &amp; Strack, 2014; Simons, 2014; Pashler &amp; Harris, 2012). My position in this debate is the following. The more general the claim that the finding is claimed to support, the more “conceptual” the replication of the supporting findings can (and should) be. Suppose we have a finding F that we report in order to claim evidence for scientific claim C. In the case that C is <em>identical</em> to F, such that C is a claim of the type “The participants in our experiment did X at time T in location L”, it is indeed impossible to do any type of replication study, because the exact circumstances of F were unique and therefore by definition irreproducible. But in this case (that F = C), C has obviously no generality at all, and is therefore scientifically not very interesting. In such a case, there would also be no point in doing inferential statistics. If, on the other hand, C is more general than F, the level of methodological detail that is provided should be sufficient to enable readers to attempt to replicate the finding, allowing for variation that the authors do not consider important. If the authors remark that this result arises under condition A but acknowledge that it might not arise under condition A' (let's say, with participants who are aged 21-24 rather than 18-21), then clearly a follow-up experiment under condition A' isn't a valid replication. But if their claim (explicitly or implicitly) is that it doesn't matter whether condition A or A' is in effect, then a follow-up study involving condition A' might well be considered a replication. The failure to specify any particular detail might reasonably be considered an implicit claim that this detail is not important.  </p>
<p>Second, Kahnemann is worried that even the rumor of a failed replication could damage the reputation of the original authors. But if researchers attempt to do a replication study, this does not imply that they believe or suggest that the original author was cheating. Cheating does occasionally happen, sadly, and replication studies are a good way to catch these cases. But, assuming that cheating is not completely rampant, it is much more likely that a finding cannot be replicated successfully because variables or interactions have been overlooked or not controlled for, that there were unintentional errors in the data collection or analysis, or because the results were simply a fluke, caused by our standard statistical practices severely overestimating evidence against the null hypothesis (Sellke, Bayarri &amp; Berger, 2001; Johnson, 2013).  </p>
<p>Furthermore, replication studies are not hostile or friendly. People are. I think it is safe to say that we all dislike uncollegial behavior and rudeness, and we all agree that it should be avoided. If Kahneman wants to give us a stern reminder that it is important for replicators to contact the original authors, then I support that, even though I personally suspect that the vast majority of replicators already do that. There already is etiquette in place in experimental psychology, and as far as I can tell, it’s mostly being observed. And for those cases where it is not, my impression is that the occasional unpleasant behavior originates not only from replicators, but also from original authors.  </p>
<p>Another point I would like to address is the asymmetry of the relationship between author and replicator. Kahneman writes: “The relationship is also radically asymmetric: the replicator is in the offense, the author plays defense.” This may be true in some sense, but it is counteracted by other asymmetries that work in the opposite direction: The author has already successfully published the finding in question and is reaping the benefits of it. The replicator, however, is up against the strong reluctance of journals to publish replication studies, is required to have a much higher statistical power (hence invest far more resources), and is often arguing against a moving target, as more and more newly emerging and potentially relevant details of the original study can be brought forward by the original authors.  </p>
<p>A final point: the problem that started the present replication discussion was that a number of findings that were deemed both important and implausible by many researchers failed to replicate. The defensiveness of the original authors of these findings is understandable, but so is the desire of skeptics to investigate if these effects are in fact reliable. I, both as a scientist and as a human being, <em>really want to know</em> if I can boost my creativity by putting an open box on my desk (Leung et al., 2012) or if the fact that I frequently take hot showers could be caused by loneliness (Bargh &amp; Shalev, 2012). As Kahneman himself rightly put it in his original open letter: “The unusually high openness to scrutiny may be annoying and even offensive, but it is a small price to pay for the big prize of restored credibility.”  </p>
<p><strong>References</strong></p>
<p>Bargh, J. A., &amp; Shalev, I. (2012). The substitutability of physical and social warmth in daily life. <em>Emotion</em>, 12(1), 154. doi:10.1037/a0023527</p>
<p>Johnson, V. E. (2013). Revised standards for statistical evidence. <em>Proceedings of the National Academy of Sciences</em>, 110(48), 19313-19317. doi: doi/10.1073/pnas.1313476110</p>
<p>Leung, A. K.-y., Kim, S., Polman, E., Ong, L. S., Qiu, L., Goncalo, J. A., et al. (2012). Embodied metaphors and creative "acts". <em>Psychological Science</em>, 23(5), 502-509. doi:10.1177/0956797611429801</p>
<p>Pashler, H., &amp; Harris, C. R. (2012). Is the replicability crisis overblown? Three arguments examined. <em>Perspectives on Psychological Science</em>, 7(6), 531-536. doi:10.1177/1745691612463401</p>
<p>Pashler, H., &amp; Wagenmakers, E.-J. (2012). Editors' Introduction to the Special Section on Replicability in Psychological Science A Crisis of Confidence? <em>Perspectives on Psychological 
Science</em>, 7(6), 528-530. doi:10.1177/1745691612465253</p>
<p>Sellke, T., Bayarri, M., &amp; Berger, J. O. (2001). Calibration of p values for testing precise null hypotheses. <em>The American Statistician</em>, 55(1), 62-71. doi:10.1198/000313001300339950</p>
<p>Simons, D. J. (2014). The Value of Direct Replication. <em>Perspectives on Psychological Science</em>, 9(1), 76-80. doi:10.1177/1745691613514755</p>
<p>Stroebe, W., &amp; Strack, F. (2014). The alleged crisis and the illusion of exact replication. <em>Perspectives on Psychological Science</em>, 9(1), 59-71. doi:10.1177/1745691613514450 </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/28/train-wreck-prevention/#disqus_thread" data-disqus-identifier="2014/05/28/train-wreck-prevention/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/28/train-wreck-prevention/">Posted at  1:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 20,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/20/clinical-trials-day/" rel="bookmark" title="Permanent Link to &quot;Support Publication of Clinical Trials for International Clinical Trials Day&quot;">Support Publication of Clinical Trials for International Clinical Trials Day</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p>Today is International Clinical Trials Day, held on May 20th in honor of George Lind, the famous Scottish physician who began one of the world's first clinical trials on May 20th, 1747.  This trial discovered that vitamin C deficiency was the cause of scurvy.  While it and the other life-saving trials that have been conducted in the last two hundred and sixty seven years are surely worth celebration, International Clinical Trials Day is also a time to reflect on the problems that plague the clinical trials system.  In particular, the lack of reporting on nearly half of all clinical trials has potentially deadly consequences.</p>
<p>The AllTrials campaign, launched in January 2013, aims to have all past and present clinical trials registered and reported.  From the AllTrials campaign website:</p>
<blockquote>
<p>Doctors and regulators need the results of clinical trials to make informed decisions about treatments.</p>
<p>But companies and researchers can withhold the results of clinical trials even when asked for them. The best available evidence shows that about half of all clinical trials have never been published, and trials with negative results about a treatment are much more likely to be brushed under the carpet.</p>
<p>This is a serious problem for evidence based medicine because we need all the evidence about a treatment to understand its risks and benefits. If you tossed a coin 50 times, but only shared the outcome when it came up heads and you didn’t tell people how many times you had tossed it, you could make it look as if your coin always came up heads. This is very similar to the absurd situation that we permit in medicine, a situation that distorts the evidence and exposes patients to unnecessary risk that the wrong treatment may be prescribed.</p>
<p>It also affects some very expensive drugs. Governments around the world have spent billions on a drug called Tamiflu: the UK alone spent £500 million on this one drug in 2009, which is 5% of the total £10bn NHS drugs budget. But Roche, the drug’s manufacturer, published fewer than half of the clinical trials conducted on it, and continues to withhold important information about these trials from doctors and researchers. So we don’t know if Tamiflu is any better than paracetamol.  (<em>Author's note: in April 2014 <a href="http://www.cochrane.org/features/tamiflu-relenza-how-effective-are-they">a review based on full clinical trial data</a> determined that Tamiflu was almost entirely ineffective.</em>)</p>
<p>Initiatives have been introduced to try to fix this problem, but they have all failed. Since 2008 in the US the FDA has required results of all trials to be posted within a year of completion of the trial. However an audit published in 2012 has shown that 80% of trials failed to comply with this law. Despite this fact, no fines have ever been issued for non-compliance. In any case, since most currently used drugs came on the market before 2008, the trial results that are most important for current medical practice would not have been released even if the FDA’s law was fully enforced.</p>
<p>We believe that this situation cannot go on. The AllTrials initiative is campaigning for the publication of the results (that is, full clinical study reports) from all clinical trials – past, present and future – on all treatments currently being used.</p>
<p>We are calling on governments, regulators and research bodies to implement measure to achieve this.</p>
<p>And we are calling for all universities, ethics committees and medical bodies to enact a change of culture, recognise that underreporting of trials is misconduct and police their own members to ensure compliance.</p>
</blockquote>
<p>You can learn more about the problem of missing clinical trial data in <a href="http://www.alltrials.net/wp-content/uploads/2013/01/Missing-trials-briefing-note.pdf">this brief</a>.  AllTrials also provides slides on this issue to incorporate into talks and presentations as well as a <a href="http://www.alltrials.net/2013/get-involved/">petition you can sign</a>.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/20/clinical-trials-day/#disqus_thread" data-disqus-identifier="2014/05/20/clinical-trials-day/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/20/clinical-trials-day/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 15,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/" rel="bookmark" title="Permanent Link to &quot;How anonymous peer review fails to do its job and damages science.&quot;">How anonymous peer review fails to do its job and damages science.</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jp-de-ruiter-bielefeld-university.html" rel="author">JP de Ruiter, Bielefeld University</a>

		           	

<p>Churchill believed that democracy was the “worst form of government except all those other forms that have been tried from time to time.” Something analogous is often said about anonymous peer review (APR) in science: “it may have its flaws, but it’s the ‘least bad’ of all possible systems.” In this contribution, I present some arguments to the contrary. I believe that APR is threatening scientific progress, and therefore that it urgently needs to be fixed.  </p>
<p>The reason we have a review system in the first place is to uphold basic standards of scientific quality. The two main goals of a review system are to minimize both the number of bad studies that are accepted for publication and the number of good studies that are rejected for publication. Borrowing terminology of signal detection theory, let’s call these false positives and false negatives respectively.  </p>
<p>It is often implicitly assumed that minimizing the number of false positives is the primary goal of APR. However, signal detection theory tells us that reducing the number of false positives inevitably leads to an increase in the rate of false negatives. I want to draw attention here to the fact that the cost of false negatives is both invisible and potentially very high. It is invisible, obviously, because we never get to see the good work that was rejected for the wrong reasons. And the cost is high, because it removes not only good papers from our scientific discourse, but also entire scientists. I personally know a number of very talented and promising young scientists who first sent their work to a journal, fully expecting to be scrutinized, but then receiving reviews that were so personal, rude, scathing, and above all, unfair, that they decided to look for another profession and never looked back. I also know a large number of talented young scientists who are still in the game, but who suffer intensely every time they attempt to publish something and get trashed by anonymous reviewers. I would not be surprised if they also leave academia soon. The inherent conservatism in APR means that people with new, original approaches to old problems run the risk of being shut out, humiliated, and consequently chased away from academia. In the short term, this is to the advantage of the established scientists who do not like their work to be challenged. In the long run, this is obviously very damaging for science. This is especially true of the many journals that will only accept papers that receive unanimously positive reviews. These journals are not facilitating scientific progress, because work with even the faintest hint of controversy is almost automatically rejected.  </p>
<p>With all this in mind, it is somewhat surprising that APR <em>also</em> fails to keep out many obviously bad papers.  </p>
 <a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/#disqus_thread" data-disqus-identifier="2014/05/15/anonymous-peer-review/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/">Posted at 11:30 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May  7,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/07/selecting-for-fraud/" rel="bookmark" title="Permanent Link to &quot;When Science Selects for Fraud&quot;">When Science Selects for Fraud</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This post is in response to <a href="http://osc.centerforopenscience.org/2014/05/02/avoiding-a-witch-hunt/">Jon Grahe's recent article</a> in which he invited readers to propose metaphors that might help us understand why fraud occurs and how to prevent it.</em></p>
<p>Natural selection is the process by which populations change as individual organisms succeed or fail to adapt to their environments.  It is also an apt metaphor for how human cultures form and thrive.  The scientific community, broadly speaking, selects for a number of personality traits, and those traits are more common among scientists than in the general population.  In some cases, this is necessary and beneficial.  In other cases, it is tragic.  </p>
<p>The scientific community selects for curiosity.  Not every scientist is driven by a deep desire to understand the natural world, but so many are.  How boring would endless conferences, lab meetings, and lectures be if one didn’t delight in asking questions and figuring out answers.  It also selects for a certain kind of analytical thinking.  Those who can spot a confound or design a carefully controlled experiment are more likely to succeed.  And it selects for perseverance.  Just ask the researchers who work late into the night running gels, observing mice, or analyzing data.  </p>
<p>The scientific community, like the broader culture of which it is a part, sometimes selects unjustly.  It selects for the well-off: those who can afford the kind of schools where a love of science is cultivated rather than ignored or squashed, those who can volunteer in labs because they don’t need to work to support themselves and others, those who can pay $30 to read a journal article.  It selects for white men: those who don’t have to face conscious and unconscious discrimination, cultural stereotyping, and microaggressions.  </p>
<p>Of particular relevance right now is the way the scientific community selects for fraud.  If asked, most scientists would say that the ideal scientist is honest, open-minded, and able to accept being wrong.  But we do not directly reward these attributes.  Instead, success - publication of papers, grant funding, academic positions and tenure, the approbation of our peers - is too often based on a specific kind of result.  We reward those who can produce novel and positive results.  We don’t reward based on how they produce them.  </p>
<p>This does give an advantage to those with good scientific intuitions, which is a reasonable thing to select for.  It also gives an advantage to risk-takers, those willing to risk their careers on being right.  The risk averse?  They have two options: to drop out of scientific research, as I did, or to commit fraud in order to ensure positive results, as Diederik Stapel, Marc Hauser and Jens Foster did.  Among the risk-averse, those who are unwilling to do shoddy or unethical science are selected against.  Those who are willing are selected for, and often reach the tops of their fields.  </p>
<p>One of the more famous examples of natural selection is the peppered moth of England.  Before the Industrial Revolution, these moths were lightly colored, allowing them to blend in with the light gray bark of the average tree.  During the Industrial Revolution, extreme pollution painted the trees of England black with soot.  To adapt, peppered moths evolved dark, soot-colored wings.  </p>
<p>We can censure the individuals who commit fraud, but this is like punishing the peppered moth for its dirty wings.  As long as success in the scientific community is measured by results and not process, we will continue to select for those willing to violate process in order to ensure results.  Our species, the scientists, need to change our environment if we want to evolve past fraud.  </p>
<p><img src="images/moth.jpg" alt="Photo of Jon Grahe" align="center" width="600px" />
<a href="https://www.flickr.com/photos/dhobern/7522055588/in/photolist-bwwkzi-a2zqz8-a2CkHA-4RecUF-6uwtBr-a9tjNF-dyGUvV-6Mhm2U-2tw4Qf-cTZ46w-fzqdc-fzqdj-9QBTbx-f5YrkT-csGu1y-csGv4A-bV36Wk-c4U7JS-fpNHoh">Biston betularia by Donald Hobern</a>, CC BY 2.0</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/07/selecting-for-fraud/#disqus_thread" data-disqus-identifier="2014/05/07/selecting-for-fraud/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/07/selecting-for-fraud/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May  2,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/" rel="bookmark" title="Permanent Link to &quot;Avoiding a Witch Hunt: What is the Next Phase of our Scientific Inquisition?&quot;">Avoiding a Witch Hunt: What is the Next Phase of our Scientific Inquisition?</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jon-grahe.html" rel="author">Jon Grahe</a>

		           	

<p>Earlier this week, I learned about another case of fraud in psychological science (<a href="http://retractionwatch.com/2014/04/29/new-dutch-psychology-scandal-inquiry-cites-data-manipulation-calls-for-retraction/">Retraction Watch, 4.29.2014</a>). The conclusions from the evidence in the case against him after an extended investigation are hard to ignore. The probability that the findings could have occurred by chance are so minute that it is hard to believe that they didn’t result from falsified data. In an email to the scientific community (<a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/">Retraction Watch, 4.30.2014</a>), the target of this investigation strongly asserted that he never faked any data, while assuring us that the coauthor target never worked on the data, it was all his. Some comments from the Retraction Watch post use the term “witch hunt.” It was the first term I used in response as well, suggesting caution before judgment. A colleague pointed out that the difference was that there were no witches, and that there are clearly dishonest scientists. I have no choice but to agree; I think a better analogy is that of the Inquisition. We are entering the era of the Scientific Inquisition. A body of experts (LOWI in this case) will use a battery of sophisticated tools to examine the likelihood that the findings’ irregularities occurred by chance. In this case it is hard to believe his denial, but thankfully I am not a judge in the Scientific Inquisition.  </p>
 <a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/#disqus_thread" data-disqus-identifier="2014/05/02/avoiding-a-witch-hunt/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/02/avoiding-a-witch-hunt/">Posted at  5:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr 23,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/" rel="bookmark" title="Permanent Link to &quot;Memo From the Office of Open Science&quot;">Memo From the Office of Open Science</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

		           	

<p>Dear Professor Lucky,</p>
<p>Congratulations on your new position as assistant professor at Utopia University. We look forward to your joining our community and are eager to aid you in your transition from Antiquated Academy. It’s our understanding that Antiquated Academy does not have an Office of Open Science, so you may be unfamiliar with who we are and what we do.</p>
<p>The Office of Open Science was created to provide faculty, staff and students with the technical, educational, social and logistical support they need to do their research openly. We recognize that the fast pace of research and the demands placed on scientists to be productive make it difficult to prioritize open science. We collaborate with researchers at all levels to make it easier to do this work.</p>
<p>Listed below are some of the services we offer.</p>
 <a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/#disqus_thread" data-disqus-identifier="2014/04/23/memo-from-the-office-of-open-science/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/04/23/memo-from-the-office-of-open-science/">Posted at  2:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://centerforopenscience.github.io/osc/category/content2.html" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="http://centerforopenscience.github.io/osc/category/content4.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 3 of 6</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>