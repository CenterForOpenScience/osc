<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>Open Science Collaboration Blog &middot; articles in the "content" category</title>
<!--        <link rel="shortcut icon" href="http://osc.centerforopenscience.org/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://osc.centerforopenscience.org/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Open Science Collaboration Blog Atom Feed" />

        <link rel="stylesheet" href="http://osc.centerforopenscience.org/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://osc.centerforopenscience.org/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://osc.centerforopenscience.org/category/content.html">content</a></li>
                <li><a href="http://osc.centerforopenscience.org">Home</a></li>
                <li><a href="http://osc.centerforopenscience.org/pages/about.html">About</a></li>
                <li><a href="http://osc.centerforopenscience.org/pages/authors.html">Authors</a></li>
                <li><a href="http://osc.centerforopenscience.org/pages/policy.html">Policy</a></li>
                <li><a href="http://osc.centerforopenscience.org/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://osc.centerforopenscience.org"><img src="http://osc.centerforopenscience.org/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Apr 23,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/04/23/memo-from-the-office-of-open-science/" rel="bookmark" title="Permanent Link to &quot;Memo From the Office of Open Science&quot;">Memo From the Office of Open Science</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

		           	

<p>Dear Professor Lucky,</p>
<p>Congratulations on your new position as assistant professor at Utopia University. We look forward to your joining our community and are eager to aid you in your transition from Antiquated Academy. It’s our understanding that Antiquated Academy does not have an Office of Open Science, so you may be unfamiliar with who we are and what we do.</p>
<p>The Office of Open Science was created to provide faculty, staff and students with the technical, educational, social and logistical support they need to do their research openly. We recognize that the fast pace of research and the demands placed on scientists to be productive make it difficult to prioritize open science. We collaborate with researchers at all levels to make it easier to do this work.</p>
<p>Listed below are some of the services we offer.</p>
 <a href="http://osc.centerforopenscience.org/2014/04/23/memo-from-the-office-of-open-science/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/04/23/memo-from-the-office-of-open-science/#disqus_thread" data-disqus-identifier="2014/04/23/memo-from-the-office-of-open-science/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/04/23/memo-from-the-office-of-open-science/">Posted at  2:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr 16,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/04/16/expectations-2/" rel="bookmark" title="Permanent Link to &quot;Expectations of replicability and variability in priming effects, Part II: When should we expect replication, how does this relate to variability, and what do we do when we fail to replicate?&quot;">Expectations of replicability and variability in priming effects, Part II: When should we expect replication, how does this relate to variability, and what do we do when we fail to replicate?</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/joseph-cesario-kai-jonas.html" rel="author">Joseph Cesario, Kai Jonas</a>

		           	

<p><em>Continued from <a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/">Part 1</a>.</em></p>
<p>Now that some initial points and clarifications have been offered, we can move to the meat of the argument. Direct replication is essential to science. What does it mean to replicate an effect? All effects require a set of contingencies to be in place. To replicate an effect is to set up those same contingencies that were present in the initial investigation and observe the same effect, whereas to fail to replicate an effect is to set up those same contingencies and fail to observe the same effect. Putting aside what we mean by "same effect" (i.e., directional consistency versus magnitude), we don't see any way in which people can reasonably disagree on this point. This is a general point true of all domains of scientific inquiry.</p>
<p>The real question becomes, <em>how can we know what contingencies produced the effect in the original investigation</em>? Or more specifically, <em>how can we separate the important contingencies from the unimportant contingencies</em>? There are innumerable contingencies present in a scientific investigation that are totally irrelevant to obtaining the effect: the brand of the light bulb in the room, the sock color of the experimenter, whether the participant got a haircut last Friday morning or Friday afternoon. Common sense can provide some guidance, but in the end <em>the theory used to explain the effect</em> specifies the necessary contingencies and, by omission, the unnecessary contingencies. Therefore, if one is operating under the wrong theory, one might think some contingencies are important when really they are unimportant, and more interestingly, one might <em>miss</em> some necessary contingencies because the theory did not mention them as being important.</p>
<p>Before providing an example, it might be useful to note that, as far as we can tell, no one has offered any criticism of the logic outlined above. Many sarcastic comments have been made along the lines of, "apparently we can never learn anything because of all these mysterious moderators." And it is true that the argument can be misused to defend poor research practices. But at core, there is no criticism about the basic point that contingencies are necessary for all effects and a theory establishes those contingencies.</p>
 <a href="http://osc.centerforopenscience.org/2014/04/16/expectations-2/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/04/16/expectations-2/#disqus_thread" data-disqus-identifier="2014/04/16/expectations-2/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/04/16/expectations-2/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr  9,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/" rel="bookmark" title="Permanent Link to &quot;Expectations of replicability and variability in priming effects, Part I: Setting the scope and some basic definitions&quot;">Expectations of replicability and variability in priming effects, Part I: Setting the scope and some basic definitions</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/joseph-cesario-kai-jonas.html" rel="author">Joseph Cesario, Kai Jonas</a>

		           	

<p>We are probably thought of as "defenders" of priming effects and along with that comes the expectation that we will provide some convincing argument for why priming effects are real. We will do no such thing. The kinds of priming effects under consideration (priming of social categories which result in behavioral priming effects) is a field with relatively few direct replications<sup>1</sup> and we therefore lack good estimates of the effect size of any specific effect. Judgments about the nature of such effects can only be made after thorough, systematic research, which will take some years still (assuming priming researchers change their research practices). And of course, we must be open to the possibility that further data will show any given effect to be small or non-existent.</p>
<p>One really important thing we could do to advance the field to that future ideal state is to <strong>stop calling everything priming</strong>. It appears now, especially with the introduction of the awful term "social priming," that any manipulation used by a social cognition researcher can be called priming and, if such a manipulation fails to have an effect, it is cheerfully linked to this nebulous, poorly-defined class of research called "social priming." <strong>There is no such thing as "social priming."</strong> There is priming of social categories (elderly, professor) and priming of motivational terms (achievement) and priming of objects (flags, money) and so on. And there are priming <em>effects</em> at the level of cognition (increased activation of concepts) or affect (valence, arousal, or emotions) or behavior (walking, trivial pursuit performance) or physiology, and some of these priming <em>effects</em> will be automatic and some not (and even then recognizing the different varieties of automaticity; Bargh, 1989). These are all different things and need to be treated separately.</p>
 <a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/#disqus_thread" data-disqus-identifier="2014/04/09/expectations-1/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Apr  2,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/04/02/deathly-hallows/" rel="bookmark" title="Permanent Link to &quot;The Deathly Hallows of Psychological Science&quot;">The Deathly Hallows of Psychological Science</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/brent-w-roberts.html" rel="author">Brent W. Roberts</a>

		           	

<p><em>This piece was <a href="http://pigee.wordpress.com/2014/03/10/the-deathly-hallows-of-psychological-science/">originally posted</a> to the Personality Interest Group and Espresso (PIG-E) web blog at the University of Illinois.</em></p>
<p>As of late, psychological science has arguably done more to address the ongoing <a href="http://wp.me/p1b8ZP-3l">believability crisis</a> than most other areas of science.  Many notable efforts have been put forward to improve our methods.  From the Open Science Framework (<a href="https://osf.io/">OSF</a>), to changes in <a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/JRPEditorial2013.pdf">journal reporting practices</a>, to <a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Cumming%20-%20The%20New%20Statistics%20Why%20and%20How%20-%20PS2014.pdf">new statistics</a>, psychologists are doing more than any other science to rectify practices that allow far too many unbelievable findings to populate our journal pages.</p>
<p>The efforts in psychology to improve the believability of our science can be boiled down to some relatively simple changes.  We need to replace/supplement the typical reporting practices and statistical approaches by:</p>
<ol>
<li>Providing more information with each paper so others can double-check our work, such as the study materials, hypotheses, data, and syntax (through the OSF or journal reporting practices).</li>
<li>Designing our studies so they have adequate power or precision to evaluate the theories we are purporting to test (i.e., use larger sample sizes).</li>
<li>Providing more information about effect sizes in each report, such as what the effect sizes are for each analysis and their respective confidence intervals.</li>
<li>Valuing direct replication.</li>
</ol>
<p>It seems pretty simple.  Actually, the proposed changes are simple, even mundane.</p>
<p>What has been most surprising is the consistent push back and protests against these seemingly innocuous recommendations.  When confronted with these recommendations it seems many psychological researchers balk. Despite calls for transparency, most researchers avoid platforms like the OSF.  A striking number of individuals argue against and are quite disdainful of reporting effect sizes. Direct replications are disparaged. In response to the various recommendations outlined above, prototypical protests are:</p>
<ol>
<li>Effect sizes are unimportant because we are “testing theory” and effect sizes are only for “applied research.”</li>
<li>Reporting effect sizes is nonsensical because our research is on constructs and ideas that have no natural metric, so that documenting effect sizes is meaningless.</li>
<li>Having highly powered studies is cheating because it allows you to lay claim to effects that are so small as to be uninteresting.</li>
<li>Direct replications are uninteresting and uninformative.</li>
<li>Conceptual replications are to be preferred because we are testing theories, not confirming techniques.</li>
</ol>
<p>While these protestations seem reasonable, the passion with which they are provided is disproportionate to the changes being recommended.  After all, if you’ve run a t-test, it is little trouble to estimate an effect size too. Furthermore, running a direct replication is hardly a serious burden, especially when the typical study only examines 50 to 60 odd subjects in a 2×2 design. Writing entire <a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Stroebe%20%26%20Strack%202014.pdf">treatises</a> arguing against direct replication when direct replication is so easy to do falls into the category of “the lady doth protest too much, methinks.” Maybe it is a reflection of my repressed Freudian predilections, but it is hard not to take a Depth Psychology stance on these protests.  If smart people balk at seemingly benign changes, then there must be something psychologically big lurking behind those protests.  What might that big thing be?  I believe the reason for the passion behind the protests lies in the fact that, though mundane, the changes that are being recommended to improve the believability of psychological science undermine the incentive structure on which the field is built.</p>
<p>I think this confrontation needs to be more closely examined because we need to consider the challenges and consequences of deconstructing our incentive system and status structure.  This, then begs the question, what is our incentive system and just what are we proposing to do to it?  For this, I believe a good analogy is the dilemma faced by Harry Potter in the last book of the eponymously titled book series.</p>
 <a href="http://osc.centerforopenscience.org/2014/04/02/deathly-hallows/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/04/02/deathly-hallows/#disqus_thread" data-disqus-identifier="2014/04/02/deathly-hallows/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/04/02/deathly-hallows/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar 26,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/" rel="bookmark" title="Permanent Link to &quot;Behavioral Priming: Time to Nut Up or Shut Up&quot;">Behavioral Priming: Time to Nut Up or Shut Up</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/ej-wagenmakers.html" rel="author">EJ Wagenmakers</a>

		           	

<p>In the epic movie "Zombieland", one of the main protagonists –Tallahassee, played by Woody Harrelson– is about to enter a zombie-infested supermarket in search of Twinkies. Armed with a banjo, a baseball bat, and a pair of hedge shears, he tells his companion it is "time to nut up or shut up". In other words, the pursuit of happiness sometimes requires that you expose yourself to grave danger. Tallahasee could have walked away from that supermarket and its zombie occupants, but then he would never have discovered whether or not it contained the Twinkies he so desired.   </p>
<p>At its not-so-serious core, Zombieland is about leaving one's comfort zone and facing up to your fears. This I believe is exactly the challenge that confronts the proponents of behavioral priming today. To recap, the phenomenon of behavioral priming refers to unconscious, indirect influences of prior experiences on actual behavior. For instance, presenting people with words associated with old age ("Florida", "grey", etc.) primes the elderly stereotype and supposedly makes people walk more slowly; in the same vein, having people list the attributes of a typical professor ("confused", "nerdy", etc.) primes the concept of intelligence and supposedly makes people answer more Trivia questions correctly.   </p>
<p>In recent years, the phenomenon of behavioral priming has been scrutinized with increasing intensity. Crucial to the debate is that many (if not all) of the behavioral priming effects appear to vanish like thin air in the hands of other researchers. Many of these researchers –from now on, the skeptics– have reached the conclusion that behavioral priming effects are elusive, brought about mostly by confirmation bias, the use of questionable research practices, and selective reporting.   </p>
 <a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/#disqus_thread" data-disqus-identifier="2014/03/26/behavioral-priming/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar 19,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/03/19/if-you-have-data/" rel="bookmark" title="Permanent Link to &quot;If You Have Data, Use It When Theorizing&quot;">If You Have Data, Use It When Theorizing</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/daniel-lakens.html" rel="author">Daniel Lakens</a>

		           	

<p>There is a reason data collection is part of the empirical cycle. If you have a good theory that allows for what Platt (1964) called ‘strong inferences’, then statistical inferences from empirical data can be used to test theoretical predictions. In psychology, as in most sciences, this testing is not done in a Popperian fashion (where we consider a theory falsified if the data does not support our prediction), but we test ideas in Lakatosian lines of research, which can either be progressive or degenerative (e.g., Meehl, 1990). In (meta-scientific) <em>theory</em>, we judge (scientific) theories based on whether they have something going for them.  </p>
<p>In scientific <em>practice</em>, this means we need to evaluate research lines. One really flawed way to do this is to use ‘vote-counting’ procedures, where you examine the literature, and say: "Look at all these significant findings! And there are almost no non-significant findings! This theory is the best!” Read Borenstein, Hedges, Higgins, &amp; Rothstein (2006) who explain “Why Vote-Counting Is Wrong” (p. 252 – but read the rest of the book while you’re at it).  </p>
 <a href="http://osc.centerforopenscience.org/2014/03/19/if-you-have-data/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/03/19/if-you-have-data/#disqus_thread" data-disqus-identifier="2014/03/19/if-you-have-data/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/03/19/if-you-have-data/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar 12,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/" rel="bookmark" title="Permanent Link to &quot;In the Previous Episodes of the Tale of Social Priming and Reproducibility&quot;">In the Previous Episodes of the Tale of Social Priming and Reproducibility</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/ase-innes-ker.html" rel="author">Åse Innes-Ker</a>

   			   <p>We have lined up a nice set of posts responding to the recent special section in PoPS on social priming and replication/reproducibility, which we will publish in the coming weeks. It has proven easier to find critics of social priming than to find defenders of the phenomenon, and if there are primers out there who want to chime in they are most welcome and may contact us at oscblog@googlegroups.com.</p>
<p>The special section in PoPS was immediately prompted by this wonderful <a href="http://pps.sagepub.com/content/7/6.toc">November 2012 issue</a> from PoPS on replicability in psychology  (open access!), but the Problems with Priming started prior to this. For those of you who didn’t seat yourself in front of the screen with a tub of well-buttered pop-corn every time behavioral priming made it outside the trade journals, I’ll provide some back-story, and links to posts and articles that frames the current response.</p>
<p>The mitochondrial Eve of behavioral priming is Bargh’s <a href="http://psycnet.apa.org/journals/psp/71/2/230/">Elderly Prime</a><sup>1</sup>. The unsuspecting participants were given scrambled sentences, and were asked to create proper sentences out of four of the five words in each. Some of the sentences included words like Bingo or Flordia – words that may have made you think of the elderly, if you were a student in New York in the mid nineties. Then, they measured the speed with which the participant walked down the corridor to return their work, and, surprising to many, those that unscrambled sentences that included “Bingo” and “Florida” walked slower than those that did not. Conclusion: the construct of “elderly” had been primed, causing participants to adjust their behavior (slower walk) accordingly. You can check out sample sentences in <a href="http://marginalrevolution.com/marginalrevolution/2012/03/walking-fast-and-slow.html">this Marginal Revolution post</a> – yes, priming made it to this high-traffic economy blog.</p>
<p>This paper has been cited 2571 times, so far (according to Google Scholar). It even appears in Kahneman’s <a href="http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555">Thinking, Fast and Slow</a>, and has been high on the wish-list for replication on Pashler’s <a href="http://www.psychfiledrawer.org/view_article_list.php">PsychFile Drawer</a>. (No longer in the top 20, though).</p>
<p>Finally, in January 2012, Doyen, Klein, Pichon &amp; Cleeremans (a Belgian group) <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029081.">published a replication attempt</a> in PLOSone where they suggest the effect was due to demand.  Ed Yong did <a href="http://blogs.discovermagazine.com/notrocketscience/2012/01/18/primed-by-expectations-why-a-classic-psychology-experiment-isnt-what-it-seemed/#.UuimfLRwG70">this nice write-up</a> of the research.</p>
<p>Bargh was not amused, and wrote a scathing rebuttal on his blog in the Psychology Today domain.  He took it down after some time (for good reason – I think it can be found, but I won’t look for it.). <a href="http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/#.UuimZ7RwG70">Ed commented on this too</a>.</p>
<p>A number of good posts from blogging psychological scientists also commented on the story. A sampling are <a href="http://hardsci.wordpress.com/2012/03/12/some-reflections-on-the-bargh-doyen-elderly-walking-priming-brouhaha/">Sanjay Srivastava</a> on his blog Hardest Science, <a href="http://neurochambers.blogspot.se/2012/03/you-cant-replicate-concept.html">Chris Chambers</a> on NeuroChambers, and <a href="http://cedarsdigest.wordpress.com/2012/03/21/put-your-head-up-to-the-meta-a-peer-reviews-post-post-publication-peer-review-a-bargh-full-of-links/">Cedar Riener</a> on his Cedarsdigest. </p>
<p>The British Psychological Society published <a href="http://www.thepsychologist.org.uk/blog/11/blogpost.cfm?threadid=2196&amp;catid=48">a notice about it</a> in The Psychologist which links to additional commentary.  In May, Ed Yong had <a href="http://www.nature.com/news/replication-studies-bad-copy-1.10634">an article in Nature</a> discussing the status of non-replication in psychology in general, but where he also brings up the Doyen/Bargh controversy. On January 13, the Chronicle published <a href="http://chronicle.com/article/Power-of-Suggestion/136907/">a summary of what had happened</a>.</p>
<p>But, prior to that, Daniel Kahneman made a call for psychologists to clean up their act as far as behavioral priming goes. Ed Yong (again) published two pieces about it. One in <a href="http://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535">Nature</a> and one on <a href="http://blogs.discovermagazine.com/notrocketscience/2012/10/04/daniel-kahneman-daisy-chain-replications-priming-psychology/#.UuTRArRwHIU">his blog</a>.</p>
<p>The controversies surrounding priming continued in the spring of 2013. This time it was David Shanks who, as a hobby (<a href="http://www.ucl.ac.uk/psychlangsci/">from his video</a> - scroll down below the fold) had taken to attempting to replicate priming of intelligence, work originally done by <a href="http://psycnet.apa.org/index.cfm?fa=search.displayrecord&amp;uid=1998-01060-003">Dijksterhuis and van Knippenberg</a> in 1998. He had his students perform a series of replications, all of which showed no effect, and was then collected in <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0056515">this PLOSone paper</a>. </p>
<p>Dijksterhuis retorted in <a href="http://www.plosone.org/annotation/listThread.action?root=64751">the comment section</a><sup>2</sup>. Rolf Zwaan <a href="http://rolfzwaan.blogspot.nl/2013/04/social-priming-in-theory.html">blogged about it</a>. Then, Nature posted <a href="http://www.nature.com/news/disputed-results-a-fresh-blow-for-social-psychology-1.12902#/correction1">a breathless article</a> suggesting that this was a fresh blow for us who are Social Psychologists.</p>
<p>Now, most of us who do science thought instead that this was science working just like it ought to be working, and blogged up a storm about it – with some of the posts (including one of mine) linked in Ed Yong’s <a href="http://phenomena.nationalgeographic.com/2013/05/04/ive-got-your-missing-links-right-here-4-may-2013/">“Missing links” feature</a>. The links are all in the fourth paragraph, above the scroll, and includes additional links to discussions on replicability, and the damage done by a certain Dutch fraudster.</p>
<p>So here you are, ready for the next set of installments.</p>
<p><sup>1</sup> Ancestral to this is <a href="http://psycnet.apa.org/index.cfm?fa=search.displayRecord&amp;uid=1981-01290-001">Srull &amp; Wyer’s</a> (1979) story of Donald, who is either hostile or kind, depending on which set of sentences the participant unscrambled in that earlier experiment that had nothing to do with judging Donald.</p>
<p><sup>2</sup> A nice feature.  No waiting years for the retorts to be published in the dead tree variant we all get as PDF’s anyway.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/#disqus_thread" data-disqus-identifier="2014/03/12/previous-episodes/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/social-priming-and-reproducibility.html">social-priming-and-reproducibility</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Mar  6,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/03/06/confidence intervals/" rel="bookmark" title="Permanent Link to &quot;Confidence Intervals for Effect Sizes from Noncentral Distributions&quot;">Confidence Intervals for Effect Sizes from Noncentral Distributions</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/russ-clay.html" rel="author">Russ Clay</a>

   			   <p><sup>(Thanks to Shauna Gordon-McKeon, Fred Hasselman, Daniël Lakens, Sean Mackinnon, and Sheila Miguez for their contributions and feedback to this post.)</sup></p>
<p><em>I recently took on the task of calculating a confidence interval around an effect size stemming from a noncentral statistical distribution (the F-distribution to be precise).  This was new to me, and as I am of the view that such statistical procedures would add value to the work being done in the social and behavioral sciences, but that they are not common in practice at the present time, potentially due to lack of awareness, I wanted to pass along some of the things that I found.</em><br />
In an effort to estimate the replicability of psychological science, an important first step is to determine the criteria for declaring a given replication attempt as successful.  Lacking clear consensus around this criteria, the OpenScience group determined that rather than settling on a single set of criteria by which the replicability of psychological research would be assessed, multiple methods would be employed, all which provide a measure of valuable insight regarding the reproducibility of published findings in psychology (OpenScience Collaboration, 2012).  One such method is to examine the confidence interval around the original target effect and to see if this confidence interval overlaps with the confidence interval from the replication effect.  However, estimating the confidence interval around many effects in social science research requires the use of  non-central probability distributions, and most mainstream statistical packages (e.g. SAS, SPSS) do not provide off the shelf capabilities for deriving confidence intervals from these distributions (Kelley, 2007).  </p>
<p>Most of us probably picture common statistical distributions such as the t-distribution, the F-distribution, and the χ2 distribution as being two dimensional, with the x-axis representing the value of the test statistic and the area under the curve representing the likelihood of observing such a value in a sample population.  When first learning to conduct these statistical tests, such visual representations likely provided a helpful way to convey the concept that more extreme values of the test statistic were less likely.  In the realm of null hypothesis statistical testing (NHST), this provides a tool for visualizing how extreme the test statistic would need to be before we would be willing to reject a null hypothesis.  However, it is important to remember that these distributions vary along a third parameter as well: the noncentrality parameter.  The distribution that we use to determine the cut-off points for rejecting a null hypothesis is a special, central case of the distribution when the noncentrality parameter is zero.  This special-case distribution gives the probabilities of test statistic values when the null hypothesis is true (i.e., when the population effect is zero).  As the noncentrality parameter changes (i.e., when we assume that an effect does exist), the shape of the distribution which defines the probabilities of obtaining various values of the parameter in our statistical tests changes as well.  The following figure (copied from the Wikipedia page for the noncentral t-distribution) might help provide a sense of how the shape of the t-distribution changes as the noncentrality parameter varies.  </p>
<p><img src="images/CIs.png" alt="non-central T distribution" align="center" style="padding-right: 20px;" width="360px" /><br />
<sup>Figure by <a href="http://en.wikipedia.org/wiki/File:Nc_student_t_pdf.svg">Skbkekas</a>, licensed <a href="http://creativecommons.org/licenses/by/3.0/deed.en">CC BY 3.0.</a></sup></p>
<p>The first two plots (orange and purple) illustrate the different shapes of the distribution under the assumption that the true population parameter (the difference in means) is zero.  The value of v indicates the degrees of freedom used to determine the probabilities under the curve.  The difference between these first two curves stems from the fact that the purple curve has more degrees of freedom (a larger sample), and thus there will be a higher probability of observing values near the mean.  These distributions are central (and symmetrical), and as such, values of x that are equally higher or lower than the mean are equally probable.  The second two plots (blue and green) illustrate the shapes of the distribution under the assumption that the true population parameter is two.  Notice that both of these curves are positively skewed, and that this skewness is particularly pronounced in the blue curve as it is based on fewer degrees of freedom (smaller sample size).  The important thing to note is that for these plots, values of x that are equally higher or lower than the mean are NOT equally probable.  Observing a value of x = 4 under the assumption that the true value of x is two is considerably more probable than observing a value of x = 0.  Because of this, a confidence interval around an effect that is anything other than zero will be asymmetrical and will require a bit of work to calculate.  </p>
<p>Because the shape (and thus the degree of symmetry) of many statistical distributions depends on the size of the effect that is present in the population, we need a noncentrality parameter to aid in determining the shape of the distribution and the boundaries of any confidence interval of the population effect.  As mentioned previously, these complexities do not arise as often as we might expect in everyday research because when we use these distributions in the context of null-hypothesis statistical testing (NHST), we can assume a special, ‘centralized’ case of the distributions that occurs when the true population effect of interest is zero (the typical null hypothesis).  However, confidence intervals can provide different information than what can be obtained through NHST.  When testing a null hypothesis, what we glean from our statistics is the probability of obtaining the effect observed in our sample if the true population effect is zero.  The p-value represents this probability, and is derived from a probability curve with a noncentrality parameter of zero.  As mentioned above, these special cases of statistical distributions such as the t, F, and χ2 are ‘central’ distributions.  On the other hand, when we wish to construct a confidence interval of a population effect, we are no longer in the NHST world, and we no longer operate under the assumption of ‘no effect’.  In fact, when we build a confidence interval, we are not necessarily making assumptions at all about the existence or non-existence of an effect.  Instead, when we build a confidence interval, we want a range of values that is likely to contain the true population effect with some degree of confidence.  To be crystal clear, when we construct a 95% confidence interval around a test statistic, what we are saying is that if we repeatedly tested random samples of the same size from the target population under identical conditions, the true population parameter will be bounded by the 95% confidence interval derived from these samples 95% of the time.  </p>
<p>From a practical standpoint, a confidence interval can tell us everything that NHST can, and then some.  If the 95% confidence interval of a given effect contains the value of zero, then there is a good chance that there is a negligible effect in the relationship you are testing.  In this case, as a researcher, the conclusion that you would reach is conceptually similar to declaring that you are not willing to reject a null hypothesis of zero effect on the grounds that there is greater than a 5% chance that the effect is actually zero.  However, a confidence interval allows the researcher to say a bit more about the potential size of a population effect as well as the degree of variability that exists in it’s estimate, whereas NHST only permits the researcher to state, with a specified level of confidence, the likelihood that an effect exists at all.  </p>
<p>Why, then, is NHST the overwhelming choice of statisticians in the social sciences?  The likely answer has to do with the idea of non-centrality stated above.  When we build a confidence interval around an effect size, we generally do not build the confidence interval around an effect of zero.  Instead, we build the confidence interval around the effect that we find in our sample.  As such, we are unable to build the confidence interval using the symmetrical, special case instances of many of our statistical distributions.  We have to build it using an asymmetrical distribution that has a shape (a degree of non-centrality) that depends on the effect that we found in our sample.  This gets messy, complicated, and requires a lot of computation.  As such, the calculation of these confidence intervals was not practical until it became commonplace for researchers to have at their disposal the computational power available in modern computing systems.  However, research in the social sciences has been around much longer than your everyday, affordable, quad-core laptop, and because building confidence intervals around effects from non-central distributions was impractical for much of the history of the social sciences, these statistical techniques were not often taught, and their lack of use is likely to be an artifact of institutional history (Steiger &amp; Fouladi, 1997).
All of this to say that in today’s world, researchers generally have more than enough computational power at their disposal to easily and efficiently construct a confidence interval around an effect from a non-central distribution.  The barriers to these statistical techniques have been largely removed, and as the value of the information obtained from a confidence interval exceeds the value of the information that can be obtained from NHST, it is useful to spread the word about resources that can help in the computation of confidence intervals around common effect size metrics in the social and behavioral sciences.  </p>
<p>One resource that I found to be particularly useful is the MBESS (Methods for the Behavioral, Educational, and Social Sciences) package for the <a href="http://www.r-project.org/">R statistical software platform</a>.  For those unfamiliar with R, it is a free, open-source statistical software package which can be run on Unix, Mac, and Windows platforms.  The standard R software contains basic statistics functionality, but also provides the capability for contributors to develop their own functionality (typically referred to as ‘packages’) which can be made available to the larger user community for download.   MBESS is one such package which provides ninety-seven different functions for statistical procedures that are readily applicable to statistical analysis techniques in the behavioral, educational, and social sciences.  Twenty-five of these functions involve the calculation of confidence intervals or confidence limits, mostly for statistics stemming from noncentral distributions.  </p>
<p>For example, I used the ci.pvaf (confidence interval of the proportion of variance accounted for) function from the MBESS package to obtain a 95% confidence interval around an η2 effect of 0.11 from a one-way between groups analysis of variance.  In order to do this, I only needed to supply the function with several relevant arguments:  </p>
<blockquote>
<p><strong>F-value:</strong>  This is the F-value from a fixed-effects ANOVA<br />
<strong>df:</strong>  The numerator and denominator degrees of freedom from the analysis<br />
<strong>N:</strong>  The sample size<br />
<strong>Confidence Level:</strong>  The confidence level coverage that you desire (i.e. 95%)  </p>
</blockquote>
<p>No more information is required.  Based on this, the function can calculate the desired confidence interval around the effect. Here is a copy of the code that I entered and what was produced (with comments in italics to explain what is going on in each step):  </p>
<blockquote>
<p><strong>library(MBESS);</strong>  </p>
<p><em>once you have installed the MBESS package, this command makes it available for your current session of R</em>  </p>
<p><strong>ci.pvaf(F.value=4.97, df.1=2, df.2=81, N=84, conf.level=.95)</strong>  </p>
<p><em>this uses the ci.pvaf function in the MBESS package to calculate the confidence interval.  I have given # the function an F-value (F.value) of 4.97, with 2 degrees of freedom between groups (df.1), and 81 # degrees of freedom within groups (df.2), a sample size (N) of 84, and have asked it to produce a 95% confidence interval (conf.level).  Executing the above command produces the following output:</em>  </p>
<p><strong>$Lower.Limit.Proportion.of.Variance.Accounted.for</strong><br />
<strong>[1] 0.007611619</strong>  </p>
<p><strong>$Probability.Less.Lower.Limit</strong><br />
<strong>[1] 0.025</strong>  </p>
<p><strong>$Upper.Limit.Proportion.of.Variance.Accounted.for</strong><br />
<strong>[1] 0.2320935</strong>  </p>
<p><strong>$Probability.Greater.Upper.Limit</strong><br />
<strong>[1] 0.025</strong>  </p>
<p><strong>$Actual.Coverage</strong><br />
<strong>[1] 0.95</strong>  </p>
<p><em>Thus, the 95% confidence interval around my η2 effect is <strong>[0.01 - 0.23]</strong>.</em>  </p>
</blockquote>
<p>Similar functions are available in the MBESS package for calculating confidence intervals around a contrast in a fixed-effects ANOVA, multiple correlation coefficient, squared multiple correlation coefficient, regression coefficient, reliability coefficient, RMSEA, standardized mean difference, signal-to-noise ratio, and χ2 parameters, among others.  </p>
<h5>Additional Resources</h5>
<ul>
<li>
<p>Fred Hasselman has created <a href="http://osc.centerforopenscience.org/static/CIs_in_r.html">a brief tutorial</a> for computing effect size confidence intervals using R.  </p>
</li>
<li>
<p>For those more familiar with conducting statistics in an SPSS environment, Dr. Karl Wuensch at East Carolina University provides links to several SPSS programs on his Web Page.  <a href="http://core.ecu.edu/psyc/wuenschk/SPSS/SPSS-Programs.htm">This program</a> is for calculating confidence intervals for a standardized mean difference (Cohen’s d).  </p>
</li>
<li>
<p>In addition, I came across several publications that I found useful in providing background information regarding non-central distributions (a few of which are cited above).  I’m sure there are more, but I found these to be a good place to start:</p>
</li>
</ul>
<blockquote>
<p>Cumming, G. (2006).  <em>How the noncentral t distribution got its hump.</em>  Paper presented at the seventh International Conference on Teaching Statistics, Salvador, Bahia, Brazil.  </p>
<p>Cumming, G. (2014).  <em>The new statistics: Why and how.</em>  Psychological Science, 25, 7-29.  DOI: 10.1177/0956797613504966  </p>
<p>Kelley, K. (2007).  <em>Confidence intervals for standardized effect sizes: Theory, application, and implementation.</em>  Journal of Statistical Software, 20, 1-24.  </p>
<p>Smithson, M. (2001). <em>Correct confidence intervals for various regression effect sizes and parameters: The importance of noncentral distributions in computing intervals.</em> Educational And Psychological Measurement, 61(4), 605-632. doi:10.1177/00131640121971392  </p>
<p>Steiger, J. H., &amp; Fouladi, R. T. (1997).  <em>Noncentrality interval estimation and the evaluation of statistical models.</em>  In L. Harlow, S. &gt; Mulaik, &amp; J. Steiger (Eds.), What if there were no significance tests? (pp. 221-256).  Mahwah, NJ: Erlbaum.  </p>
</blockquote>
<ul>
<li>There is also <a href="http://cran.r-project.org/web/packages/MBESS/MBESS.pdf">thorough documentation of the MBESS package itself</a>.</li>
</ul>
<p>Hopefully others find this information as useful as I did!</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/03/06/confidence intervals/#disqus_thread" data-disqus-identifier="2014/03/06/confidence intervals/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/03/06/confidence intervals/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Feb 27,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/" rel="bookmark" title="Permanent Link to &quot;Data trawling and bycatch – using it well&quot;">Data trawling and bycatch – using it well</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/ruben-arslan.html" rel="author">Ruben Arslan</a>

		           	

<p>Pre-registration is starting to outgrow its old home, <a href="http://neuroskeptic.blogspot.co.uk/2012/04/fixing-science-systems-and-politics.html">clinical trials</a>. Because it is a good way to (a) show that your theory can make viable predictions and (b) that your empirical finding is not vulnerable to hypothesising after the results are known (HARKing) and some other questionable research practices, more and more scientists endorse and actually do pre-registration. <a href="http://funderstorms.wordpress.com/2014/02/25/nsf-gets-an-earful-about-replication/">Many</a> remain wary though and <a href="http://andrewgelman.com/2014/01/23/discussion-preregistration-research-studies/">some</a> simply think pre-registration cannot work for their kind of research. A recent amendment (October 2013) to the Declaration of Helsinki mandates public registration of all research on humans before recruiting the first subject and the publication of all results, positive, negative and inconclusive. </p>
<p>For some of  science the widespread “fishing for significance” metaphor illustrates the problem well: Like an experimental scientist the fisherman casts out the rod many times, tinkering with a variety of baits and bobbers, one at a time, trying to make a good catch, but possibly developing a superstition about the best bobber. And, like an experimental scientist, if he returns the next day to the same spot, it would be easy to check whether the success of the bobber replicates. If he prefers to tell fishing lore and enshrine his bobber in a display at his home, other fishermen can evaluate his lore by doing as he did in his stories.</p>
<p>Some disciplines (epidemiology, economics, developmental and personality psychology come to mind) proceed, quite legitimately, more like fishing trawlers – that is to say data collection is a laborious, time-consuming, collaborative endeavour. Because these operations are so large and complex, some data bycatch will inevitably end up in the dragnet. </p>
 <a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/#disqus_thread" data-disqus-identifier="2014/02/27/data-trawling/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Feb  5,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/" rel="bookmark" title="Permanent Link to &quot;Open Data and IRBs&quot;">Open Data and IRBs</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/bryan-burnham.html" rel="author">Bryan Burnham</a>

   			   <p>Among other things the open science movement encourages “open data” practices, that is, researchers making data freely available on personal/lab websites or institutional repositories for others to use. For some, open data is a necessity as the <a href="http://grants.nih.gov/grants/policy/data_sharing/data_sharing_guidance.htm#goals">NIH</a> and <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a> have adopted data-sharing policies and require some grant applications to include data management and dissemination plans. According to the NIH:</p>
<blockquote>
<p>“...all data should be considered for data sharing. <strong>Data should be made as widely and freely available as possible while safeguarding the privacy of participants, and protecting confidential and proprietary data.</strong>” (emphasis theirs)</p>
</blockquote>
<p>Before making human subject data open several issues must be considered. First, data should be de-identified to maintain subject confidentiality so responses cannot be linked to identities and data are seemingly anonymous. Second, researchers should consider Institutional Review Board’s (IRB) policies about data sharing. (Disclosure: I have been a member of my university's IRB for 6 years and chair of my Departmental Review Board, DRB, for 7 years.)</p>
<p>Unfortunately, while the policies and procedure of all IRBs require researchers to obtain consent, disclose study procedures to subjects, and maintain confidentiality, it is unknown how many IRBs have policies and procedures for open data dissemination. Thus, a conflict may arise between researchers who want to adopt open data practices or <em>need</em> to disseminate data (those with NIH or NSF grants) and judgements of IRBs.</p>
<p>This is an especially important issue for those who want to share data that are already collected: can use data be openly disseminated without IRB review? (I address this below when I offer recommendations.) What can researchers do when they want or need to share data freely, but their IRB does not have a clear policy? And what say does an IRB have in open data practices?</p>
<p>While IRBs should be consulted and informed about open data, as I delineate below IRBs are not now and were never intended to be data-monitoring groups (Bankert &amp; Amdur, 2000). IRBs are regulated and have little say in whether a researcher can share data, based on the purview, scope, and responsibilities of IRBs.</p>
<p>IRBs in the United States are regulated under <a href="http://www.hhs.gov/ohrp/humansubjects/guidance/45cfr46.html">US Health and Human Services (HHS)</a> guidelines for Protection of Human Subjects. The guidelines describe the composition of IRBs, record keeping, define levels of risk, and list specific duties of IRBs and hint at their limits.</p>
<p>When they function appropriately IRBs review research protocols to (1) evaluate risks; (2) determine whether subject confidentiality is maintained, that is, whether responses are linked to identities (‘confidentiality’ differs from ‘privacy’, which means others will not know a person participated in a study); and (3) evaluate whether subjects are given sufficient information about risks, procedures, privacy, and confidentiality. HHS Regulations Part 46, Subpart A, Section 111 ("Criteria for IRB Approval of Research") (a)(2), is very specific on the purview of IRBs in evaluating protocols:</p>
<blockquote>
<p>"In evaluating risks and benefits, <strong>the IRB should consider only those risks and benefits that may result from the research</strong> (as distinguished from risks and benefits of therapies subjects would receive even if not participating in the research). <strong>The IRB should not consider possible long-range effects of applying knowledge gained in the research (for example, the possible effects of the research on public policy) as among those research risks that fall within the purview of its responsibility.</strong>" [emphasis added]</p>
</blockquote>
<p>And regulations §46.111 (a)(6) and (a)(7) state that IRBs are to evaluate the safety, privacy, and confidentiality of subjects in proposed research:</p>
<blockquote>
<p>(a)(6)  "When appropriate, the research plan makes adequate provision for monitoring the data collected to ensure the safety of subjects.”
(a)(7) “When appropriate, there are adequate provisions to protect the privacy of subjects and to maintain the confidentiality of data." </p>
</blockquote>
<p>The regulations make it clear that IRBs should consider only risks directly related to the study, and explicitly forbid IRBs from evaluating potential long-range effects of new knowledge gained from the study, as in new knowledge resulting from data sharing. Thus, IRBs should concern themselves with evaluating a study for safety, confidentiality, and that information is disclosed; reviewing existing data for dissemination is <em>not</em> under the purview of the IRB. The <em>only</em> issue that should concern IRBs about open data is whether the data are de-identified to “...protect the privacy of subjects and to maintain the confidentiality of data." It is not the responsibility of the IRB to monitor data, that responsibility falls to the researcher.</p>
<p>Nonetheless, IRBs may take the position that they are data monitors and deny a researcher’s request to openly disseminate data. In denying a request an IRB may use the argument ‘<em>subjects would not have participated if they knew the data would be openly shared.</em>’ In this case, IRBs would be playing mind-readers; there is no way an IRB can assume subjects would not have participated if they knew data would be openly shared. However, whether a person would decline to participate if they were informed about a researcher’s intent to openly disseminate data is an empirical question.</p>
<p>Also, with this argument the IRB is implicitly suggesting subjects would need to have been informed about open data dissemination in the consent form. But, such a requirement for consent forms neglects other federal guidelines. The <a href="http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html%20">Belmont Report</a> provides responsibilities for human researchers, much like the <a href="http://www.apa.org/ethics/code/index.aspx">APA's ethical principles</a>, and describes what information should be included in the consent process:</p>
<blockquote>
<p>“Most codes of research establish specific items for disclosure intended to assure that subjects are given sufficient information. These items generally include: the research procedure, their purposes, risks and anticipated benefits, alternative procedures (where therapy is involved), and a statement offering the subject the opportunity to ask questions and to withdraw at any time from the research.”</p>
</blockquote>
<p>The Belmont Report does not even mention that subjects should be informed about the potential long-range plans or uses of the data they provide. Indeed, researchers do not have to tell subjects what analyses will be used, and for good reason. All the Belmont requires is for subjects be informed about the purpose of the study, the procedures, and be informed about their privacy and confidentiality of responses.</p>
<p>Another argument an IRB could make is the data could be used maliciously. For example, a researcher could make a data set open that included ethnicity and test scores and someone else could use that data to show certain ethnic groups are smarter than others. (This example is based on a recent <a href="https://groups.google.com/forum/#!topic/openscienceframework/JHucNxN19hc">Open Science Framework post</a> that is the basis for this post.) </p>
<p>Although it is more likely that open data would be used as intended, someone could use data as they were not intended and may find a relationship between ethnicity and test scores. So what? The data are not malicious or problematic, it is the person using (misusing?) the data, and IRBs should not be in the habit of allowing only politically correct research to proceed (Lilienfeld, 2010). Also, by considering what others <em>might</em> do with open data, IRBs would be mind-reading and overstepping its purview by considering “...long-range effects of applying knowledge gained in the research (for example, the possible effects of the research on public policy).”</p>
<p>The bottom line is IRBs cannot know whether subjects would not have participated in a project if they knew the data would be openly disseminated, or potential findings by others. Federal regulations inform IRBs of their specific duties, which do <em>not</em> include data monitoring or making judgments on open data dissemination; those duties are the responsibilities of the researcher.</p>
<p>So what should you do if you want to make your data open? First, don't fear the IRB, but don’t forget the IRB. Perhaps re-examine IRB policies any time you plan a new project to remind yourself of the IRB requirements.</p>
<p>Second, making your data open does depend on what subjects agree to on the consent form, and this is especially important if you want to make existing data open. If subjects are told their participation will remain private (identities not disclosed) and responses will remain confidential (identities not linked to responses), openly disseminating de-identified data would not violate the agreement. However, if subjects were told the data would ‘not be disseminated’, the researcher <em>may</em> violate the agreement if they openly share data. In this case the IRB would need to be involved, subjects may need to re-consent to allow their responses to be disseminated, and new IRB approval may be needed as the original consent agreement may change.</p>
<p>Third, <a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/">de-identify data sets you plan to make open</a>. This includes removing names, student IDs, the subject numbers, timestamps, and anything else that could be used to uniquely identify a person.</p>
<p>Fourth, inform your IRB and department of your intentions. Describe your de-identification process and that you are engaging in open data practices as you see appropriate while maintaining subject confidentiality and privacy. (If someone objects, direct them toward federal IRB regulations.)</p>
<p>Finally, work with your IRB to develop guidelines and policies for data sharing. Given the speed and recency of the open science and open data movements, it is unlikely many IRBs have considered such policies.</p>
<p>We want greater transparency in science, and open data is one practice the can help. The IRB should not be seen as a hurdle or barrier to disseminating data, but as a reminder that one of the best practices in science is to ensure the integrity of our data and information communications by responsibly maintaining the confidence and privacy of our research subjects.</p>
<p><strong>References</strong></p>
<p>Bankert, E., &amp; Amdur, R. (2000). <a href="http://www.jstor.org/stable/3563586">The IRB is not a data and safety monitoring board.</a> IRB: Ethics and Human Research, 22(6), 9-11. </p>
<p>De Wolfe, V. A., Sieber, J. E., Steel, P. M., &amp; Zarate, A. O. (2005). <a href="http://www.jstor.org/stable/3563537">Part I: What is the requirement for data sharing?</a> IRB: Ethics and Human Research, 27(6), 12-16. </p>
<p>De Wolfe, V. A., Sieber, J. E., Steel, P. M., &amp; Zarate, A. O. (2006). <a href="http://www.jstor.org/stable/30033191">Part III: Meeting the challenge when data sharing is required.</a> IRB: Ethics and Human Research, 28(2), 10-15. </p>
<p>Lilienfeld, S.O. (2010).  <em>Can psychology become a science?</em>  Personality and Individual Differences, 49, 281-288.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/#disqus_thread" data-disqus-identifier="2014/02/05/open-data-and-IRBs/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://osc.centerforopenscience.org/category/content3.html" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="http://osc.centerforopenscience.org/category/content5.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 4 of 6</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://osc.centerforopenscience.org/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>