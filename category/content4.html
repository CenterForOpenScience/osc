<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>A Pelican Blog &middot; articles in the "content" category</title>
<!--        <link rel="shortcut icon" href="http://osc.centerforopenscience.org/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://osc.centerforopenscience.org/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <link rel="stylesheet" href="http://osc.centerforopenscience.org/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://osc.centerforopenscience.org/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://osc.centerforopenscience.org/category/content.html">content</a></li>
                <li><a href="http://osc.centerforopenscience.org">Home</a></li>
<li><a href="http://osc.centerforopenscience.org/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://osc.centerforopenscience.org"><img src="http://osc.centerforopenscience.org/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Dec 13,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/" rel="bookmark" title="Permanent Link to &quot;Chasing Paper, Part 3&quot;">Chasing Paper, Part 3</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This is part three of a three part post brainstorming potential improvements to the journal article format.  Part one is <a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/">here</a>, part two is <a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/">here</a>.</em></p>
<h3>The classic journal article is only readable by domain experts.</h3>
<p>Journal articles are currently written for domain experts.  While novel concepts or terms are usually explained, there is the assumption of a vast array of background knowledge and jargon is the rule, not the exception.  While this leads to quick reading for domain experts, it can make for a difficult slog for everyone else.</p>
<p>Why is this a problem?  For one thing, it prevents interdisciplinary collaboration.  Researchers will not make a habit of reading outside their field if it takes hours of painstaking, self-directed work to comprehend a single article.  It also discourages public engagement.  While science writers do admirable work boiling hard concepts down to their comprehensible cores, many non-scientists want to actually read the articles, and get discouraged when they can’t.  </p>
<p>While opaque scientific writing exists in every format, technologies present new options to translate and teach.  Jargon could be linked to a glossary or <a href="https://en.wikipedia.org/wiki/Mouseover">other reference material</a>.  You could be given a plain english explanation of a term when your mouse hovers over it.  Perhaps each article could have multiple versions - for domain experts, other scientists, and for laypeople.  </p>
<p>Of course, the ability to write accessibly is a skill not everyone has.  Luckily, any given paper would mostly use terminology already introduced in previous papers.  If researchers could easily credit the teaching and popularization work done by others, they could acknowledge the value of those contributions while at the same time making their own work accessible.</p>
<h3>The classic journal article has no universally-agreed upon standards.</h3>
<p>Academic publishing, historically, has been a distributed system.  Currently, the top three publishers still account for less than half (42%) of all published articles (<a href="http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html">McGuigan and Russell, 2008</a>).  While certain format and content conventions are shared among publishers, generally speaking it’s difficult to propagate new standards, and even harder to enforce them.  Not only do standards vary, they are frequently hidden, with most of the review and editing process taking place behind closed doors.</p>
<p>There are benefits to decentralization, but the drawbacks are clear.  Widespread adoption of new standards, such as <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588">Simmons et al’s 21 Word Solution</a> or <a href="http://centerforopenscience.org/journals/">open science practices</a>, depends on the hard work and high status of those advocating for them.  How can the article format be changed to better accommodate changing standards, while still retaining individual publishers’ autonomy?</p>
<p>One option might be to create a new section of each journal article, a free-form field where users could record whether an article met this or that standard.  Researchers could then independently decide what standards they wanted to pay attention to.  While this sounds messy, if properly implemented this feature could be used very much like a search filter, yet would not require the creation or maintenance of a centralized database.</p>
<p>A different approach is already being embraced: an effort to make the standards that currently exist more transparent by bringing peer review out into the open.  <a href="https://en.wikipedia.org/wiki/Open_peer_review">Open peer review</a> allows readers to view an article’s pre-publication history, including the authorship and content of peer reviews, while <a href="http://publications.copernicus.org/services/public_peer_review.html">public peer review</a> allows the public to participate in the review process.  However, these methods have yet to be generally adopted.</p>
<p>*</p>
<p>It’s clear that journal articles are already changing.  But they may not be changing fast enough.  It may be better to forgo the trappings of the journal article entirely, and seek a new system that more naturally encourages collaboration, curation, and the efficient use of the incredible resources at our disposal.  With journal articles commonly costing more than $30 each, some might jump at the chance to leave them behind.</p>
<p>Of course, it’s easy to play “what if” and imagine alternatives; it’s far harder to actually implement them.  And not all innovations are improvements.  But with <a href="http://www.battelle.org/media/press-releases/battelle-r-d-magazine-annual-global-funding-forecast-predicts-r-d-spending-growth-will-continue-while-globalization-accelerates">over a billion dollars</a> spent on research each day in the United States, with <a href="http://www.bmj.com/content/341/bmj.c6815">over 25,000 journals</a> in existence, and over a million articles published each year, surely there is room to experiment.</p>
<h4>Bibliography</h4>
<p>Budd, J.M., Coble, Z.C. and Anderson, K.M.  (2011)  <a href="http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf">Retracted Publications in Biomedicine: Cause for Concern.</a></p>
<p>Wright, K. and McDaid, C.  (2011).  <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/?report=classic">Reporting of article retractions in bibliographic databases and online journals.</a>  J Med Libr Assoc. 2011 April; 99(2): 164–167.</p>
<p>McGuigan, G.S. and Russell, R.D.  (2008).  <a href="http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html">The Business of Academic Publishing: A Strategic Analysis of the Academic Journal Publishing Industry and its Impact on the Future of Scholarly Publishing</a>.  Electronic Journal of Academic and Special Librarianship.  Winter 2008; 9(3).</p>
<p>Simmons, J.P., Nelson, L.D. and Simonsohn, U.A.  (2012)  <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588">A 21 Word Solution</a>.  </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/#disqus_thread" data-disqus-identifier="2013/12/13/chasing-paper-3/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/chasing-paper.html">chasing-paper</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 12,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/" rel="bookmark" title="Permanent Link to &quot;Chasing Paper, Part 2&quot;">Chasing Paper, Part 2</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This is part two of a three part post brainstorming potential improvements to the journal article format.  Part one is <a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/">here</a>, part three is here <a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/">here</a>.</em></p>
<h3>The classic journal article format is not easily updated or corrected.</h3>
<p>Scientific understanding is constantly changing as phenomena are discovered and mistakes uncovered.  The classic journal article, however, is static.  When a serious flaw in an article is found, the best a paper-based system can do is issue a retraction, and hope that a reader going through past issues will eventually come across the change.</p>
<p>Surprisingly, retractions and corrections continue to go mostly unnoticed in the digital era.  Studies have shown that retracted papers go on to receive, on average, more than 10 post-retraction citations, with less than 10% of those citations acknowledging the retraction (<a href="http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf">Budd et al, 2011</a>).  Why is this happening?  While many article databases such as PubMed provide retraction notices, the articles themselves are often not amended.  Readers accessing papers directly from publishers’ websites, or from previously saved copies, can sometimes miss it.  A case study of 18 retracted articles found several which they classified as “high risk of missing [the] notice”, with no notice given in the text of the pdf or html copies themselves (<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/#mlab.1536-5050.99.2.010.sg002">Wright et al, 2011</a>).  It seems likely that corrections have even more difficulty being seen and acknowledged by subsequent researchers.</p>
<p>There are several technological solutions which can be tried.  One promising avenue would be the adoption of version control.  Also called revision control, this is a way of tracking all changes made to a project.  This technology has been used for decades in computer science and is becoming more and more popular - Wikipedia and Google Docs, for instance, both use version control.  Citations for a paper could reference the version of the paper then available, but subsequent readers would be notified that a more recent version could be viewed.  In addition to making it easy to see how articles have been changed, adopting such a system would acknowledge the frequency of retractions and corrections and the need to check for up to date information.</p>
<p>Another potential tool would be an alert system.  When changes are made to an article, the authors of all articles which cite it could be notified.  However, this would require the maintenance of up-to-date contact information for authors, and the adoption of communications standards across publishers (something that has been accomplished before with initiatives like <a href="http://www.crossref.org/">CrossRef</a>).
A more transformative approach would be to view papers not as static documents but as ongoing projects that can be updated and contributed to over time.  Projects could be tracked through version control from their very inception, allowing for a kind of <a href="http://www.theguardian.com/science/blog/2013/jun/05/trust-in-science-study-pre-registration">pre-registration</a>.  Replications and new analyses could be added to the project as they’re completed.  The most insightful questions and critiques from the public could lead to changes in new versions of the article.</p>
<h3>The classic journal article only recognizes certain kinds of contributions.</h3>
<p>When journal articles were first developed in the 1600s, the idea of crediting an author or authors must have seemed straightforward.  After all, most research was being done by individuals or very small groups, and there were no such things as curriculum vitae or tenure committees.  Over time, academic authorship has become the single most important factor in determining career success for individual scientists.  The limitations of authorship can therefore have an incredible impact on scientific progress.</p>
<p>There are two major problems with authorship as it currently functions, and they are sides of the same coin.  Authorship does not tell you what, precisely, each author did on a paper.  And authorship does not tell you who, precisely, is responsible for each part of a paper.
Currently, the authorship model provides only a vague idea of who is responsible for a paper.  While this is sometimes elaborated upon briefly in the footnotes, or mentioned in the article, more often readers employ simple heuristics.  In psychology, the first author is believed to have led the work, the last author to have provided physical and conceptual resources for the experiment, and any middle authors to have contributed in an unknown but significant way.  This is obviously not an ideal way to credit people, and often leads to disputes, with first authorship <a href="http://www.nature.com/naturejobs/science/articles/10.1038/nj7417-591a">sometimes misattributed</a>.  It has grown increasingly impractical as <a href="http://archive.sciencewatch.com/newsletter/2012/201207/multiauthor_papers/">multiauthor papers</a> have become more and more common.  What does authorship on a 500-author paper even mean?</p>
<p>The situation is even worse for people whose contributions are not awarded with authorship.  While contributions may be mentioned in the acknowledgements or cited in the body of the paper, neither of these have much impact when scientists are applying for jobs or up for tenure.  This gives them little motivation to do work which will not be recognized with authorship.  And such work is greatly needed.  The development of tools, the collection and release of open data sets, the creation of popularizations and teaching materials, and the deep and thorough review of others’ work - these are all done as favors or side projects, even though they are vital to the progress of research.
How can new technologies address these problems?  There have been few changes made in this area, perhaps due to the heavy weight of authorship in scientific life, although there are some tools like <a href="http://figshare.com/">Figshare</a> which allow users to share non-traditional materials such as datasets and posters in citable (and therefore creditable) form.  A more transformative change might be to use the version control system mentioned above.  Instead of tracking changes to the article from publishing onwards, it could follow the article from its beginning stages.  In that way, each change could be attributed to a specific person.</p>
<p>Another option might simply be to describe contributions in more detail.  Currently if I use your methodology wholesale, or briefly mention a finding of yours, I acknowledge you in the same way - a citation.  What if, instead, all significant contributions were listed?  Although space is not a constraint with digital articles, the human attention span remains limited, and so it might be useful to create common categories for contribution, such as reviewing the article, providing materials, doing analyses, or coming up with an explanation for discussion.</p>
<p>There are two other problems are worth mentioning in brief.  First, the phenomenon of <a href="https://en.wikipedia.org/wiki/Academic_authorship#Ghost_authorship">ghost authorship</a>, where substantial contributions to the running of a study or preparation of a manuscript go unacknowledged.  This is frequently done in industry-sponsored research to hide conflicts of interest.  If journal articles used a format where every contribution was tracked, ghost authorship would be impossible.  Another issue is the assignment of contact authors, the researchers on a paper who readers are invited to direct questions to.  Contact information can become outdated fairly quickly, causing access to data and materials to be lost; if contact information can be changed, or responsibility passed on to a new person, such loss can be prevented.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/#disqus_thread" data-disqus-identifier="2013/12/12/chasing-paper-2/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/chasing-paper.html">chasing-paper</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 11,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/" rel="bookmark" title="Permanent Link to &quot;Chasing Paper, Part 1&quot;">Chasing Paper, Part 1</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This is part one of a three part post.  Parts <a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/">two</a> and <a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/">three</a> have now been posted.</em></p>
<p>The academic paper is old - older than the steam engine, the pocket watch, the piano, and the light bulb.  The first journal, <a href="http://rstl.royalsocietypublishing.org/">Philosophical Transactions</a>, was published on March 6th, 1665.  Now that doesn’t mean that the journal article format is obsolete - many inventions much older are still in wide use today.  But after a third of a millennium, it’s only natural that the format needs some serious updating.</p>
<p>When brainstorming changes, it may be useful to think of the limitations of ink and paper.  From there, we can consider how new technologies can improve or even transform the journal article.  Some of these changes have already been widely adopted, while others have never even been debated.  Some are adaptive, using the greater storage capacity of computing to extend the functions of the classic journal article, while others are transformative, creating new functions and features only available in the 21st century.</p>
<p>The ideas below are suggestions, not recommendations - it may be that some aspects of the journal article format are better left alone.  But we all benefit from challenging our assumptions about what an article is and ought to be.</p>
<h3>The classic journal article format cannot convey the full range of information associated with an experiment.</h3>
<p>Until the rise of modern computing, there was simply no way for researchers to share all the data they collected in their experiments.  Researchers were forced to summarize: to gloss over the details of their methods and the reasoning behind their decisions and, of course, to provide statistical analyses in the place of raw data.  While fields like <a href="http://www.techrepublic.com/blog/european-technology/cern-where-the-big-bang-meets-big-data/">particle physics</a> and <a href="http://www.nytimes.com/2011/12/01/business/dna-sequencing-caught-in-deluge-of-data.html?pagewanted=all&amp;_r=0">genetics</a> continue to push the limits of memory, most experimenters now have the technical capacity to share all of their data.</p>
<p>Many journals have taken to publishing supplemental materials, although this rarely encompasses the entirety of data collected, or enough methodological detail to allow for independent replication.  There are plenty of explanations for this slow adoption, including ethical considerations around human subjects data, the potential to patent methods, or the cost to journals of hosting this extra materials.  But these are obstacles to address, not reasons to give up.  The potential benefits are enormous:  What if every published paper contained enough methodological detail that it could be independently replicated?  What if every paper contained enough raw data that it could be included in meta-analysis?  How much of meta-scientific work is never undertaken, because it's dependent on getting dozens or hundreds of contact authors to return your emails, and on universities to properly store data and materials?</p>
<p>Providing supplemental material, no matter how extensive, is still an adaptive change.  What might a transformative change look like?  Elsevier’s <a href="http://www.articleofthefuture.com/">Article of the Future</a> project attempts to answer that question with new, experimental formats that include videos, interactive models, and infographics.  These designs are just the beginning.  What if articles allowed readers to actually interact with the data and perform their own analyses?  <a href="https://en.wikipedia.org/wiki/Virtual_environment_software">Virtual environments</a> could be set up, lowering the barrier to independent verification of results.  What if authors reported when they made questionable methodological decisions, and allowed readers, where possible, to see the results when a variable was not controlled for, or a sample was not excluded?</p>
<h3>The classic journal article format is difficult to organize, index or search.</h3>
<p>New technology has already transformed the way we search the scientific literature.  Where before researchers were reliant on catalogues and indexes from publishers, and used abstracts to guess at relevance, databases such as PubMed and Google Scholar allow us to find all mentions of a term, tool, or phenomena across vast swathes of articles.  While searching databases is itself a skill, its one that allows us to search comprehensively and efficiently, and gives us more opportunities to explore.</p>
<p>Yet old issues of organization and curation remain.  Indexes used to speed the slow process of skimming through physical papers.  Now they’re needed to help researchers sort through the abundance of articles constantly being published.  With <a href="http://duncan.hull.name/2010/07/15/fifty-million/">tens of millions</a> of journal articles out there, how can we be sure we’re really accessing all the relevant literature?  How can we compare and synthesize the thousands of results one might get on a given search?</p>
<p>Special kinds of articles - reviews and meta-analyses - have traditionally helped us synthesize and curate information.  As discussed above, new technologies can help make meta-analyses more common by making it easier for researchers to access information about past studies.  We can further improve the search experience by creating more detailed <a href="https://en.wikipedia.org/wiki/Metadata_standards">metadata</a>.  Metadata, in this context, is the information attached to an article which lets us categorize it without having to read the article itself.  Currently, fields like title, author, date, and journal are quite common in databases.  More complicated fields less often adopted, but you can find metadata on study type, population, level of clinical trial (where applicable), and so forth.  What would truly comprehensive metadata look like?  Is it possible to store the details of experimental structure or analysis in machine-readable format - and is that even desirable?</p>
<p>What happens when we reconsider not the metadata but the content itself?  Most articles are structurally complex, containing literature reviews, methodological information, data, and analysis.  Perhaps we might be better served by breaking those articles down into their constituent parts.  What if methods, data, analysis were always published separately, creating a network of papers that were linked but discrete?  Would that be easier or harder to organize?  It may be that what we need here is not a better kind of journal article, but a new way of curating research entirely.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/#disqus_thread" data-disqus-identifier="2013/12/11/chasing-paper/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/">Posted at 11:30 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://osc.centerforopenscience.org/tag/chasing-paper.html">chasing-paper</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec  9,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/12/09/reviewer-statement-initiative/" rel="bookmark" title="Permanent Link to &quot;New “Reviewer Statement” Initiative Aims to (Further) Improve Community Norms Toward Disclosure&quot;">New “Reviewer Statement” Initiative Aims to (Further) Improve Community Norms Toward Disclosure</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/etienne-lebel.html" rel="author">Etienne LeBel</a>

   			   <p><img src="/images/etiennelebel.jpg" alt="Photo of Etienne LeBel" align="left" style="padding-right: 20px;" /></p>
<p>An Open Science Collaboration -- made up of Uri Simonsohn, Etienne LeBel, Don Moore, Leif D. Nelson, Brian Nosek, and Joe Simmons -- is glad to announce a new initiative aiming to improve community norms toward the disclosure of basic methodological information during the peer-review process. Endorsed by the Center for Open Science, the initiative involves a standard reviewer statement that any peer reviewer can include in their review requesting that authors add a statement to the paper confirming that they have disclosed all data exclusions, experimental conditions, assessed measures, and how they determined their samples sizes (following from the 21-word solution; Simmons, Nelson, &amp; Simonsohn, 2012, 2013; see also <a href="http://psychdisclosure.org/">PsychDisclosure.org</a>; LeBel et al., 2013). Here is the statement, which is <a href="http://osf.io/project/hadz3">available on the Open Science Framework</a>:</p>
<p><em>"I request that the authors add a statement to the paper confirming whether, for all experiments, they have reported all measures, conditions, data exclusions, and how they determined their sample sizes. The authors should, of course, add any additional text to ensure the statement is accurate. This is the standard reviewer disclosure request endorsed by the Center for Open Science (<a href="http://osf.io/project/hadz3">see http://osf.io/project/hadz3</a>). I include it in every review."</em></p>
<p>The idea originated from the realization that as peer reviewers, we typically lack fundamental information regarding how the data was collected and analyzed which prevents us from be able to properly evaluate the claims made in a submitted manuscript. Some reviewers interested in requesting such information, however, were concerned that such requests would make them appear selective and/or compromise their anonymity. Discussions ensued and contributors developed a standard reviewer disclosure request statement that overcomes these concerns and allows the community of reviewers to improve community norms toward the disclosure of such methodological information across all journals and articles.</p>
<p>Some of the contributors, including myself, were hoping for a reviewer statement with a bit more teeth. For instance, requesting the disclosure of such information as a requirement before accepting to review an article or requiring the re-review of a revised manuscript once the requested information has been disclosed. The team of contributors, however, ultimately decided that it would be better to start small to get acceptance, in order to maximize the probability that the initiative has an impact in shaping the community norms.</p>
<p>Hence, next time you are invited to review a manuscript for publication at any journal, please remember to include the reviewer disclosure statement!</p>
<p><strong>References</strong></p>
<p>LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K. R., Ratliff, K. A., &amp; Smith, C. T. (2013). <a href="http://www.google.com/url?q=http%3A%2F%2Fpps.sagepub.com%2Fcontent%2F8%2F4%2F424.full&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEcYQrJzKL33sb33l9teszIxVUNAg">PsychDisclosure.org: Grassroots support for reforming reporting standards in psychology.</a> Perspectives on Psychological Science, 8(4), 424-432. doi: 10.1177/1745691613491437</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2011) <a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1850704&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGqvW1VEcf3RzOb7zE0Y25FXdQHBQ">False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allow Presenting Anything as Significant.</a> Psychological Science, 22(11), 1359-1366.</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2012) <a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D2160588&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGt1mELWBFnDRS2Yh-E-qm_tAp_2A">A 21 Word Solution.</a> Dialogue: The Official Newsletter of the Society for Personality and Social Psychology, 26(2), 4-7.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/12/09/reviewer-statement-initiative/#disqus_thread" data-disqus-identifier="2013/12/09/reviewer-statement-initiative/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/12/09/reviewer-statement-initiative/">Posted at 11:30 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov 27,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/11/27/the-state-of-open-access/" rel="bookmark" title="Permanent Link to &quot;The State of Open Access&quot;">The State of Open Access</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p>To celebrate <a href="http://www.openaccessweek.org/">Open Access Week</a> last month, we asked people four questions about the state of open access and how it's changing.  Here are some in depth answers from two people working on open access: <a href="http://cyber.law.harvard.edu/~psuber/wiki/Peter_Suber">Peter Suber</a>, Director of the <a href="https://osc.hul.harvard.edu/">Harvard Office for Scholarly Communication</a> and the <a href="http://cyber.law.harvard.edu/research/hoap">Harvard Open Access Project</a>, and <a href="http://www.plos.org/staff/elizabeth-silva/">Elizabeth Silva</a>, associate editor at the Public Library of Science (<a href="http://www.plos.org/">PLOS</a>).</p>
<p><strong>How is your work relevant to the changing landscape of Open Access?  What would be a successful outcome of your work in this area?</strong></p>
<p><em>Elizabeth</em>:  PLOS is now synonymous with open access publishing, so it’s hard to believe that 10 years ago, when PLOS was founded, most researchers were not even aware that availability of research was a problem. We all published our best research in the best journals. We assumed our colleagues could access it, and we weren’t aware of (or didn’t recognize the problem with) the inability of people outside of the ivory tower to see this work. At that time it was apparent to the founders of PLOS, who were among the few researchers who recognized the problem, that the best way to convince researchers to publish open access would be for PLOS to become an open access publisher, and prove that OA could be a viable business model and an attractive publishing venue at the same time. I think that we can safely say that the founders of PLOS succeeded in this mission, and they did it decisively. </p>
<p>We’re now at an exciting time, where open access in the natural sciences is all but inevitable. We now get to work on new challenges, trying to solve other issues in research communication. </p>
<p><em>Peter</em>:  My current job has two parts. I direct the Harvard Office for Scholarly Communication (OSC), and I direct the Harvard Open Access Project (HOAP). The OSC aims to provide OA to research done at Harvard University. We implement Harvard's OA policies and maintain its OA repository. We focus on peer-reviewed articles by faculty, but are expanding to other categories of research and researchers. In my HOAP work, I consult pro bono with universities, scholarly societies, publishers, funding agencies, and governments, to help them adopt effective OA policies. HOAP also maintains a guide to good practices for university OA policies, manages the Open Access Tracking Project, writes reference pages on federal OA-related legislation, such as FASTR, and makes regular contributions to the Open Access Directory and the catalog of OA journals from society publishers. </p>
<p>To me success would be making OA the default for new research in every field and language. However, this kind of success more like a new plateau than a finish line. We often focus on the goal of OA itself, or the goal of removing access barriers to knowledge. But that's merely a precondition for an exciting range of new possibilities for making use of that knowledge. In that sense, OA is closer to the minimum than the maximum of how to take advantage of the internet for improving research. Once OA is the default for new research, we can give less energy to attaining it and more energy to reaping the benefits, for example, integrating OA texts with open data, improving the methods of meta-analysis and reproducibility, and building better tools for knowledge extraction, text and data mining, question answering, reference linking, impact measurement, current awareness, search, summary, translation, organization, and recommendation.</p>
<p>From the researcher's side, making OA the new default means that essentially all the new work they write, and essentially all the new work they want to read, will be OA. From the publisher's side, making OA the new default means that sustainability cannot depend on access barriers that subtract value, and must depend on creative ways to add value to research that is already and irrevocably OA. </p>
<p><strong>How do you think the lack of Open Access is currently impacting how science is practiced?</strong></p>
<p><em>Peter</em>:  The lack of OA slows down research. It distorts inquiry by making the retrievability of research a function of publisher prices and library budgets rather than author consent and internet connectivity. It hides results that happen to sit in journals that exceed the affordability threshold for you or your institution. It limits the correction of scientific error by limiting the number of eyeballs that can examine new results. It prevents the use of text and data mining to supplement human analysis with machine analysis. It hinders the reproducibility of research by excluding many who would want to reproduce it. At the same time, and ironically, it increases the inefficient duplication of research by scholars who don't realize that certain experiments have already been done. </p>
<p>It prevents journalists from reading the latest developments, reporting on them, and providing direct, usable links for interested readers. It prevents unaffiliated scholars and the lay public from reading new work in which they may have an interest, especially in the humanities and medicine. It blocks research-driven industries from creating jobs, products, and innovations. It prevents taxpayers from maximizing the return on their enormous investment in publicly-funded research.</p>
<p>I assume we're talking about research that authors publish voluntarily, as opposed to notes, emails, and unfinished manuscripts, and I assume we're talking about research that authors write without expectation of revenue. If so, then the lack of OA harms research and researchers without qualification. The lack of OA benefits no one except conventional publishers who want to own it, sell it, and limit the audience to paying customers. </p>
<p><em>Elizabeth</em>:  There is a prevailing idea that those that need access to the literature already have it; that those that have the ability to understand the content are at institutions that can afford the subscriptions. First, this ignores the needs of physicians, educators, science 
communicators, and smaller institutions and companies. More fundamentally, limiting access to knowledge, so that rests in the hands of an elite 1%, is archaic, backwards, and counterproductive. There has never been a greater urgency to find solutions to problems that fundamentally threaten human existence – climate change, disease transmission, food security – and in the face of this why would we advocate limited dissemination of knowledge? Full adoption of open access has the potential to fundamentally change the pace of scientific progress, as we make this information available to everyone, worldwide.</p>
<p>When it comes to issues of reproducibility, fraud or misreporting, all journals face similar issues regardless of the business model. Researchers design their experiments and collect their data long before they decide the publishing venue, and the quality of the reporting likely won’t change based on whether the venue is OA. I think that these issues are better tackled by requirements for open data and improved reporting. Of course these philosophies are certainly intrinsically linked – improved transparency and access can only improve matters.</p>
<p><strong>What do you think is the biggest reason that people resist Open Access?  Do you think there are good reasons for not making a paper open access?</strong></p>
<p><em>Elizabeth</em>:  Of course there are many publishers who resist open access, which reflects a need to protect established revenue streams. In addition to large commercial publishers, there are a lot of scholarly societies whose primary sources of income are the subscriptions for the journals they publish.</p>
<p>Resistance from authors, in my experience, comes principally in two forms. The first is linked to the impact factor, rather than the business model. Researchers are stuck in a paradigm that requires them to publish as ‘high’ as possible to achieve career advancement. While there are plenty of high impact OA publications with which people choose to publish, it just so happens that the highest are subscription journals. We know that open access increases utility, visibility and impact of individual pieces of research, but the fallacy that a high impact journal is equivalent to high impact research persists.</p>
<p>The second reason cited is that the cost is prohibitory. This is a problem everyone at PLOS can really appreciate, and we very much sympathize with authors who do not have the money in their budget to pay author publication charges (APCs). However, it’s a problem that should really be a lot easier to overcome. If research institutions were to pay publication fees, rather than subscription fees, they would save a fortune; a few institutions have realized this and are paying the APCs for authors who choose to go OA. It would also help if funders could recognize publishing as an intrinsic part of the research, folding the APC into the grant. We are also moving the technology forward in an effort to reduce costs, so that savings can be passed onto authors. PLOS ONE has been around for nearly 7 years, and the fees have not changed. This reflects efforts to keep costs as low as we can. Ironically, the biggest of the pay-walled journals already charge authors to publish: for example, it can be between $500 and $1000 for the first color figure, and a few hundred for each additional one; on top of this there are page charges and reprint costs. Not only is the public paying for the research and the subscription, they are paying for papers that they can’t read.</p>
<p><em>Peter</em>:  There are no good reasons for not making a paper OA, or at least for not wanting to. </p>
<p>There are sometimes reasons not to publish in an OA journal. For example, the best journals in your field may not be OA. Your promotion and tenure committee may give you artificial incentives to limit yourself to a certain list of journals. Or the best OA journals in your field may charge publication fees which your funder or employer will not pay on your behalf. However, in those cases you can publish in a non-OA journal and deposit the peer-reviewed manuscript in an OA repository. </p>
<p>The resistance of non-OA publishers is easier to grasp. But if we're talking about publishing scholars, not publishers, then the largest cause of resistance by far is misunderstanding. Far too many researchers still accept false assumptions about OA, such as these 10:</p>
<p>--that the only way to make an article OA is to publish it in an OA journal
--that all or most OA journals charge publication fees
--that all or most publication fees are paid by authors out of pocket
--that all or most OA journals are not peer reviewed
--that peer-reviewed OA journals cannot use the same standards and even the same people as the best non-OA journals
--that publishing in a non-OA journal closes the door on lawfully making the same article OA
--that making work OA makes it harder rather than easier to find
--that making work OA limits rather than enhances author rights over it
--that OA mandates are about submitting new work to OA journals rather than depositing it in OA repositories, or
--that everyone who needs access already has access. </p>
<p>In a <a href="http://www.theguardian.com/higher-education-network/blog/2013/oct/21/open-access-myths-peter-suber-harvard">recent article</a> in <em>The Guardian</em> I corrected six of the most widespread and harmful myths about OA. In a <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:4322571">2009 article</a>, I corrected 25. And in my 2012 <a href="http://bit.ly/oa-book">book</a>, I tried to take on the whole legendarium. </p>
<p><strong>How has the Open Access movement changed in the last five years?  How do you think it will change in the next five years?</strong></p>
<p><em>Peter</em>:  OA has been making unmistakable progress for more than 20 years. Five years ago we were not in a qualitatively different place. We were just a bit further down the slope from where we are today.</p>
<p>Over the next five years, I expect more than just another five years' worth of progress as usual. I expect five years' worth of progress toward the kind of success I described in my answer to your first question. In fact, insofar as progress tends to add cooperating players and remove or convert resisting players, I expect five years' worth of compound interest and acceleration. </p>
<p>In some fields, like particle physics, OA is already the default. In the next five years we'll see this new reality move at an uneven rate across the research landscape. Every year more and more researchers will be able to stop struggling for access against needless legal, financial, and technical barriers. Every year, those still struggling will have the benefit of a widening circle of precedents, allies, tools, policies, best practices, accommodating publishers, and alternatives to publishers.</p>
<p>Green OA mandates are spreading among universities. They're also spreading among funding agencies, for example, in the US, the EU, and global south. This trend will definitely continue, especially with the support it has received from Global Research Council, Science Europe, the G8 Science Ministers, and the World Bank. </p>
<p>With the exception of the UK and the Netherlands, countries adopting new OA policies are learning from the experience of their predecessors and starting with green. I've argued in many places that mandating gold OA is a mistake. But it's a mistake mainly for historical reasons, and historical circumstances will change. Gold OA mandates are foolish today in part because too few journals are OA, and there's no reason to limit the freedom of authors to publish in the journals of their choice. But the percentage of peer-reviewed journals that are OA is growing and will continue to grow. (Today it's about 30%.) Gold OA mandates are also foolish today because gold OA is much more expensive than green OA, and there's no reason to compromise the public interest in order to guarantee revenue for non-adaptive publishers. But the costs of OA journals will decline, as the growing number of OA journals compete for authors, and the money to pay for OA journals will grow as libraries redirect money from conventional journals to OA. </p>
<p>We'll see a rise in policies linking deposit in repositories with research assessment, promotion, and tenure. These policies were pioneered by the University of Liege, and since adopted at institutions in nine countries, and recommended by the Budapest Open Access Initiative, the UK House of Commons Select Committee on Business, Innovation and Skills, and the Mediterranean Open Access Network. Most recently, this kind of policy has been proposed at the national level by the Higher Education Funding Council for England. If it's adopted, it will mitigate the damage of a gold-first policy in the UK. A similar possibility has been suggested for the Netherlands.</p>
<p>I expect we'll see OA in the humanities start to catch up with OA in the sciences, and OA for books start to catch up with OA for articles. But in both cases, the pace of progress has already picked up significantly, and so has the number of people eager to see these two kinds of progress accelerate.</p>
<p>The recent decision that Google's book scanning is fair use means that a much larger swath of print literature will be digitized, if not in every country, then at least in the US, and if not for OA, then at least for searching. This won't open the doors to vaults that have been closed, but it will open windows to help us see what is inside.</p>
<p>Finally, I expect to see evolution in the genres or containers of research. Like most people, I'm accustomed to the genres I grew up with. I love articles and books, both as a reader and author. But they have limitations that we can overcome, and we don't have to drop them to enhance them or to create post-articles and post-books alongside them. The low barriers to digital experimentation mean that we can try out new breeds until we find some that carry more advantages than disadvantages for specific purposes. Last year I sketched out one idea along these lines, which I call an <a href="http://dash.harvard.edu/bitstream/handle/1/10055732/12-02-12.html?sequence=2#rack">evidence rack</a>, but it's only one in an indefinitely large space constrained only by the limits on our imagination. </p>
<p><em>Elizabeth</em>: It’s starting to feel like universal open access is no longer “if” but “when”. In the next five years we will see funders and institutions recognize the importance of access and adopt policies that mandate and financially support OA; resistance will fade away, and it will simply be the way research is published. As that happens, I think the OA movement will shift towards tackling other issues in research communication: providing better measures of impact in the form of article level metrics, decreasing the time to publication, and improving reproducibility and utility of research.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/11/27/the-state-of-open-access/#disqus_thread" data-disqus-identifier="2013/11/27/the-state-of-open-access/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/11/27/the-state-of-open-access/">Posted at  2:15 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov 20,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/11/20/theoretical-amnesia/" rel="bookmark" title="Permanent Link to &quot;Theoretical Amnesia&quot;">Theoretical Amnesia</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/denny-borsboom.html" rel="author">Denny Borsboom</a>

   			   <p><img src="/images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /></p>
<p>In the past few months, the Center for Open Science and its associated enterprises have gathered enormous support in the community of psychological scientists. While these developments are happy ones, in my view, they also cast a shadow over the field of psychology: clearly, many people think that the activities of the Center for Open Science, like organizing massive replication work and promoting preregistration, are <em>necessary</em>. That, in turn, implies that something in the current scientific order is seriously <em>broken</em>. I think that, apart from working towards improvements, it is useful to investigate what that something is. In this post, I want to point towards a factor that I think has received too little attention in the public debate; namely, the near absence of unambiguously formalized scientific theory in psychology.</p>
<p>Scientific theories are perhaps the most bizarre entities that the scientific imagination has produced. They have incredible properties that, if we weren’t so familiar with them, would do pretty well in a <em>Harry Potter</em> novel. For instance, scientific theories allow you to work out, on a piece of paper, what would happen to stuff in conditions that aren’t actually realized. So you can figure out whether an imaginary bridge will stand or collapse in imaginary conditions. You can do this by simply just feeding some imaginary quantities that your imaginary bridge would have (like its mass and dimensions) to a scientific theory (say, Newton’s) and out comes a prediction on what will happen. In the more impressive cases, the predictions are so good that you can actually design the entire bridge on paper, then build it according to specifications (by systematically mapping empirical objects to theoretical terms), and then the bridge will do precisely what the theory says it should do. No surprises.</p>
<p>That’s how they put a man on the moon and that’s how they make the computer screen you’re now looking at. It’s all done in theory before it’s done for real, and that’s what makes it possible to construct complicated but functional pieces of equipment. This is, in effect, why scientific theory makes technology possible, and therefore this is an absolutely central ingredient of the scientific enterprise which, <em>without</em> technology, would be much less impressive than it is.</p>
<p>It’s useful to take stock here, and marvel. A good scientific theory allows you infer what would happen to things in certain situations <em>without creating the situations</em>. Thus, scientific theories are crystal balls that actually work. For this reason, some philosophers of science have suggested that scientific theories should be interpreted as <em>inference tickets</em>. Once you’ve got the ticket, you get to sidestep all the tedious empirical work. Which is great, because empirical work is, well, tedious. Scientific theories are thus exquisitely suited to the needs of lazy people.</p>
<p>My field – psychology – unfortunately does not afford much of a lazy life. We don’t have theories that can offer predictions sufficiently precise to intervene in the world with appreciable certainty. That’s why there exists no such thing as a psychological engineer. And that’s why there are fields of theoretical physics, theoretical biology, and even theoretical economics, while there is no parallel field of theoretical psychology. It is a sad but, in my view, inescapable conclusion: we don’t have much in the way of scientific theory in psychology. For this reason, we have very few inference tickets – let alone inference tickets that work.</p>
<p>And that’s why psychology is so hyper-ultra-mega empirical. We never know how our interventions will pan out, because we have no theory that says how they will pan out (incidentally, that’s also why we need preregistration: in psychology, predictions are made by individual researchers rather than by standing theory, and you can’t trust people the way you can trust theory). The upshot is that, if we want to know what would happen if we did X, we have to actually do X. Because we don’t have inference tickets, we never get to take the shortcut. We always have to wade through the empirical morass. Always.</p>
<p>This has important consequences. For instance, as a field has less theory, it has to leave more to the data. Since you can’t learn anything from data without the armature of statistical analysis, a field without theory tends to grow a thriving statistical community. Thus, the role of statistics grows as soon as the presence of scientific theory wanes. In extreme cases, when statistics has entirely taken over, fields of inquiry can actually develop a kind of philosophical disorder: theoretical amnesia. In fields with this disorder, researchers no longer know what a theory is, which means that they can neither recognize its presence nor its absence. In such fields, for instance, a statistical model – like a factor model – can come to occupy the vacuum created by the absence of theory. I am often afraid that this is precisely what has happened with the advent of “theories” like those of general intelligence (a single factor model) and the so-called “Big Five” of personality (a five-factor model). In fact, I am afraid that this happened in many fields in psychology, where statistical models (which, in their barest null-hypothesis testing form, are misleadingly called “effects”) rule the day.</p>
<p>If your science thrives on experiment and statistics, but lacks the power of theory, you get peculiar problems. Most importantly, you get slow. To see why, it’s interesting to wonder how psychologists would build a bridge, if they were to use their typical methodological strategies. Probably, they would build a thousand bridges, record whether they stand or fall, and then fit a regression equation to figure out which properties are predictive of the outcome. Predictors would be chosen on the basis of statistical significance, which would introduce a multiple testing problem. In response, some of the regressors might be clustered through factor analysis, to handle the overload of predictive variables. Such analyses would probably indicate lots of structure in the data, and psychologists would likely find that the bridges’ weight, size, and elasticity loads on a single latent “strength factor”, producing the “theory” that bridges higher on the “strength factor” are less likely to fall down. Cross validation of the model would be attempted by reproducing the analysis in a new sample of a thousand bridges, to weed out chance findings. It’s likely that, after many years of empirical research, and under a great number of “context-dependent” conditions that would be poorly understood, psychologists would be able to predict a modest but significant proportion of the variance in the outcome variable. Without a doubt, it would ta  ke a thousand years to establish empirically what Newton grasped in a split second, as he wrote down his F=m*a.</p>
<p>Because increased reliance on empirical data makes you so incredibly slow, it also makes you susceptible to fads and frauds. A good theory can be tested in an infinity of ways, many of which are directly available to the interested reader (this is what give classroom demonstrations such enormous evidential force). But if your science is entirely built on generalizations derived from specifics of tediously gathered experimental data, you can’t really test these generalizations without tediously gathering the same, or highly similar, experimental data. That’s not something that people typically like to do, and it’s certainly not what journals want to print. As a result, a field can become dominated by poorly tested generalizations. When that happens, you’re in very big trouble. The reason is that your scientific field becomes susceptible to the equivalent of what evolutionary theorists call free riders: people who capitalize on the invested honest work of others by consistently taking the moral shortcut. Free riders can come to rule a scientific field if two conditions are satisfied: (a) fame is bestowed on whoever dares to make the most <em>adventurous</em> claims (rather than the most <em>defensible</em> ones), and (b) it takes longer to falsify a bogus claim than it takes to become famous. If these conditions are satisfied, you can build your scientific career on a fad and get away with it. By the time they find out your work really doesn’t survive detailed scrutiny, you’re sitting warmly by the fire in the library of your National Academy of Sciences<sub>1</sub>.</p>
<p>Much of our standard methodological teachings in psychology rest on the implicit assumption that scientific fields are similar if not identical in their methodological setup. That simply isn’t true. Without theory, the scientific ball game has to be played by different rules. I think that these new rules are now being invented: without good theory, you need fast acting replication teams, you need a reproducibility project, and you need preregistered hypotheses. Thus, the current period of crisis may lead to extremely important methodological innovations – especially those that are crucial in fields that are low on theory.</p>
<p>Nevertheless, it would be extremely healthy if psychologists received more education in fields which do have some theories, even if they are empirically shaky ones, like you often see in economics or biology. In itself, it’s no shame that we have so little theory: psychology probably has the hardest subject matter ever studied, and to change that may very well take a scientific event of the order of Newton’s discoveries. I don’t know how to do it and I don’t think anybody else knows either. But what we can do is keep in contact with other fields, and at least try to remember what theory is and what it’s good for, so that we don’t fall into theoretical amnesia. As they say, it’s the unknown unknowns that hurt you most.</p>
<p><sub>1  Caveat: I am not saying that people do this on purpose. I believe that free riders are typically unaware of the fact that they are free riders – people are very good at labeling their own actions positively, especially if the rest of the world says that they are brilliant. So, if you think this post isn’t about you, that could be entirely wrong. In fact, I cannot even be   sure that this post isn’t about me.</sub></p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/11/20/theoretical-amnesia/#disqus_thread" data-disqus-identifier="2013/11/20/theoretical-amnesia/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/11/20/theoretical-amnesia/">Posted at  2:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov 13,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/11/13/report-findings-more-transparently/" rel="bookmark" title="Permanent Link to &quot;Let’s Report Our Findings More Transparently – As We Used to Do&quot;">Let’s Report Our Findings More Transparently – As We Used to Do</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/etienne-lebel.html" rel="author">Etienne LeBel</a>

   			   <p><img src="/images/etiennelebel.jpg" alt="Photo of Etienne LeBel" align="left" style="padding-right: 20px;" /></p>
<p>In 1959, Festinger and Carlsmith reported the results of an experiment that spawned a voluminous body of research on cognitive dissonance. In that experiment, all subjects performed a boring task. Some participants were paid $1 or $20 to tell the next subject the task was interesting and fun whereas participants in a control condition did no such thing. All participants then indicated how enjoyable they felt the task was, their desire to participate in another similar experiment, and the scientific importance of the experiment. The authors hypothesized that participants paid $1 to tell the next participant that the boring task they just completed was interesting would experience internal dissonance, which could be reduced by altering their attitudes on the three outcomes measures. A little known fact about the results of this experiment, however, is that only on <em>one</em> of these outcome measures did a statistically significant effect emerge across conditions. The authors reported that subjects paid $1 enjoyed the task more than those paid $20 (or the control participants), but no statistically significant differences emerged on the other two measures. </p>
<p>In another highly influential paper, Word, Zanna, and Cooper (1974) documented the self-fulfilling prophecies of racial stereotypes. The researchers had white subjects interview trained white and black applicants and coded for six non-verbal behaviors of immediacy (distance, forward lean, eye contact, shoulder orientation, interview length, and speech error rate). They found that that white interviewers treated black applicants with lower levels of non-verbal immediacy than white applicants. In a follow-up study involving white subjects only, applicants treated with less immediate non-verbal behaviors were judged to perform less well during the interview than applicants treated with more immediate non-verbal behaviors. A fascinating result, however, a little known fact about this paper is that only three of the six measures of non-verbal behaviors assessed in the first study (and subsequently used in the second study) were statistically significant. </p>
<p>What do these two examples make salient in relation to how psychologists report their results nowadays? Regular readers of prominent journals like <em>Psychological Science</em> and <em>Journal of Personality and Social Psychology</em> may see what I’m getting at: It is very rare these days to see articles in these journals wherein half or most of the reported dependent variables (DVs) fail to show statistically significant effects. Rather, one typically sees squeaky-clean looking articles where all of the DVs show statistically significant effects across all of the studies, with an occasional mention of a DV achieving “marginal significance” (Giner-Sorolla, 2012).</p>
<p>In this post, I want us to consider the possibility that psychologists’ reporting practices may have changed in the past 50 years. This then raises the question as to how this came about. One possibility is that as incentives became increasingly more perverse in psychology (Nosek,Spies, &amp; Motyl, 2012), some researchers realized that they could out-compete their peers by reporting “cleaner” looking results which would appear more compelling to editors and reviewers (Giner-Sorolla, 2012). For example, decisions were made to simply not report DVs that failed to show significant differences across conditions or that only achieved “marginal significance”. Indeed, nowadays sometimes even editors or reviewers will demand that such DVs not be reported (see <a href="http://PsychDisclosure.org">PsychDisclosure.org</a>; LeBel et al., 2013). A similar logic may also have contributed to researchers’ deciding not to fully report independent variables that failed to yield statistically significant effects and not fully reporting the exclusion of outlying participants due to fear that this information may raise doubts among the editor/reviewers and hurt their chance of getting their foot in the door (i.e., at least getting a revise-and-resubmit).</p>
<p>An alternative explanation is that new tools and technology have given us the ability to measure a greater number of DVs, which makes it more difficult to report on all them. For example, neuroscience (e.g., EEG, fMRI) and eye-tracking methods yield multitudes of analyzable data that were not previously available. Though this is undeniably true, the internet and online article supplements gives us the ability to fully report our methods and results and use the article to draw attention to the most interesting data.</p>
<p>Considering the possibility that psychologists’ reporting practices have changed in the past 50 years has implications for how to construe recent calls for the need to raise reporting standards in psychology (LeBel et al., 2013; Simmons, Nelson, &amp; Simonsohn, 2011; Simmons, Nelson, &amp; Simonsohn, 2012). Rather than seeing these calls as rigid new requirements that might interfere with exploratory research and stifle our science, one could construe such calls as a plea to revert back to the fuller reporting of results that used to be the norm in our science. From this perspective, it should not be viewed as overly onerous or authoritarian to ask researchers to disclose all excluded observations, all tested experimental conditions, all analyzed measures, and their data collection termination rule (what I’m now calling the BASIC 4 methodological categories covered by PsychDisclosure.org and Simmons et al.’s, 2012 21-word solution). It would simply be the way our forefathers used to do it.</p>
<p><strong>References</strong></p>
<p>Festinger, L. &amp; Carlsmith, J. M. (1959). <a href="http://psychclassics.yorku.ca/Festinger/">Cognitive consequences of forced compliance.</a> <em>The Journal of Abnormal and Social Psychology</em>, 58(2), Mar 1959, 203-210. doi: 10.1037/h0041593</p>
<p>Giner-Sorolla, R. (2012). Science or art? <a href="http://pps.sagepub.com/content/7/6/562.full">How esthetic standards grease the way through the publication bottleneck but undermine science.</a> <em>Perspectives on Psychological Science</em>, 7(6), 562–571. 10.1177/1745691612457576</p>
<p>LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K. R., Ratliff, K. A., &amp; Smith, C. T. (2013). <a href="http://pps.sagepub.com/content/8/4/424.full">PsychDisclosure.org: Grassroots support for reforming reporting standards in psychology.</a> Perspectives on Psychological Science, 8(4), 424-432. doi: 10.1177/1745691613491437</p>
<p>Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012).  <a href="http://pps.sagepub.com/content/7/6/615.full">Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability.</a> Perspectives on Psychological Science, 7, 615-631. doi: 10.1177/1745691612459058</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2011) <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704">False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allow Presenting Anything as Significant.</a> <em>Psychological Science</em>, 22(11), 1359-1366.</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2012) <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588">A 21 Word Solution</a> <em>Dialogue: The Official Newsletter of the Society for Personality and Social Psychology</em>, 26(2), 4-7.</p>
<p>Word, C. O., Zanna, M. P., &amp; Cooper, J. (1974). <a href="https://catalyst.uw.edu/workspace/file/download/72b19d8df8de321d0fed3803109a14aaf7c7b6f3800f40f477d902ab0a5e173b">The nonverbal mediation of self-fulfilling prophecies in interracial interaction.</a> <em>Journal of Experimental Social Psychology</em>, 10(2), 109–120. doi: 10.1016/0022-1031(74)90059-6</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/11/13/report-findings-more-transparently/#disqus_thread" data-disqus-identifier="2013/11/13/report-findings-more-transparently/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/11/13/report-findings-more-transparently/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov  3,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/" rel="bookmark" title="Permanent Link to &quot;Increasing statistical power in psychological research without increasing sample size&quot;">Increasing statistical power in psychological research without increasing sample size</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/sean-mackinnon.html" rel="author">Sean Mackinnon</a>

   			   <p><strong>What is statistical power and precision?</strong></p>
<p>This post is going to give you some practical tips to increase statistical power in your research. Before going there though, let’s make sure everyone is on the same page by starting with some definitions. </p>
<p>Statistical power is the probability that the test will reject the null hypothesis when the null hypothesis
is false. Many authors suggest a statistical power rate of at least .80, which corresponds to an 80% probability of <em>not</em> committing a<a href="http://www.investopedia.com/terms/t/type-ii-error.asp"> Type II error</a>.</p>
<p>Precision refers to the width of the<a href="http://www.psychologicalscience.org/index.php/publications/observer/2010/april-10/understanding-confidence-intervals-cis-and-effect-size-estimation.html"> confidence interval</a> for an <a href="http://www.leeds.ac.uk/educol/documents/00002182.htm">effect size</a>. The smaller this width, the more precise your results are. For 80% power, the confidence interval width will be roughly plus or minus 70% of the population effect size (<a href="http://www.ncbi.nlm.nih.gov/pubmed/8017747">Goodman &amp; Berlin, 1994</a>). Studies that have low precision have a greater probability of both<a href="http://www.investopedia.com/terms/t/type_1_error.asp"> Type I</a> and<a href="http://www.investopedia.com/terms/t/type-ii-error.asp"> Type II</a> errors (<a href="http://dx.doi.org/10.1038/nrn3475">Button et al., 2013</a>).</p>
<p>To get an idea of how this works, here are a few examples of the sample size required to achieve .80 power for small, medium, and large (<a href="http://www2.psych.ubc.ca/~schaller/349and449Readings/Cohen1992.pdf">Cohen, 1992</a>) correlations as well as the expected confidence intervals</p>
<table>
  <tr>
    <td>Population Effect Size</td>
    <td>Sample Size for 80% Power</td>
    <td>Estimated Precision</td>
  </tr>
  <tr>
    <td>r = .10</td>
    <td>782</td>
    <td>95% CI [.03, .17]</td>
  </tr>
  <tr>
    <td>r = .30</td>
    <td>84</td>
    <td>95% CI [.09, .51]</td>
  </tr>
  <tr>
    <td>r = .50</td>
    <td>29</td>
    <td>95% CI [.15, .85]</td>
  </tr>
</table>

<p><strong>Studies in psychology are grossly underpowered</strong></p>
<p>Okay, so now you know what power is. But why should you care? Fifty years ago,<a href="http://dx.doi.org/10.1037/h0045186"> Cohen (1962)</a> estimated the statistical power to detect a medium effect size in abnormal psychology was about .48. That’s a false negative rate of 52%, which is no better than a coin-flip! The situation has improved slightly, but it’s still a serious problem today. For instance, one review suggested only 52% of articles in the applied psychology literature achieved .80 power for a medium effect size (<a href="http://dx.doi.org/10.1111/j.1744-6570.1996.tb01793.x">Mone et al., 1996</a>). This is in part because psychologists are studying small effects. One massive review of 322 meta-analyses including 8 million participants (<a href="http://dx.doi.org/10.1037/1089-2680.7.4.331">Richard et al., 2003</a>) suggested that the average effect size in social psychology is relatively small (<em>r</em> = .21). To put this into perspective, you’d need 175 participants to have .80 power for a simple correlation between two variables at this effect size. This gets even worse when we’re studying interaction effects. One review suggests that the average effect size for interaction effects is even smaller (f2 = .009), which means that sample sizes of around 875 people would be needed to achieve .80 power (<a href="http://dx.doi.org/10.1037/0021-9010.90.1.94">Aguinis et al., 2005</a>). Odds are, if you took the time to design a research study and collect data, you want to find a relationship if one really exists. You don’t want to "miss" something that is really there. More than this, you probably want to have a reasonably precise estimate of the effect size (it’s not that impressive to just say a relationship is positive and probably non-zero). Below, I discuss concrete strategies for improving power and precision.</p>
<p><strong>What can we do to increase power?</strong></p>
<p>It is well-known that increasing sample size increases statistical power and precision. Increasing the population effect size increases statistical power, but has no effect on precision (<a href="http://dx.doi.org/10.1146/annurev.psych.59.103006.093735">Maxwell et al., 2008</a>). Increasing sample size improves power and precision by reducing <a href="http://www.investopedia.com/terms/s/standard-error.asp">standard error</a> of the effect size. Take a look at this formula for the confidence interval of a linear regression coefficient (<a href="http://dx.doi.org/10.1037/1082-989X.2.1.3">McClelland, 2000</a>):</p>
<p><img src="/images/power-equation.png" alt="Power Equation"> </p>
<p>MSE is the mean square error, n is the sample size, Vx is the variance of X, and (1-<em>R</em>2) is the proportion of the variance in X not shared by any other variables in the model. Okay, hopefully you didn’t nod off there. There’s a reason I’m showing you this formula. In this formula, decreasing any value in the numerator (MSE) or increasing anything in the denominator (n, Vx, 1-<em>R</em>2) will decrease the standard error of the effect size, and will thus increase power and precision. This formula demonstrates that there are at least three other ways to increase statistical power aside from sample size: (a) Decreasing the mean square error; (b) increasing the variance of x; and (c) increasing the proportion of the variance in X not shared by any other predictors in the model. Below, I’ll give you a few ways to do just that. </p>
<p><strong>Recommendation 1: Decrease the mean square error</strong></p>
<p>Referring to the formula above, you can see that decreasing the mean square error will have about the same impact as increasing sample size. Okay. You’ve probably heard the term "<a href="http://stats.oecd.org/glossary/detail.asp?ID=3716">mean square error</a>" before, but the definition might be kind of fuzzy. Basically, your model makes a prediction for what the outcome variable (Y) should be, given certain values of the predictor (X). Naturally, it’s not a perfect prediction because you have measurement error, and because there are other important variables you probably didn’t measure. The mean square error is the difference between what your model predicts, and what the true values of the data actually are.  So, anything that improves the quality of your measurement or accounts for potential confounding variables will reduce the mean square error, and thus improve statistical power. Let’s make this concrete. Here are three specific techniques you can use:</p>
<p>a)      <em>Reduce measurement error by using more reliable measures</em>(i.e., better internal consistency, test-retest reliability, inter-rater reliability, etc.). You’ve probably read that .70 is the "rule-of-thumb" for acceptable reliability. Okay, sure. That’s publishable. But consider this: Let’s say you want to test a correlation coefficient. Assuming both measures have a reliability of .70, your observed correlation will be about 1.43 times <em>smaller</em> than the true population parameter (I got this using<a href="http://jeromyanglim.blogspot.ca/2009/09/adjusting-correlations-for-reliability.html"> Spearman’s correlation attenuation formula</a>).  Because you have a smaller observed effect size, you end up with less statistical power. Why do this to yourself? Reduce measurement error. If you’re an experimentalist, make sure you execute your experimental manipulations exactly the same way each time, preferably by automating them. Slight variations in the manipulation (e.g., different locations, slight variations in timing) might reduce the reliability of the manipulation, and thus reduce power. </p>
<p>b)      <em>Control for confounding variables.</em> With correlational research, this means including control variables that predict the outcome variable, but are relatively uncorrelated with other predictor variables. In experimental designs, this means taking great care to control for as many possible confounds as possible. In both cases, this reduces the mean square error and improves the overall predictive power of the model – and thus, improves statistical power. Be careful when adding control variables into your models though: There are diminishing returns for adding covariates. Adding a couple of good covariates is bound to improve your model, but you always have to balance predictive power against model complexity. Adding a large number of predictors can sometimes lead to overfitting (i.e., the model is just describing noise or random error) when there are too many predictors in the model relative to the sample size. So, controlling for a couple of good covariates is generally a good idea, but too many covariates will probably make your model worse, not better, especially if the sample is small. </p>
<p>c)      <em>Use repeated-measures designs.</em> Repeated measures designs are where participants are measured multiple times  (e.g., once a day surveys, multiple trials in an experiment, etc.). Repeated measures designs reduce the mean square error by partitioning out the variance due to individual participants. Depending on the kind of analysis you do, it can also increase the degrees of freedom for the analysis substantially. For example, you might only have 100 participants, but if you measured them once a day for 21 days, you’ll actually have 2100 data points to analyze. The data analysis can get tricky and the interpretation of the data may change, but many multilevel and structural equation models can take advantage of these designs by examining each <em>measurement occasion </em>(i.e., each day, each trial, etc.) as the unit of interest, instead of each individual participant. Increasing the degrees of freedom is much like increasing the sample size in terms of increasing statistical power.  I’m a big fan of repeated measures designs, because they allow researchers to collect a lot of data from fewer participants.</p>
<p><strong>Recommendation 2: Increase the variance of your predictor variable</strong></p>
<p>Another less-known way to increase statistical power and precision is to increase the variance of your predictor variables (X). The formula listed above shows that doubling the variance of X is has the same impact on increasing statistical precision as doubling the sample size does! So it’s worth figuring this out. </p>
<p>a)      <em>In correlational research, use more comprehensive continuous measures. </em>That is, there should be a large possible range of values endorsed by participants. However, the measure should also capture many different aspects of the construct of interest; artificially increasing the range of X by adding redundant items (i.e., simply re-phrasing existing items to ask the same question) will actually hurt the validity of the analysis. Also, avoid dichotomizing your measures (e.g., median splits), because this reduces the variance and typically reduces power (<a href="http://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf">MacCallum et al., 2002</a>).</p>
<p>b)      <em>In experimental research, unequally allocating participants to each condition can improve statistical power</em>. For example, if you were designing an experiment with 3 conditions (let’s say low, medium, or high self-esteem). Most of us would equally assign participants to all three groups, right? Well, as it turns out, assigning participants equally across groups usually reduces statistical power. The idea behind assigning participants unequally to conditions is to maximize the variance of X for the particular kind of relationship under study -- which, according the formula I gave earlier, will increase power and precision. For example, the optimal design for a linear relationship would be 50% low, 50% high, and omit the medium condition. The optimal design for a quadratic relationship would be 25% low, 50% medium, and 25% high. The proportions vary widely depending on the design and the kind of relationship you expect, but I recommend you check out<a href="http://dx.doi.org/10.1037/1082-989X.2.1.3"> McClelland (1997)</a> to get more information on efficient experimental designs. You might be surprised.</p>
<p><strong>Recommendation 3: Make sure predictor variables are uncorrelated with each other</strong></p>
<p>A final way to increase statistical power is to <em>increase the proportion of the variance in X not shared with other variables in the model</em>. When predictor variables are correlated with each other, this is known as colinearity. For example, depression and anxiety are positively correlated with each other; including both as simultaneous predictors (say, in multiple regression) means that statistical power will be reduced, especially if one of the two variables actually doesn’t predict the outcome variable. Lots of textbooks suggest that we should only be worried about this when colinearity is extremely high (e.g., correlations around &gt; .70). However, studies have shown that even modest intercorrlations among predictor variables will reduce statistical power (<a href="http://dx.doi.org/10.2307/3172863">Mason et al., 1991</a>). Bottom line: If you can design a model where your predictor variables are relatively uncorrelated with each other, you can improve statistical power.</p>
<p><strong>Conclusion</strong></p>
<p>Increasing statistical power is one of the rare times where what is good for science, and what is good for your career actually coincides. It increases the accuracy and replicability of results, so it’s good for science. It also increases your likelihood of finding a statistically significant result (assuming the effect actually exists), making it more likely to get something published. You don’t need to torture your data with obsessive re-analysis until you get <em>p</em> &lt; .05.  Instead, put more thought into research design in order to maximize statistical power. Everyone wins, and you can use that time you used to spend sweating over p-values to do something more productive. Like volunteering with the<a href="http://openscienceframework.org/"> Open Science Collaboration</a>.</p>
<p><strong>References</strong></p>
<p>Aguinis, H., Beaty, J. C., Boik, R. J., &amp; Pierce, C. A. (2005). Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review. <em>Journal of Applied Psychology, 90,</em> 94-107. doi:10.1037/0021-9010.90.1.94</p>
<p>Button, K. S., Ioannidis, J. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. J., &amp; Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365-376. doi: 10.1038/nrn3475</p>
<p>Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. <em>The Journal of Abnormal and Social Psychology, 65,</em> 145-153. doi:10.1037/h0045186</p>
<p>Cohen, J. (1992). A power primer. <em>Psychological Bulletin, 112,</em> 155-159. doi:10.1037/0033-2909.112.1.155</p>
<p>Goodman, S. N., &amp; Berlin, J. A. (1994). The use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. <em>Annals of Internal Medicine, 121, </em>200-206. </p>
<p>Hansen, W. B., &amp; Collins, L. M. (1994). Seven ways to increase power without increasing N. In L. M. Collins &amp; L. A. Seitz (Eds.), <em>Advances in data analysis for prevention intervention research</em> (NIDA Research Monograph 142, NIH Publication No. 94-3599, pp. 184–195). Rockville, MD: National Institutes of Health.</p>
<p>MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. <em>Psychological Methods, 7,</em> 19-40. doi:10.1037/1082-989X.7.1.19</p>
<p>Mason, C. H., &amp; Perreault, W. D. (1991). Collinearity, power, and interpretation of multiple regression analysis. <em>Journal of Marketing Research, 28,</em> 268-280. doi:10.2307/3172863</p>
<p>Maxwell, S. E., Kelley, K., &amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. <em>Annual Review of Psychology, 59,</em> 537-563. doi:10.1146/annurev.psych.59.103006.093735</p>
<p>McClelland, G. H. (1997). Optimal design in psychological research. <em>Psychological Methods, 2,</em> 3-19. doi:10.1037/1082-989X.2.1.3</p>
<p>McClelland, G. H. (2000). Increasing statistical power without increasing sample size. <em>American Psychologist, 55, </em>963-964. doi:10.1037/0003-066X.55.8.963</p>
<p>Mone, M. A., Mueller, G. C., &amp; Mauland, W. (1996). The perceptions and usage of statistical power in applied psychology and management research. <em>Personnel Psychology, 49,</em> 103-120. doi:10.1111/j.1744-6570.1996.tb01793.x</p>
<p>Open Science Collaboration. (in press). The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility. In V. Stodden, F. Leisch, &amp; R. Peng (Eds.), <em>Implementing Reproducible Computational Research (A Volume in The R Series)</em>.  New York, NY: Taylor &amp; Francis. doi:10.2139/ssrn.2195999</p>
<p>Richard, F. D., Bond, C. r., &amp; Stokes-Zoota, J. J. (2003). One Hundred Years of Social Psychology Quantitatively Described. <em>Review of General Psychology, 7,</em> 331-363. doi:10.1037/1089-2680.7.4.331</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/#disqus_thread" data-disqus-identifier="2013/11/03/Increasing-statistical-power/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/">Posted at 12:00 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Oct 25,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/10/25/its-easy-being-green-open-access/" rel="bookmark" title="Permanent Link to &quot;It’s Easy Being Green (Open Access)&quot;">It’s Easy Being Green (Open Access)</a>
                    </h2>

                    by <a href="http://osc.centerforopenscience.org/author/frank-farach.html" rel="author">Frank Farach</a>

   			   <p><em>This is the first installment of the Open Science Toolkit, a recurring
feature that outlines practical steps individuals and organizations can
take to make science more open and reproducible.</em></p>
<p><img src="/images/FrankFarachHeadshotBWSmall.jpg" alt="Photo of Frank
Farach" align="left" style="padding-right: 20px;" width="100px"></p>
<p>Congratulations! Your manuscript has been peer reviewed and accepted for
publication in a journal. The journal is owned by a major publisher who
wants you to know that, for $3,000, you can make your article open
access (OA) forever. Anyone in the world with access to the Internet
will have access to your article, which may be cited more often because
of its OA status. Otherwise, the journal would be happy to make your
paper available to subscribers and others willing to pay a fee.</p>
<p>Does this sound familiar? It sure does to me. For many years, when I
heard about Open Access (OA) to scientific research, it was always about
making an article freely available in a peer-reviewed journal -- the
so-called “gold” OA option -- often at considerable expense. I liked the
idea of making my work available to the widest possible audience, but
the costs were too prohibitive.</p>
<p>As it turns out, however, the “<a href="http://legacy.earlham.edu/~peters/fos/overview.htm">best-kept
secret</a>” of OA is
that you can make your work OA for free by self-archiving it in an OA
repository, even if it has already been published in a non-OA journal.
Such “green” OA is possible because <a href="http://www.sherpa.ac.uk/romeo/statistics.php?la=en&amp;fIDnum=%7C&amp;mode=simple">many
journals</a>
have already provided blanket permission for authors to deposit their
peer-reviewed manuscript in an OA repository.</p>
<p>The flowchart below shows how easy it is to make your prior work OA. The
key is to make sure you follow the self-archiving policy of the journal
your work was published in, and to deposit the work in a suitable
repository.  </p>
<p><a href="/images/GreenOALarge.png">Click to enlarge.<img src="/images/GreenOASmall.png" alt="Flowchart showing how to
archive your research" align="left" width="600px"></a></p>
<p>Journals typically display their self-archiving and copyright policies
on their websites, but you can also search for them in the
<a href="http://www.sherpa.ac.uk/romeo/">SHERPA/RoMEO</a> database, which has a
nicely curated collection of policies from 1,333 publishers.  The
database assigns a code to journals based on how far in the publication
process their permissions extend.  It distinguishes between a pre-print,
which is the version of the manuscript before it underwent peer review,
and a post-print, the peer-reviewed version before the journal
copyedited and typeset it. Few journals allow you to self-archive their
copyedited PDF version of your article, but many let you do so for the
pre-print or post-print version. Unfortunately, some journals don’t
provide blanket permission for self-archiving, or require you to wait
for an embargo period to end before doing so. If you run into this
problem, you should contact the journal and ask for permission to
deposit the non-copyedited version of your article in an OA repository.</p>
<p>It has also become easy to find a suitable OA repository in which to
deposit your work. Your first stop should be the <a href="http://www.opendoar.org">Directory of Open
Access Repositories</a> (OpenDOAR), which
currently lists over 2,200 institutional, disciplinary, and universal
repositories. Although an article deposited in an OA repository will be
available to anyone with Internet access, repositories differ in feature
sets and policies. For example, some repositories, like
<a href="http://figshare.com/cc_license">fig<strong>share</strong></a>, automatically assign a
<a href="http://creativecommons.org/licenses/by/3.0/">CC BY</a> license to all
publicly shared papers; others, like <a href="http://opendepot.org/">Open
Depot</a>, allow you to <a href="http://creativecommons.org/choose/">choose a
license</a> before making the article
public. A good OA repository will tell you how it ensures the long-term
digital preservation of your content as well as what metadata it exposes
to search engines and web services.</p>
<p>Once you’ve deposited your article in an OA repository, consider making
others aware of its existence. Link to it on your website, mention it on
social media, or add it to your CV.</p>
<p>In honor of Open Access Week, I am issuing a “Green OA Challenge” to
readers of this blog who have published at least one peer-reviewed
article. The challenge is to self-archive one of your articles in an OA
repository and link to it in the comments below. Please also feel free
to share any comments you have about the self-archiving process or about
green OA. Happy archiving!</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/10/25/its-easy-being-green-open-access/#disqus_thread" data-disqus-identifier="2013/10/25/its-easy-being-green-open-access/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/10/25/its-easy-being-green-open-access/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Oct 16,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://osc.centerforopenscience.org/2013/10/16/thriving-au-naturel-amid-science-on-steroids/" rel="bookmark" title="Permanent Link to &quot;Thriving au naturel amid science on steroids&quot;">Thriving au naturel amid science on steroids</a>
                    </h2>


   			   <p><a href="http://briannosek.com">Brian A. Nosek</a><br />
University of Virginia, Center for Open Science</p>
<p><a href="http://jeffspies.com">Jeffrey R. Spies</a><br />
Center for Open Science</p>
<p><img src="/images/bnosek.jpg" alt="Photo of Brian Nosek" align="left" style="padding-right: 20px;" width="100px"> <img src="/images/jspies.jpg" alt="Photo of Jeffrey Spies" align="left" style="padding-right: 20px;" width="100px"> Last fall, the present first author taught a graduate class called
“Improving (Our) Science” at the University of Virginia. The class
reviewed evidence suggesting that scientific practices are not operating
ideally and are damaging the reproducibility of publishing findings. For
example, the power of an experimental design in null hypothesis
significance testing is a function of the effect size being investigated
and the size the sample to test it—power is greater when effects are
larger and samples are bigger. In the authors’ field of psychology, for
example, estimates suggest that the power of published studies to detect
an average effect size is .50 or less (Cohen, 1962; Gigerenzer &amp;
Sedlmeier, 1989). Assuming that all of the published effects are true,
approximately 50% of published studies would reveal positive results
(i.e., <em>p</em> &lt; .05 supporting the hypothesis). In reality, more than 90%
of published results are positive (Sterling, 1959; <a href="http://dx.doi.org/10.1371/journal.pone.0010068">Fanelli,
2010</a>).</p>
<p>How is it possible that the average study has power to detect the
average true effect 50% of the time or less and yet does so about 90% of
the time? It isn’t. Then how does this occur? One obvious contributor is
selective reporting. Positive effects are more likely than negative
effects to be submitted and accepted for publication (Greenwald, 1975).
The consequences include [1] the published literature is more likely to
exaggerate the size of true effects because with low powered designs
researchers must still leverage chance to obtain a large enough effect
size to produce a positive result; and [2] the proportion of false
positives – there isn’t actually an effect to detect – will be inflated
beyond the nominal alpha level of 5% (Ioannidis, 2005).</p>
<p>The class discussed this and other scientific practices that may
interfere with knowledge accumulation. Some of the relatively common
ones are described in Table 1 along with some solutions that we, and
others, identified. Problem. Solution. Easy. The class just fixed
science. Now, class members can adopt the solutions as best available
practices. Our scientific outputs will be more accurate, and significant
effects will be more reproducible. Our science will be better.</p>
<p>Alex Schiller, a class member and graduate student, demurred. He agreed
that the new practices would make science better, but disagreed that we
should do them all. A better solution, he argued, is to take small
steps: adopt one solution, wait for that to become standard scientific
practice, and then adopt another solution.</p>
<p>We know that some of our practices are deficient, we know how to improve
them, but Alex is arguing that we shouldn’t implement all the solutions?
Alex’s lapse of judgment can be forgiven—he’s just a graduate student.
However, his point isn’t a lapse. Faced with the reality of succeeding
as a scientist, Alex is right.</p>
<h2>Table 1</h2>
<p><strong>Scientific practices that increase irreproducibility of
published findings, possible solutions, and barriers that prevent
adoption of those solutions</strong></p>
<table>
<thead>
<tr>
<th>Practice</th>
<th>Problem</th>
<th>Possible Solution</th>
<th>Barrier to Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Run many low-powered studies rather than few high-powered studies</td>
<td>Inflates false positive and false negative rates</td>
<td>Run high-powered studies</td>
<td>Non-significant effects are a threat to publishability; Risky to devote extra resources to high-powered tests that might not produce significant effects</td>
</tr>
<tr>
<td>Report significant effects and dismiss non-significant effects as methodologically flawed</td>
<td>Using outcome to evaluate method is a logical error and can inflate false positive rate</td>
<td>Report all effects with rationale for why some should be ignored; let reader decide</td>
<td>Non-significant and mixed effects are a threat to publishability</td>
</tr>
<tr>
<td>Analyze during data collection, stop when significant result is obtained or continue until significant result is obtained</td>
<td>Inflates false positive rate</td>
<td>Define data stopping rule in advance</td>
<td>Non-significant effects are a threat to publishability</td>
</tr>
<tr>
<td>Include multiple conditions or outcome variables, report the subset that showed significant effects</td>
<td>Inflates false positive rate</td>
<td>Report all conditions and outcome variables</td>
<td>Non-significant and mixed effects are a threat to publishability</td>
</tr>
<tr>
<td>Try multiple analysis strategies, data exclusions, data transformations, report cleanest subset</td>
<td>Inflates false positive rate</td>
<td>Prespecify data analysis plan, or report all analysis strategies</td>
<td>Non-significant and mixed effects are a threat to publishability</td>
</tr>
<tr>
<td>Report discoveries as if they had resulted from confirmatory tests</td>
<td>Inflates false positive rate</td>
<td>Pre-specify hypotheses; Report exploratory and confirmatory analyses separately</td>
<td>Many findings are discoveries, but stories are nicer and scientists seem smarter if they had thought it in advance</td>
</tr>
<tr>
<td>Never do a direct replication</td>
<td>Inflates false positive rate</td>
<td>Conduct direct replications of important effects</td>
<td>Incentives are focused on innovation, replications are boring; Original authors might feel embarrassed if their original finding is irreproducible</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> For reviews of these practices and their effects see Ioannidis,
2005; Giner-Sorolla, 2012; Greenwald, 1975; <a href="http://dx.doi.org/10.1177/0956797611430953">John et al.,
2012</a>; <a href="http://dx.doi.org/10.1177/1745691612459058">Nosek et
al., 2012</a>; <a href="http://dx.doi.org/10.1037/0033-2909.86.3.638">Rosenthal,
1979</a>; Simmons et al., 2011; Young et al., 2008      </p>
<h2>The Context</h2>
<p>In an ideal world, scientists use the best available practices to
produce accurate, reproducible science. But, scientists don’t live in an
ideal world. Alex is creating a career for himself. To succeed, he must
publish. Papers are academic currency. They are Alex’s ticket to job
security, fame, and fortune. Well, okay, maybe just job security. But,
not everything is published, and some publications are valued more than
others. Alex can maximize his publishing success by producing particular
kinds of results. Positive effects, not negative effects (<a href="http://dx.doi.org/10.1371/journal.pone.0010068">Fanelli,
2010</a>;
Sterling, 1959). Novel effects, not verifications of prior effects (<a href="http://dx.doi.org/10.1177/1745691612462588">Open
Science Collaboration, 2012</a>). Aesthetically appealing, clean results,
not results with ambiguities or inconsistencies (Giner-Sorolla, 2012).
Just look in the pages of <em>Nature</em>, or any other leading journal, they
are filled with articles producing positive, novel, beautiful results.
They are wonderful, exciting, and groundbreaking. Who wouldn’t want
that?</p>
<p>We do want that, and science advances in leaps with groundbreaking
results. The hard reality is that few results are actually
groundbreaking. And, even for important research, the results are often
far from beautiful. There are confusing contradictions, apparent
exceptions, and things that just don’t make sense. To those in the
laboratory, this is no surprise. Being at the frontiers of knowledge is
hard. We don’t quite know what we are looking at. That’s why we are
studying it. Or, as Einstein said, “If we knew what we were doing, it
wouldn’t be called research”.
 
But, those outside the laboratory get a different impression. When the
research becomes a published article, much of the muck goes away. The
published articles are like the pictures of this commentary’s authors at
the top of this page. Those pictures are about as good as we can look.
You should see the discards. Those with insider access know, for
example, that we each own three shirts with buttons and have highly
variable shaving habits. Published articles present the best-dressed,
clean-shaven versions of the actual work.</p>
<p>Just as with people, when you replicate effects yourself to see them in
person, they may not be as beautiful as they appeared in print. The
published version often looks much better than reality. The effect is
hard to get, dependent on a multitude of unmentioned limiting
conditions, or entirely irreproducible (Begley &amp; Ellis, 2012; Prinz et
al., 2011).</p>
<h2>The Problem</h2>
<p>It is not surprising that effects are presented in their best light.
Career advancement depends on publishing success. More beautiful looking
results are easier to publish and more likely to earn rewards
(Giner-Sorolla, 2012). Individual incentives align for maximizing
publishability, even at the expense of accuracy (<a href="http://dx.doi.org/10.1177/1745691612459058">Nosek et al.,
2012</a>).</p>
<p>Consider three hypothetical papers shown in Table 2. For all three, the
researchers identified an important problem and had an idea for a novel
solution. <em>Paper A</em> is a natural beauty. Two well planned studies showed
effects supporting the idea. <em>Paper B</em> and <em>Paper C</em> were conducted with
identical study designs. <em>Paper B</em> is natural, but not beautiful; <em>Paper
C</em> is a manufactured beauty. Both <em>Paper B</em> and <em>Paper C</em> were based on 3
studies. One study for each showed clear support for the idea. A second
study was a mixed success for <em>Paper B</em>, but “worked” for <em>Paper C</em> after
increasing the sample size a bit and analyzing the data a few different
ways. A third study did not work for either. <em>Paper B</em> reported the
failure with an explanation for why the methodology might be to blame,
rather than the idea being incorrect. The authors of <em>Paper C</em> generated
the same methodological explanation, categorized the study as a pilot,
and did not report it at all. Also, <em>Paper C</em> described the final sample
sizes and analysis strategies, but did not mention that extra data was
collected after initial analysis, or that alternative analysis
strategies had been tried and dismissed.</p>
<h2>Table 2</h2>
<p><strong>Summary of research practices for three hypothetical papers</strong></p>
<table>
<thead>
<tr>
<th>Step</th>
<th>Paper</th>
<th>Paper B</th>
<th>Paper C</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data collection</td>
<td>Conducted two studies</td>
<td>Conducted three studies</td>
<td>Conducted three studies</td>
</tr>
<tr>
<td>Data analysis</td>
<td>Analyzed data after completing data collection following a pre-specified analysis plan</td>
<td>Analyzed data after completing data collection following a pre-specified analysis plan</td>
<td>Analyzed during data collection and collected more data to get to significance in one case.  Selected from multiple analysis strategies for all studies.</td>
</tr>
<tr>
<td>Result reporting</td>
<td>Reported the results of the planned analyses for both studies</td>
<td>Reported the results of the planned analyses for all studies</td>
<td>Reported results of final analyses only.  Did not report one study that did not reach significance.</td>
</tr>
<tr>
<td>Final paper</td>
<td>Two studies demonstrating clear support for idea</td>
<td>One study demonstrating clear support for idea, one mixed, one not at all</td>
<td>Two studies demonstrating clear support for idea</td>
</tr>
</tbody>
</table>
<p><em>Paper A</em> is clearly better than <em>Paper B</em>. <em>Paper A</em> should be
published in a more prestigious outlet and generate more attention and
accolade. <em>Paper C</em> <strong>looks</strong> like <em>Paper A</em>, but in reality it <strong>is</strong>
like <em>Paper B</em>. The actual evidence is more circumspect than the
apparent evidence. Based on the report, however, no one can tell the
difference between <em>Paper A</em> and <em>Paper C</em>.</p>
<p>Two possibilities would minimize the negative impact of publishing
manufactured beauties like <em>Paper C</em>. First, if replication were
standard practice, then manufactured effects would be identified
rapidly. However, direct replication is very uncommon (<a href="http://dx.doi.org/10.1177/1745691612462588">Open Science
Collaboration, 2012</a>). Once an effect is the literature, there is little
systematic ethic to self-correct. Rather than be weeded out, false
effects persist or just slowly fade away. Second, scientists could just
avoid doing the practices that lead to <em>Paper C</em> making this
illustration an irrelevant hypothetical. Unfortunately, a growing body
of evidence suggests that these practices occur, and some are even
common (e.g., <a href="http://dx.doi.org/10.1177/0956797611430953">John et al., 2012</a>).</p>
<p>To avoid the practices that produce <em>Paper C</em>, the scientist must be
aware of and confront a conflict-of-interest—what is best for science
versus what is best for me. Scientists have inordinate opportunity to
pursue flexible decision-making in design and analysis, and there is
minimal accountability for those practices. Further, humans’ prodigious
motivated reasoning capacities provide a way to decide that the outcomes
that look best for us also has the most compelling rationale (<a href="http://dx.doi.org/10.1037/0033-2909.108.3.480">Kunda,
1990</a>). So, we may convince ourselves that the best course of action for
us was the best course of action period. It is very difficult to stop
doing suspect practices when we have thoroughly convinced ourselves that
we are not doing them.</p>
<h2>The Solution</h2>
<p>Alex needs to publish to succeed. The practices in Table 1 are to the
scientist, what steroids are to the athlete. They amplify the likelihood
of success in a competitive marketplace. If others are using, and Alex
decides to rely on his natural performance, then he will disadvantage
his career prospects. Alex wants to do the best science he can <em>and</em> be
successful for doing it. In short, he is the same as every other
scientist we know, ourselves included. Alex shouldn’t have to make a
choice between doing the best science and being successful—these should
be the same thing.</p>
<p>Is Alex stuck? Must he wait for institutional regulation, audits, and
the science police to fix the system? In a regulatory world, practices
are enforced and he need not worry that he’s committing career suicide
by following them. Many scientists are wary of a strong regulatory
environment in science, particularly for the possibility of stifling
innovation. Some of the best ideas start with barely any evidence at
all, and restrictive regulations on confidence in outputs could
discourage taking risks on new ideas. Nonetheless, funders, governments,
and other stakeholders are taking notice of the problematic incentive
structures in science. If we don’t solve the problem ourselves,
regulators may solve them for us.</p>
<p>Luckily, Alex has an alternative. The practices in Table 1 may be
widespread, but the solutions are also well known and endorsed as good
practice (<a href="http://dx.doi.org/10.1177/1745691612459521">Fuchs et al., 2012</a>). That is, scientists easily understand the
differences between Papers A, B, and C – if they have full access to how
the findings were produced. As a consequence, the only way to be
rewarded for natural achievements over manufactured ones is to make the
process of obtaining the results transparent. Using the best available
practices privately will improve science but hurt careers. Using the
best available practices publicly will improve science while
simultaneously improving the reputation of the scientist. With openness,
success can be influenced by the results <em>and</em> by how they were
obtained.
 </p>
<h2>Conclusion</h2>
<p>The present incentives for publishing are focused on the one thing that
we scientists are absolutely, positively not supposed to control - the
results of the investigation. Scientists have complete control over the
design, procedures, and execution of the study. The results are what
they are.</p>
<p>A better science will emerge when the incentives for achievement align
with the things that scientists can (and should) control with their
wits, effort, and creativity. With results, beauty is contingent on what
is known about their origin. Obfuscation of methodology can make ugly
results appear beautiful. With methodology, if it looks beautiful, it is
beautiful. The beauty of methodology is <em>revealed</em> by openness.</p>
<p>Most scientific results have warts. Evidence is halting, uncertain,
incomplete, confusing, and messy. It is that way because scientists are
working on hard problems. Exposing it will accelerate finding solutions
to clean it up. Instead of trying to make results look beautiful when
they are not, the inner beauty of science can be made apparent. Whatever
the results, the inner beauty—strong design, brilliant reasoning,
careful analysis—is what counts. With openness, we won’t stop aiming for
A papers. But, when we get them, it will be clear that we earned them.</p>
<h2>References</h2>
<p>Begley, C. G., &amp; Ellis, L. M. (2012). Raise standards for preclinical
cancer research. <em>Nature, 483</em>, 531-533.</p>
<p>Cohen , J. (1962). The statistical power of abnormal-social
psychological research: A review. <em>Journal of Abnormal and Social
Psychology, 65</em>, 145-153.</p>
<p>Fanelli, D. (2010). "Positive" results increase down the hierarchy of
the sciences. <em>PLoS ONE, 5(4)</em>, e10068.
<a href="http://dx.doi.org/10.1371/journal.pone.0010068">doi:10.1371/journal.pone.0010068</a></p>
<p>Fuchs H., Jenny, M., &amp; Fiedler, S. (2012). Psychologists are open to
change, yet wary of rules. <em>Perspectives on Psychological Science, 7,</em>
634-637. <a href="http://dx.doi.org/10.1177/1745691612459521">doi:10.1177/1745691612459521</a></p>
<p>Sedlmeier, P., &amp; Gigerenzer, G. (1989). Do studies of statistical power
have an effect on the power of studies? <em>Psychological Bulletin, 105</em>,
309-316.</p>
<p>Giner-Sorolla, R. (2012). Science or art? How esthetic standards grease
the way through the publication bottleneck but undermine science.
<em>Perspectives on Psychological Science</em>.</p>
<p>Greenwald, A. G. (1975). Consequences of prejudice against the null
hypothesis. <em>Psychological Bulletin, 82</em>, 1-20.</p>
<p>Ioannidis, J. P. A. (2005). Why most published research findings are
false. <em>PLoS Medicine, 2</em>, e124.</p>
<p>John, L., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence
of questionable research practices with incentives for truth-telling<em>.
Psychological Science</em>, <em>23</em>, 524-532.
<a href="http://dx.doi.org/10.1177/0956797611430953">doi:10.1177/0956797611430953</a></p>
<p>Kunda, Z. (1990). The case for motivated reasoning. <em>Psychological
Bulletin, 108,</em> 480-498. <a href="http://dx.doi.org/10.1037/0033-2909.108.3.480">doi:10.1037/0033-2909.108.3.480</a></p>
<p>Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific utopia: II.
Restructuring incentives and practices to promote truth over
publishability. <em>Perspectives on Psychological Science, 7,</em>615-631.
<a href="http://dx.doi.org/10.1177/1745691612459058">doi:10.1177/1745691612459058</a></p>
<p>Open Science Collaboration. (2012). An open, large-scale, collaborative
effort to estimate the reproducibility of psychological science.
<em>Perspectives on Psychological Science, 7,</em> 657-660.
 <a href="http://dx.doi.org/10.1177/1745691612462588">doi:10.1177/1745691612462588</a></p>
<p>Prinz, F., Schlange, T. &amp; Asadullah, K. (2011). Believe it or not: how
much can we rely on published data on potential drug targets? <em>Nature
Reviews Drug Discovery, 10</em>, 712-713.</p>
<p>Rosenthal, R. (1979). The file drawer problem and tolerance for null
results. <em>Psychological Bulletin, 86,</em> 638-641.
<a href="http://dx.doi.org/10.1037/0033-2909.86.3.638">doi:10.1037/0033-2909.86.3.638</a></p>
<p>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as significant. <em>Psychological Science, 22</em>,
1359-1366.</p>
<p>Sterling, T. D. (1959). Publication decisions and their possible effects
on inferences drawn from tests of significance - or vice versa. <em>Journal
of the American Statistical Association, 54</em>, 30-34.</p>
<p>Young, N. S., Ioannidis, J. P. A., &amp; Al-Ubaydli, O. (2008). Why current
publication practices may distort science. <em>PLoS Medicine, 5</em>,
1418-1422.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://osc.centerforopenscience.org/2013/10/16/thriving-au-naturel-amid-science-on-steroids/#disqus_thread" data-disqus-identifier="2013/10/16/thriving-au-naturel-amid-science-on-steroids/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://osc.centerforopenscience.org/2013/10/16/thriving-au-naturel-amid-science-on-steroids/">Posted at  9:54 am</a>&nbsp;&middot;&nbsp;<a href="http://osc.centerforopenscience.org/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://osc.centerforopenscience.org/category/content3.html" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="http://osc.centerforopenscience.org/category/content5.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 4 of 5</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://osc.centerforopenscience.org/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>