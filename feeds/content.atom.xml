<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Open Science Collaboration Blog</title><link href="http://osc.centerforopenscience.org/" rel="alternate"></link><link href="http://osc.centerforopenscience.org/feeds/content.atom.xml" rel="self"></link><id>http://osc.centerforopenscience.org/</id><updated>2015-03-06T01:00:00-05:00</updated><entry><title>On “Opening” Undergraduate Learning: A Student’s Journey Through Open Science</title><link href="http://osc.centerforopenscience.org/2015/03/06/opening-undergraduate-learning/" rel="alternate"></link><updated>2015-03-06T01:00:00-05:00</updated><author><name>Andrew A. Nelson</name></author><id>tag:osc.centerforopenscience.org,2015-03-06:2015/03/06/opening-undergraduate-learning/</id><summary type="html">&lt;p&gt;In following the recent cascade of online commentary and published literature surrounding the open science debate,  I’ve noticed various attempts to situate the undergraduate role within this movement. One position, held by many, is best communicated in the following passages:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“Replications of new research should be performed by students as part of their coursework in experimental methods.” (Frank &amp; Saxe, 2004)&lt;/p&gt;

&lt;p&gt;“...if students are encouraged to conduct replications as part of an effort to document and archive replications, collected classroom projects could contribute significantly to the field.” (Grahe et al., 2012)&lt;/p&gt;

&lt;p&gt;“...many academics already have set research agendas and may be unable to engage in the much-needed replication studies. In contrast, undergraduate students lack these prior commitments, creating the perfect opportunity for them to contribute to the scientific field while completing classroom requirements...When undergraduates get involved with research, everyone benefits...” (Grahe, Guillaume, &amp; Rudmann, 2013)&lt;/p&gt;

&lt;p&gt;“I am very excited about the recent opportunities that allow students to contribute to “big science” by acting as crowd-sourcing experimenters.” (“Opportunities for Collaborative Research”; Grahe, 2013)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clearly, open science proponents believe in the significance and necessity of undergraduate contributions to the movement. Even when reading between the lines, I take no offense to the implications made above; undergraduates *do* remain a valuable, blunt-force utility appropriate for open science’s “dirty work”, particularly when noting our lack of constraints (e.g. preexisting research agendas, reliance on funding, publication quotas, anxiety about tenure) relative to academics. And, though not included above, all of the aforementioned pieces make comparably frequent mentions of the reciprocal benefits to undergraduates when they participate in open science, suggesting that undergraduate involvement--even at the ground-level of replication and crowd-sourcing projects--really is advantageous to all.&lt;/p&gt;

&lt;p&gt;Even so, discussions of these benefits are painfully vague and regularly short-ended; they are also, ironically, made by academics rather than undergraduates themselves. Acknowledging these shortcomings, below I briefly recount my own immersion in open science initiatives to firm up the discussion of undergraduate participation within the movement. I seek to frame these numerous open science avenues, both those directed at undergraduates and those open to all, as a critical educational tool, one fit for helping students meet the five “Learning Goals” outlined in the *APA Guidelines for the Undergraduate Psychology Major* (American Psychological Association, 2013). And while ultimately I vouch for an empirical pretest-posttest evaluation of my opinion--namely, that undergraduate participation in open science-based research projects prepares students to meet these learning goals above and beyond class-bracketed research--my own experiences will have to suffice for now.&lt;/p&gt;

&lt;h2&gt;Goal 1: Knowledge Base in Psychology&lt;/h2&gt;

&lt;p&gt;In their first learning goal, the APA calls for students to grasp the historical, theoretical and conceptual underpinnings of psychology and its comprising subfields. They understandably assume that this knowledge will largely come from course-taking, and I certainly don’t dispute this assumption, especially when considering the distinct objectives for foundation level students (those who have completed no more than four course in the major) who may have no other extracurriculars (for example, Psi Chi or Psychology Club) from which to refine their knowledge base.&lt;/p&gt;

&lt;p&gt;Yet it’s important to remember that undergraduate coursework is generally constrained in its breadth and accessibility. More specifically, both the number of classes and the types of classes at an undergraduate’s disposal can be gravely limited. I recall not having space in my schedule to take a well-regarded cognitive psychology class, and consequently, feeling poorly versed in the domain’s theoretical framework and popular studies. Or, relatedly, hearing murmurings of human factors psychology and its inspiring applied nature, yet not having access to an elective dedicated to introducing that field.&lt;/p&gt;

&lt;p&gt;In response to these limitations, I contend that The Center of Open Science’s (COS) [*Archival Project*](http://archivalproject.org/) is a remarkable educational opportunity for circumventing these limitations. This project recruits qualified undergraduates to “objectively assess the research and replication practices” of studies published in three of psychology’s high-impact academic journals. Alongside its primary goal of deciphering the actual rate of replication within the discipline, *The Archival Project* represents a unique and meaningful chance for students to critically engage with literature spanning the array of psychological subfields.&lt;/p&gt;

&lt;p&gt;I became involved with *The Archival Project* as a member of my institution’s Psi Chi chapter, which was asked to pilot initial versions of the project’s standardized article coding scheme. I concede without bitterness that this free, crowd-sourcing initiative forced me to thoroughly digest more psychological literature than the courses I was paying thousands of dollars to attend; doing so while contributing to groundbreaking science was an added bonus and motivator. Additionally, because the journals in question are broad in scope, I dabbled in biopsychology and neuropsychology readings that I may never have encountered otherwise. With confidence I can thus assert that my discipline-specific knowledge base was significantly bolstered by a project that is virtually open to all.&lt;/p&gt;

&lt;h2&gt;Goal 2: Scientific Inquiry and Critical Thinking&lt;/h2&gt;

&lt;p&gt;The second learning outcome specified by the APA involves the “the development of scientific reasoning and problem solving, including effective research methods.” I foresee the assumption here being that class-mandated, independent projects are a suitable manner of meeting this learning goal, particularly when noting the substantial number of psychology majors required to participate in undergraduate research (Grahe et al., 2012). Again, I don’t disagree with this kind of thinking, yet coupling this requirement with open science platforms makes only more sense.&lt;/p&gt;

&lt;p&gt;Arguably, the typical undergraduate research experience, especially as a tool for instruction, contains several notable shortcomings. Even in its planning stage, student-driven research is limited by the depth of student’s conceptual knowledge and the resources at her or his disposal. Furthermore, both planning and execution can be inhibited by the length of a quarter or semester, where simply getting IRB approval eats away at weeks of valuable data collection time. And thirdly, often a more definite endpoint exists--a final grade--and beyond that, many students’ data seemingly don’t matter much.&lt;/p&gt;

&lt;p&gt;These potential drawbacks are precisely why I remain grateful for my own participation in a collective research paradigm, an experience so critical to my psychology-specific growth that I may have not pursued the discipline otherwise. Dr. Jon Grahe’s *Collective Undergraduate Research Project* (a precursor to the current [*Collaborative Replications and Education Project*](https://osf.io/wfc6u/)) connected me with the project proposal of Dr. Fabian Ramseyer, a Swiss researcher seeking additional samples to test his objective gauge for dyadic movement coordination. (A brief overview of our work is viewable [here.](https://osf.io/uki2r/files/)) I smile thinking back to the beginning days of our project, when--as a sophomore attempting to better understand Dr. Ramseyer’s protocol and its underlying theory--I kept pronouncing “rapport” with a hard, annunciated “t” (thus “report”). And in a way, that’s exactly the point. I knew absolutely nothing about the rapport construct and its behavioral correlates before this investigation, and class material never once spotlighted the topic. Just as I came to understand the ins and outs of dyadic rapport development via this open science opportunity, calls for undergraduate replication expose students to research concepts that are otherwise inconceivable, thereby broadening students’ psychological knowledge. My collaboration with Dr. Ramseyer also allowed access to methodological tools (e.g. *Motion Energy Analysis*, the “objective gauge” mentioned above) literally unavailable elsewhere. When acknowledging the limited resources of a small, liberal arts university like my own, such accessibility can be especially empowering.&lt;/p&gt;

&lt;p&gt;Also noteworthy is how the potential for “authentic research” (Grahe, [“Opportunities for Collaborative Research”](http://osc.centerforopenscience.org/2013/10/10/opportunities-for-collaborative-research/) can inspire learning far beyond that of research conducted for coursework alone. Knowing that one’s data matters--whether it will be collapsed into a larger, crowd-sourced dataset, used to validate previous findings, or employed to evaluate the reliability of a novel methodology--encourages meticulousness. And to be meticulous requires the cultivation of problem-solving and critical-thinking skills central to the APA’s second learning goal. A humorous example of this is my own consideration of how swivel chairs (versus stationary desk chairs) might confound the type and frequency of movement synchronization between dyad members. When bringing my question to Dr. Ramseyer’s attention, he exclaimed: “Your questions are proof that you have reached a good understanding of what is going on with MEA and video-analysis – congratulations!”&lt;/p&gt;

&lt;h2&gt;Goal 3: Ethical and Social Responsibility in a Diverse World&lt;/h2&gt;

&lt;p&gt;The APA’s third undergraduate learning goal emphasizes student’s awareness of their social and ethical responsibilities. As it applies to students completing their baccalaureate degree, this objective further encourages students to manifest these responsibilities in ways that “optimize” or “strengthen” their professional contributions. I commend the APA’s inclusion of this criterion, specifically when noting the recent string of ethical breaches in psychology and beyond. The transparency, impartiality, and general foundation of open science makes it a special, yet underutilized option to hone undergraduates’ understanding of scientific ethics.&lt;/p&gt;

&lt;p&gt;That said, class-bracketed research might then be counterproductive. While students conduct research, they are concurrently refining their professional values and understanding of ethical science; doing so in a format that limits data-sharing and possible dissemination modes seems to then be sending the wrong message. Could it even be teaching bad science? Without doubt, it *is* minimizing students’ potential contributions to the field.&lt;/p&gt;

&lt;p&gt;I can speak from experience when saying that students quickly latch onto the drama of running their painstakingly-collected data through SPSS, crossing their fingers, and praying for results that support their hypotheses. Far too many, in consequence, are greeted with a lack of significance or something discouraging...and that momentary passion is indefinitely quelled.  My head hung pretty low the countless times Dr. Ramseyer and I’s data yielded nothing worthy of much excitement. With persistent encouragement from my advisor, however, I’ve shared our robust, vastly underexplored data on the [*Open Science Framework*](https://osf.io/dyntp/) and via a data paper published in [*The Journal for Open Psychology Data*](http://openpsychologydata.metajnl.com/article/view/jopd.ae/13).&lt;/p&gt;

&lt;p&gt;Without hyperbole, I believe that the process of contributing to these open science platforms has forever shaped my values as a psychologist and my ethical commitment as a researcher. Firstly, I’ve come to frame my efforts in publicizing these data as, indeed, maximizing my contribution (given my resources and my data) to the field, a contribution that would be nearly impossible without the above-mentioned open science opportunities. Secondly, in sharing these data, I’ve had to also disseminate contextual and procedural information necessary for replication, potential collaboration, or data-checking. I now understand scientific transparency (alluded to in section 8.14 of the [APA’s Ethics Code](http://apa.org/ethics/code/index.aspx)) and encouraged replication to be paramount to good science. And most generally, along the way, I’ve accumulated knowledge about the current replication crisis, data falsification scandals, and other trends that verify the need for an open science network. As an example, choosing to post these data comes from my awareness of (and push back against) the “file drawer problem” (Rosenthal, 1979) plaguing modern science. I can almost guarantee that these current happenings in psychological science--and their interconnection with social and ethical responsibilities--are no more than alluded to in undergraduate psychology classes. This is why teaching open science is so crucial to plugging this educational hole and satisfying the APA’s third undergraduate learning objective.&lt;/p&gt;

&lt;h2&gt;Goal 4: Communication&lt;/h2&gt;

&lt;p&gt;With their fourth learning goal, the APA insists that psychology majors should be able to effectively communicate through writing, presentations, and interpersonal discussion. My undergraduate experience dedicated a significant amount of time to sharpening my written communication and oral presentation skills, most often through in-class exercises or feedback on assignments. Yet, in learning other types of information exchange, students were left to fend for themselves. The point I hope to make below is that open science requires contributors to communicate via nontraditional mediums that are significant but seldom covered in class. Because these mediums ensure that students can maximize their scientific potential (through collaboration, data-sharing, and the like), they should be embraced and taught across all levels of higher education.&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Primarily, I argue that open science is crucial to cushioning the otherwise intimidating prospect of undergraduate-initiated dialogue with graduate students or faculty. An indisputable point is that most undergraduates don’t even know how to initiate these conversations, let alone how to sustain them without the fear of asking stupid questions or appearing ignorant. The neat thing about many open science opportunities is that they give students a figurative seat at the academic table, where the student-professor relationship can evolve into one that feels more collegial.&lt;/p&gt;

&lt;p&gt;In my own journey, I distinctly recall finding my voice in the midst of my partnership with Dr. Ramseyer. Long-distance collaborations, particularly when using someone else’s detailed protocol and associated software package, requires a heap of emails, all of which needed to be professional, clear, and concise. This partnership forced me to become a proficient communicator over email (something that many students could improve on), and additionally, boosted my confidence when talking with academic superiors. Relatedly, Dr. Grahe provided onsite supervision of this project, and our weekly meetings provided ample time to feel better about expressing my own opinion to--and even debating with--faculty. A fitting test for these developing skills occurred during a study abroad experience, when I took a side trip and visited Dr. Ramseyer in Bern. Amazingly, it felt as if we were able to pick up right where we left off over email, discussing potential updates for movement coordination software and his newfound interest in vocal pitch synchronization. Certainly I was nervous for that meeting in Switzerland, and I still get anxious before chats with faculty, however I attribute these communication improvements to my participation in open science projects.&lt;/p&gt;

&lt;p&gt;And briefly, while the writing necessary to develop a project page with the *Open Science Framework* or draft a data paper with *The Journal of Open Psychology Data* remains unfamiliar to most undergraduates, it also represents a terrific opportunity for student writing to move beyond the generic “APA style manuscript” guidelines often imposed on it. Sure, I found these modes challenging to adapt to at first, yet I was reminded that proficient communication requires exactly that: adaptation to its setting. As we watch the tides of science change, it is important to expose students to mediums that are gaining traction and popularity within the discipline. Only then will students overwhelming meet the criteria included in this fourth goal.&lt;/p&gt;

&lt;h2&gt;Goal 5: Professional Development&lt;/h2&gt;

&lt;p&gt;In culmination, the APA’s fifth learning objective asks programs to best equip their students with skills increasing their competitiveness for post-college employment or graduate school. Specifications of these abilities include the maturation of “self-efficacy and self-regulation,” “project-management skills,” enhancing one’s “teamwork capacity,” and the application of “psychology content and skills to career goals.” While I understand the value in these skills for postbaccalaureate success, I also must note that undergraduate coursework (including research) usually serves as the end-goal through which these skills are taught. Few instructors devote intentional lesson plans to improving students’ ability to work in a team or develop self-efficacy; instead, these abilities are learned along the way.&lt;/p&gt;

&lt;p&gt;Consequently, the content of this fifth goal seems almost circular, especially to those aiming for graduate school or psychology-related employment. Consider the previous four objectives and my arguments for how open science opportunities might benefit learning above and beyond course-bracketed research projects. As students learn how to learn by building a psychology-specific knowledge base, boost their critical thinking skills through intensive scientific inquiry, augment their learning with the awareness of social and science-specific ethics, and polish their communication skills, they are developing abilities paramount to their professional development. And if their participation in open science only betters their development of such skills, then it seems almost imperative that students be exposed to these projects in a setting that, above all else, exists to ready students for their future careers.&lt;/p&gt;

&lt;p&gt;I’d like to conclude by acknowledging the disagreements swirling around open science implementation. Debate is inherent to changing the status quo, and ultimately, I don’t want to blame either side for their passions or viewpoints. Nonetheless, there’s an underlying point here that seems far less disputable. Improving the quality of undergraduate education should be a priority to all, particularly to those same academics engaged in this debate. Can’t we all agree on the educational capacity of a movement (call it “open science” if you wish) that stimulates undergraduate learning, in ways previously overlooked or inconceivable? The movement is ready. Now it’s simply a matter of embracing it.&lt;/p&gt;

&lt;h2&gt;Works Cited&lt;/h2&gt;

&lt;p&gt;American Psychological Association. (2013). APA Guidelines for the Undergraduate Psychology Major. Retrieved from [http://www.apa.org/ed/precollege/about/psymajor-guidelines.pdf](http://www.apa.org/ed/precollege/about/psymajor-guidelines.pdf.).&lt;/p&gt;

&lt;p&gt;Frank, M. C., &amp; Saxe, R. (2012). Teaching replication. *Perspectives on Psychological Science*, *7*(6), 600-604. doi:10.1177/1745691612460686&lt;/p&gt;

&lt;p&gt;Grahe. J. E., Gullaume-Hanes, E., &amp; Rudmann, J. (2013). Students collaborate to advance science: The International Situations Project. *Council for Undergraduate Research Quarterly*, *34*(2), 4-9. [http://www.cur.org/publications/curq_on_the_web/](http://www.cur.org/publications/curq_on_the_web/)&lt;/p&gt;

&lt;p&gt;Grahe, J. E., Reifman, A., Hermann, A. D., Walker, M., Oleson, K. C., Nario-Redmond, M., &amp; Wiebe, R. P. (2012). Harnessing the undiscovered resource of student research projects. *Perspectives on Psychological Science*, *7*(6), 605-607. doi:10.1177/1745691612459057&lt;/p&gt;

&lt;p&gt;Rosenthal, R. (1979). The file drawer problem and tolerance for null results. *Psychological Bulletin*, *86*(3), 638-641.&lt;/p&gt;</summary></entry><entry><title>It’s All Happening - The Future of Crowdsourcing Science</title><link href="http://osc.centerforopenscience.org/2015/01/22/crowdsourcing-science/" rel="alternate"></link><updated>2015-01-22T11:50:00-05:00</updated><author><name>Erica Baranski</name></author><id>tag:osc.centerforopenscience.org,2015-01-22:2015/01/22/crowdsourcing-science/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Excitement in the air&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My generation – the 20-something, &lt;a href="http://en.wikipedia.org/wiki/Millennials"&gt;millennial generation&lt;/a&gt; — has led technological advancement that has had an unprecedented impact on business, medicine, politics, and even social interactions. Technology is in a constant state of advancement and if you’re plugged in, you can easily get swept away. There is an overwhelming feeling that we are living in an age where ‘it’s all happening’, and in ways it has never happened before. &lt;/p&gt;
&lt;p&gt;I’ve been thinking about this state of mind in the context of the current “reproducibility crisis” (&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.ejwagenmakers.com%2F2012%2FPashlerWagenmakers2012.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNHZejJuIKsQcAYIEk6Z8h7DXDc4zA"&gt;Pashler &amp;amp; Wagenmakers, 2012&lt;/a&gt;).  I recently asked a few (more established) social psychologists if this feeling is based on actual innovation and change or if it’s just a product being young in the field. They emphatically agreed that now is different than any other time in psychology and things really are changing; the air of excitement is real. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Crowdsourcing science&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the midst of the reproducibility crisis, crowdsourcing science has emerged as a means to produce transparent science. With each large-scale, crowdsourcing project, most notably, the &lt;a href="https://osf.io/ezcuj/wiki/researcher%20guide/compare/1/"&gt;Reproducibility Project - Psychology&lt;/a&gt; and &lt;a href="https://osf.io/e81xl/"&gt;Cancer Biology&lt;/a&gt;, &lt;a href="http://osf.io/xiz7c"&gt;Many Labs 2&lt;/a&gt; and &lt;a href="https://www.google.com/url?q=https%3A%2F%2Fosf.io%2Fswiz8%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFR6vnw8G5hsmAW_PhNNCT3pRxCmQ"&gt;3&lt;/a&gt;, and &lt;a href="http://osf.io/wfc6u"&gt;CREP&lt;/a&gt;, the benefits of such methodology are made clear. Sharing the scientific process with other researchers from different institutions enables those outside the primary lab to reap the rewards of the project. The many contributors of these projects have, in their own right, some sense of ownership of the project. For the Reproducibility Project – Psychology (RP-P), collaborators not only are helping the lead researchers answer meta-scientific questions, but are also involved in the excitement that the project has created in the field. &lt;/p&gt;
&lt;p&gt;As a contributor to the RP-P, I feel directly involved in this huge open-science movement. This involvement is one of the reasons the reproducibility conversation has had so much momentum over the last couple of years. Crowdsourcing science in conjunction with projects explicitly devoted to improve the integrity and transparency of science will keep the spark alive.&lt;/p&gt;
&lt;p&gt;The lab I work in at the University of California – Riverside is utilizing crowdsourcing to spread the benefits of collecting rich and big data. The &lt;a href="http://rap.ucr.edu/ISP.html"&gt;International Situations Project and International Personality Project&lt;/a&gt; are cross-cultural studies in which collaborators from 19 and 13 countries, respectively, have managed the data collection process from a combined 7,454 participants worldwide. We’ve also partnered with &lt;a href="http://psichi.org"&gt;Psi Chi&lt;/a&gt; and &lt;a href="http://psibeta.org/site/"&gt;Psi Beta&lt;/a&gt; to crowdsource undergraduates and collect data from 13 colleges and universities across the US (&lt;a href="http://www.cur.org/assets/1/23/Winter2013_v34.2_Grahe.Guilaume.Rudmann.pdf"&gt;Grahe, Guillaume, &amp;amp; Rudman, 2013&lt;/a&gt;). Generally, we’re interested in how people experience a situation and how behavior and personality informs, and is informed by, these experiences. To do this, we had our collaborators ask their participants to log on to the study’s website, answer the question “What were you doing at 7:00 pm the previous night?” and then evaluate the situation, their behavior and their personality using Q-sort methodology. The comparison of a large number of cultures along common personality variables is sure to yield important scientific "news" regardless of what is found. With these projects we are aiming to quantify the differences in personality and situations among cultures. Though this research is exploratory in nature, we expect to gain insight into the degree to which behavior, situations, and personality vary both between and within cultures. &lt;/p&gt;
&lt;p&gt;In line with the crowdsource methodology’s mission, our many (33+) collaborators have the opportunity to analyze the wealth of data they helped collect to answer important psychological questions. Indeed, a few have already made waves in the field in doing so (&lt;a href="http://www.bigeightdiamonds.com/app/download/5804847892/DIAMONDS+-+JPSP+-+Rauthmann+et+al.+(2014).pdf"&gt;Rauthmann et al., 2014&lt;/a&gt;). Each ISP/IPP contributor feels (like I do as an RP-P contributor) well-deserved ownership for the project, and shares in the feeling of excitement associated with the project’s  analytical and theoretical possibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The future of crowdsourced science&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recently, I have had the unique opportunity to use open-source software to expand these projects.  Over this past summer I worked as a developer intern at the Center for Open Science (COS) in Charlottesville, VA. COS is a non-profit whose mission is to improve the integrity, openness and reproducibility across all scientific disciplines. To support this mission, COS has developed an online tool called the &lt;a href="https://osf.io/"&gt;Open Science Framework&lt;/a&gt; (OSF) to enable scientists to maintain productive collaborations, publicly display their methods and data, and encourage the growth of open-science conversation. My experience led me to two major intellectual accomplishments: understanding how to build online tools that make crowdsourcing projects possible, and instilling a new passion in me that will inform all of my future projects. I have begun to appreciate how tools like the OSF and methods like crowd-sourcing can aid in pushing the reproducibility movement forward. &lt;/p&gt;
&lt;p&gt;Despite the use of crowdsourcing methods in both projects, the Reproducibility Project and the International Situation (and Personality) Project have distinct scientific goals and it is this distinction that illuminates the scope of scientific endeavors crowdsourcing and technology like the Open Science Framework can facilitate. For example, RP-P utilizes various labs’ resources around the world in order to generalize answers to meta-scientific questions. ISP, on the other hand, recruits its US and international researchers to establish strong scientific connections and lend unique theoretical insights on the massive amount of data collected. Both projects use crowd-sourced data. Both projects intend to strengthen the generalizability power with use of these data. Both projects use the Open-Science Framework to display the methods, materials, and data to encourage the openness of the projects. Yet their research goals exist on two different, yet equally important scientific planes. &lt;/p&gt;
&lt;p&gt;The future of crowdsourced science is a bright one. While metascientific projects like RP-P and Many Labs 2 and 3 have illuminated the many benefits crowdsourcing has on science, the projects going on in my lab use this method to involve many scientists in theoretically novel projects. And we do so cross-culturally. The International Situations and Personality Projects offer a unique opportunity to the open-science community to reap the rewards of a project with so much potential for further questions. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It’s all happening&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although I appreciate that some of the excitement around psychological research is a product of being young and new in the field, there is an undeniable sense that things are changing in meaningful ways.  Conducting research that uses open-source technology and crowdsourcing methodology will ensure that I am directly involved.  It’s important to harness this excitement and continue to establish innovative ways to utilize these crowdsourcing methods. To involve many in the scientific process – no matter what the end goal is – and utilize technology that is constantly pushing forward and carrying scientific integrity and value along with it. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pashler, H., &amp;amp; Wagenmakers, E. J. (2012). Editors’ Introduction to the Special Section on Replicability in Psychological Science A Crisis of Confidence?.Perspectives on Psychological Science, 7(6), 528-530.&lt;/p&gt;
&lt;p&gt;Grahe, J. E., Guillaume, E., &amp;amp; Rudmanm, J (2013). Students collaborate to advance science: The International Situations Project. CUR Quarterly-Online, 34 (2), 4-9.&lt;/p&gt;
&lt;p&gt;Rauthmann, J. F., Gallardo-Pujol, D., Guillaume, E. M., Todd, E., Nave, C. S., Sherman, R. A., &amp;amp; Funder, D. C. (2014). The Situational Eight DIAMONDS: A taxonomy of major dimensions of situation characteristics.&lt;/p&gt;</summary></entry><entry><title>Why I Dressed up as a Replication Bully for Halloween and Otherwise Dress as a Clown for Open Science</title><link href="http://osc.centerforopenscience.org/2014/11/26/clown-for-open-science/" rel="alternate"></link><updated>2014-11-26T14:00:00-05:00</updated><author><name>Jon Grahe</name></author><id>tag:osc.centerforopenscience.org,2014-11-26:2014/11/26/clown-for-open-science/</id><summary type="html">&lt;p&gt;Do you follow me on Twitter, are we Facebook friends? If so, you might know that my pictures almost always include me in a COS t-shirt. That is unless I’m promoting Little Jerry’s, my local burgers and breakfast joint that I also provide free endorsements for. This past summer, I traveled to Psi Chi and Council for Undergraduate Research meetings plus a family visit for three weeks. I only had 6 shirts in my carryon luggage, and there were four colors of COS t-shirts. I still wear my COS t-shirts at least weekly, and on any occasion where a new audience may have a chance to ask me about COS and various aspects of open science, and I try to make sure I post a picture of it on Facebook and Twitter. In the scientific revolution, I like to think of myself as a bard; but my colleagues might be more likely to label me as a clown.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/jgrahe_shirt1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;My mother-in-law is starting to think that something is wrong with me. What do I get out of it? Maybe you are starting to wonder too; more likely you don’t really care. However, I think you should, and I welcome competition from more popular, more creative people to take the title as “most variously photographed COS t-shirt wearer” in 2015. I think the challengers should also identify a better title for the winner. So I challenge you to become more active, personal advertisers of the COS specifically and open science generally.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reasons to Highlight the COS&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The COS is about a movement, not a person or a product; though they have products I often try to sell. More accurately, I should describe their products as give-aways. Their primary give-away, the OSF website (which makes scientific workflow easier and more transparent or kept private) and is ever improving. Their other products (Open Science Badges, opportunities to contribute to meta-science projects, the OSC Blog) are similarly valuable and worth investigating.  Throughout the fury about whether replication science is valuable or not, I predict that the COS will maintain a focus on methods of improving science and encouraging the positive revolution of science. I like the vision of a Scientific Utopia posited by Brian Nosek and colleagues. What role will you play in achieving that phase of our science?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Conversations From T-shirts&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When considering between your COS t-shirt or other clothing options, or when considering what to talk about with your friends, family, and social network, consider the following. Social promotion of the COS benefits other sciences more than psychology. Although I have introduced some peers and students to the COS by bringing them to the COS booth at conferences last year, I think my local market is saturated. Someone should study the social psychology population about their understanding of the scientific revolution and the practices that are being employed. So much of it is happening around us. However, it is other scientists, administrators, and young people in the general public that I most frequently engage in conversation about the t-shirts and why I wear them all the time.&lt;/p&gt;
&lt;p&gt;In these brief conversations, I talk about Psychology’s Crisis of Confidence and the importance and potential of Replication Science. I tell them about Ioannidis and that the problems extend to anywhere there is publication bias and entrenched methodology and status quo. I explain solutions offered by Open Science and the COS. My own personal agenda also includes a monologue about the potential contribution of undergraduates, to improving scientific discourse. Depending on the audience and the conversation, I may also talk about the dark side of open science. I explain my frustration with the status quo and the resistance I have witnessed to Replication Science. If they get interested, I suggest they look for phrases like “replication bully” or “replication science” and review the public discourse of the leaders in psychology. I tell them that while bullying is a potential problem, I think that the claim is inverted in this case. I invite them to draw their own conclusions and I encourage them to embrace open science where they can.  &lt;/p&gt;
&lt;p&gt;Most recently, I got a miniscule amount of Twitter activity wearing one of my COS t-shirts in a costume as a Replication Bully. I loved the costume. I included Open Science Badges as facepaint and drew them on my shield. I fashioned a nasty staff and joined my family medieval fantasy theme. Throughout the night, I probably explained that costume 25 times to peers at one event and then a host of younger people at another. Only a few prior students understood. &lt;/p&gt;
&lt;p&gt;&lt;img src="images/jgrahe_shirt2.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Many party goers ignored it completely, and some asked questions. On Twitter, I made some pithy statements about open science and the evils of the Replication Bully. What I hope is that some of the witnesses go visit the COS or talk about Open Science with their peers. Though this conversation is nauseating with each other, students and the general public seem to be interested and can help reach across scientific disciplines. Consider the disciplines impacted at your host institutions.&lt;/p&gt;
&lt;p&gt;Halloween demands villains, and I think the Replication Bully is a villain. Bullying should be avoided, and the replication bully should not be welcome in open science circles. However, as a caricature of this discussion &lt;a href="http://edge.org/conversation/simone-schnall-moral-intuitions-replication-and-the-scientific-study-of-human-nature"&gt;which still continues&lt;/a&gt;, it can help identify what should be avoided. Dr. Schnall suggests some parameters that should be avoided, and there are likely others. I therefore invite conversation when I wear my t-shirts, and allow the listener to draw conclusions. I want to keep the replication bully away and identifying these characteristics that are not tolerable is critical to Scientific Utopia. I explain how mistakes can be made and were made, and how they could and should be fixed. I have no shortage of opinions. At the core is the need to focus on open science principles. My experience with the COS sponsored crowd-sourcing science projects is that they always strive to achieve these principles.&lt;/p&gt;
&lt;p&gt;Is the COS a valuable contribution to our field?  Go visit their website and follow their work. There is no hurry, unless you want to miss out on an exciting part of the scientific revolution. The Reproducibility Project for all its limitations due to wide scope and challenging feasibility was a thoughtful precursor to the various Many Labs Projects. As I edit the undergraduate version of crowdsourcing science in psychology (see the Collaborative Replications and Education Project), I follow their lead. I try to maintain a similar level of quality, but it isn’t easy. They were nimble until just before data collection began, and responded to feedback and answer questions. They are also well-resourced both in their crowd of collaborators, and their collaboration with the COS. The excitement that I felt while working on the methodology summaries this summer as they prepared the pre-registered report was positive and refreshing and inviting. I wished I could do more, but there are only so many hours in a day. As I look to the future, I expect these projects will only continue to diversify. I hope they continue to answer interesting and important questions in our field.&lt;/p&gt;
&lt;p&gt;But I think that discussion is critical and if there are criticisms of the Many Labs 1, 2, or 3 methodology, it should be reported. I predict that it will likely be addressed in Many Labs 4, 5, ..., k. I welcome the Many Labs teams into my lab, and my classroom, and I welcome their vibrant introduction of openness and persistence into our science. For the CREP, the study selection resulted from identifying studies based on impact factor and feasibility for undergraduates to complete, but we recently started soliciting requests for favorite or classic studies. How does study selection impact bias of effect estimates? I bet some of those replication science hordes (a revolutionary population filled with normally distributed personalities and interpersonal styles no doubt) will identify methods of testing this and suggesting further tests and corrections.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/jgrahe_shirt3.jpg"&gt;&lt;/p&gt;
&lt;p&gt;When I planned my Halloween costume, I thought about dressing as a hero and imitating some replication science icons, but it is a time for horror so I chose a villain. Thanksgiving is coming; this blog is not likely to be live by then, but if I have the opportunity, I’ll pose with a COS tshirt and try to represent Thanks. I’ll tweet it and post it on FB. I’ll hope that it is ingenious enough to peek some interest in the COS and meta-science projects generally and the CREP indirectly. My next opportunity is the holiday season, and then conference season will ensue. In each case, I will expose as many people as possible to open science, either through personal connection or via social media. My goal will be trying to visually connect my renewed excitement in our field with the organization that will most likely bring about the improvement many of us have craved for decades at least.&lt;/p&gt;
&lt;p&gt;Why not embrace replication science opportunities? I saturate my social network with images of Open Science and invite them to ponder that question. While I am not a social media star, some of you might be. Pose in your COS tshirt, show the love, increase the impact and help brighten rather than darken our sciences’ future. I would like to see more interesting poses and locations than my limited imagination brings.&lt;/p&gt;
&lt;p&gt;If you prefer to demonstrate your effort by working hard and quietly advancing open science principles, so be it. If you engage in replication bullying, I will distance myself from you. If you are fun and lively and fancy yourself a bard, but might sometimes act like a clown, expand the conversation about open science beyond psychology. As with other social movements, change will come slowly and it is likely the next generation that will see the largest gains from the current revolution in methods and scientific values. In the meantime, I’ll go put my t-shirts in the laundry and think about what they may look like on a turkey.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/jgrahe_shirt4.jpg"&gt;&lt;/p&gt;</summary></entry><entry><title>The Noba Project and Open Science</title><link href="http://osc.centerforopenscience.org/2014/11/24/the-noba-project/" rel="alternate"></link><updated>2014-11-24T17:00:00-05:00</updated><author><name>Robert Diener</name></author><id>tag:osc.centerforopenscience.org,2014-11-24:2014/11/24/the-noba-project/</id><summary type="html">&lt;p&gt;The Noba Project is an open psychology education initiative of the Diener Education Fund. Noba was envisioned as an open and free substitute to traditional intro-to-psychology materials that cost anywhere from $100 to $200 for a single book.&lt;/p&gt;
&lt;p&gt;The primary goals of Noba are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To reduce financial burden on students by providing access to free educational content&lt;/li&gt;
&lt;li&gt;To provide instructors with a platform to customize educational content to better suit their curriculum&lt;/li&gt;
&lt;li&gt;To present free, high-quality material written by a collection of experts and authorities in the field of psychology&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Noba Project consists of a website (&lt;a href="http://www.nobaproject.com"&gt;www.nobaproject.com&lt;/a&gt;) where instructors, students, or independent learners can find a large and growing collection of learning modules written by well-known instructors and researchers. The website allows users to mix and match those modules into any order they like and publish a custom textbook that is preserved in each user's personal “Library” on the Noba website and that can be downloaded and distributed in PDF format as well.&lt;/p&gt;
&lt;p&gt;Noba has also curated the various modules into a number of “Ready-Made” textbooks that instructors can use as-is or edit to suit their needs. One example is the Noba book &lt;a href="http://nobaproject.com/textbooks/discover-psychology-a-brief-introductory-text"&gt;“Discover Psychology”&lt;/a&gt;, which was built to match the scope and sequence of standard Intro-to-Psychology courses. All Noba materials are licensed under the Creative Commons CC BY-NC-SA license. Users can freely use, redistribute, re-mix, and re-purpose the Noba content.&lt;/p&gt;
&lt;p&gt;The Diener Education Fund is co-founded by Drs. Ed and Carol Diener. Ed is the Joseph Smiley Distinguished Professor of Psychology (Emeritus) at the University of Illinois. Carol Diener is the former director of the Mental Health Worker and the Juvenile Justice Programs at the University of Illinois. Both Ed and Carol are award-winning university teachers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is there a role for undergraduates and graduate students to contribute?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We love students and want to engage them. This is why we created the &lt;a href="http://nobaproject.com/student-video-award/"&gt;Noba Video Award&lt;/a&gt;. Each year we award 10 thousand dollars for high quality short videos related to a specific Noba content area. Last year it was "memory" and we received more than 50 submissions from around the world. This year our topic is "social influence" and we hope to receive even more submissions. We believe that this presents an opportunity for students to engage in their learning in a more creative and active way. We also display top videos and integrate winning videos into our actual content modules. We also invite graduate and undergraduate students to write guest newsletter and blog posts for us. This is an opportunity for students to have their voice heard by their peers around the world. &lt;/p&gt;
&lt;p&gt;Another way we’ve included grad students is that when Noba first started, at the beginning of the writing process we asked our well-known authors if they had deserving graduate students that they wanted to work with and many of them were very receptive to that suggestion. A good number of our modules have graduate students as second authors. It meant a lot to us to be able to help give these younger researchers a chance to be involved in the writing and publication process. In the future as we produce new modules for Noba we'll encourage the same kind of collaboration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How often are the modules updated/edited?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because of our digital format we are able to make updates large and small as needed. On the smaller side we make regular updates to modules including new links that may recently have become available, or dropping in new examples where we think they might be helpful. We invite feedback from our users and we often get small recommendations for illustrations, figures, subtopics, etc. We try to make these types of revisions very quickly.&lt;/p&gt;
&lt;p&gt;The idea of making larger changes that keep pace with the research is a very difficult issue. The truth is most textbooks provide a broad overview and simply do not have room to dive deeply in new developments. Traditionally, textbook publishers have dealt with this by updating pop culture references, graphics, and adding brief mentions of newer research. Again, because of our digital format, we are lucky to be able to make major revisions in multiple ways. First, we can create whole new modules without concern for overall book length or publishing costs. For example, we have a whole module on the &lt;a href="http://nobaproject.com/chapters/biochemistry-of-love"&gt;“Biochemistry of Love."&lt;/a&gt; This is a niche topic that draws heavily from contemporary research and yet we can easily include it. We also make major changes by writing whole new modules that represent content at the 100/200 or the 300/400 level. For example, David Lubinski wrote a very advanced module on &lt;a href="http://nobaproject.com/chapters/intellectual-abilities-interests-and-mastery?r=NCw2Mjc4"&gt;"Intellectual abilities, Interest, and Mastery."&lt;/a&gt; This will be particularly interesting to students who want more challenge, sophistication, and depth. For more introductory level material, on the other hand, we have a basic unit on &lt;a href="http://nobaproject.com/chapters/intelligence"&gt;"Intelligence"&lt;/a&gt; that is comparable to what you would find in traditional intro psych textbooks. &lt;/p&gt;
&lt;p&gt;With regards to updating our editorial strategy is this: if we receive feedback about suggestions for small changes we try to revise immediately. If there are exciting breakthroughs we try to make mention of these emerging studies or new directions within our existing modules. Finally, for areas for which there is a cascade of new findings we entertain the idea of creating all new modules.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What subfields of psychology is NOBA interested in exploring next?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is an excellent question. If our resources were unlimited we would build out complete collections for clinical, developmental, biological, statistical and social psychology. In reality, we have to make careful decisions about what we invest in. We don't want to develop too much new material without first evaluating the strength of our current material. Ideas we have discussed recently include: developing more interactive learning for the modules we currently have, developing more material regarding actual careers in psychology, and developing more in-depth material in cognitive/neuroscience and social/personality. &lt;/p&gt;
&lt;p&gt;One of the most exciting developments for us was the realization that no matter how good the content is many instructors will be slow to adopt it because they need a suite of materials to help them teach it. As a result we invested heavily in creating power point presentations, test bank items, and a comprehensive instructor manual. All available free of charge, of course! &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To what extent can Noba contribute to open science/teaching in other countries? The major textbooks we get tend to have european/international editions where at least the anecdotes are adjusted so that non-american students could relate to them better.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are aware that we Americans can be-- well-- American centric in our views. We have attempted to side step that problem by using examples that are more global in nature, reporting measurements using the metric system, and inviting non-American authors to help with everything from our test bank items to our modules. At the same time some of our authors have probably used examples that in some cases are more recognizable to North American students.&lt;/p&gt;
&lt;p&gt;What we would love to see happen is for users outside the US to create new versions of Noba content to better suit their local context and then put that material back into the Creative Commons for others to use and perhaps build on again. That was one of the most important reasons for choosing an open CC license for Noba that allows derivatives. It would be impossible for us to customize Noba to suit the most specific needs of users everywhere. But local instructors do know what will work best for their students and are free to take Noba as the foundation and then iterate to make a better learning experience that reflects their local situation. And we encourage instructors in the US to do this as well. We trust instructors to make good choices about ways to add on to Noba to benefit their students wherever they are.&lt;/p&gt;
&lt;p&gt;In the end, Noba can be thought of as more than a text-book substitute. It represents a trend toward open educational resources. While the primary aim of Noba is to save students money and remove obstacles to education there is a secondary aim as well. By using the virtues of the digital medium and by paying attention to the latest research on teaching and learning we have the opportunity to fundamentally reimagine what instructional materials are. &lt;/p&gt;</summary></entry><entry><title>Clinical trials, replication, and crowdsourcing</title><link href="http://osc.centerforopenscience.org/2014/11/19/clinical-trials-replication-crowdsourcing/" rel="alternate"></link><updated>2014-11-19T12:00:00-05:00</updated><author><name>Gustav Nilsonne</name></author><id>tag:osc.centerforopenscience.org,2014-11-19:2014/11/19/clinical-trials-replication-crowdsourcing/</id><summary type="html">&lt;p&gt;The replication drive in psychology continues to inspire debate. Simone Schnall has recently published a &lt;a href="http://www.psychol.cam.ac.uk/cece/blog"&gt;set of views&lt;/a&gt; which I think has potential to helpfully advance a conversation about replication research. In her discussion, one analogy struck me as particularly interesting: that to clinical trials. Schnall suggests that crowdsourced replications of clinical trials would be absurd. Surely, we can’t have patients organizing themselves into trials spontaneously, buying over-the-counter drugs, and reporting their outcomes online?&lt;/p&gt;
&lt;p&gt;Possibly not. But we can have patients and doctors working together to do it, and for a wide range of clinical questions. Ideally, if there is uncertainty about what treatment would be best for me, my physician should key in my information into a computer and receive an immediate offer to randomize me to either of several reasonable treatment options. My health outcomes would then be automatically transferred into a clinical outcomes registry. Once enough patients have been randomized and results are conclusive, we will know which treatment is best, and then we can move on to the next question and keep randomizing. What I have just described is a &lt;a href="http://dx.doi.org/10.1056/NEJMp1310102"&gt;randomised registry clinical trial&lt;/a&gt;. These trials &lt;a href="http://dx.doi.org/10.3310/hta18430"&gt;can and will be&lt;/a&gt; implemented step by step into health systems that are able to monitor outcomes well.&lt;/p&gt;
&lt;p&gt;Randomized registry trials can be used not only to test new treatments. They can also replicate clinical trials cheaply and easily. And &lt;a href="http://www.badscience.net/2014/06/what-statins-tell-us-about-the-mess-in-evidence-based-medicine/"&gt;we sorely need it&lt;/a&gt;. Think about it: Randomized controlled trials, in spite of ever-present challenges and limitations, tend to provide reasonably good evidence for the effect of a drug among the patients in the trial. But are you like those patients? Maybe the trial was done 20 years ago. Maybe the trial included only men, and you are a woman. Maybe you have another disease besides the one for which you are taking this drug. To catch moderating factors like these we need another trial. There is no end. To keep improving, we must keep randomizing, keep replicating, and keep extending our clinical questions.&lt;/p&gt;
&lt;p&gt;Thus, randomized registry trials can be thought of in a sense as crowdsourced replications. They can be used for direct replications to improve estimates of treatment effects when evidence is inconclusive. They can also be used to verify the external validity of concluded clinical trials, i.e. to test whether a treatment really works in other people, in other places, and in this day and age. Compared to regular clinical trials, they can be a cheap and effective way to improve the evidence base for shared decision making.&lt;/p&gt;
&lt;p&gt;Schnall advances two main counterarguments. First, she worries that patients’ confidence in drugs may be undermined. In my opinion, that would be likely to happen mostly if confidence was too high in the first place. If we show that another treatment option has stronger benefits, we will have done the patients a great service. They can then discuss their treatment options with their physicians again, and benefit from a new incremental increase in survival, quality of life, and/or decrease in costs. Here is a direct analogy to basic research. If we can succeed in undermining confidence in results that are not true, then the scientific community and the general public will benefit.&lt;/p&gt;
&lt;p&gt;Schnall also suggests that pharmaceutical companies will not like to have their results replicated, because it may harm their reputation if the replication fails. I hope it is true that pharmaceutical companies’ concern for their reputation is a factor that helps to keep them from making unsupported claims about their products. But if they did so anyway, patients and physicians have a right to know. It is well known that industry-sponsored clinical trials have &lt;a href="http://dx.doi.org/10.1136/bmj.g213"&gt;considerable publication bias&lt;/a&gt;, leading to inflated effect estimates. Patients of flesh and blood are harmed every day as a result. Pharmaceutical companies can improve their reputation by &lt;a href="http://www.alltrials.net/find-out-more/"&gt;publishing all their trial data&lt;/a&gt;. Again, the analogy to basic research holds. Publication bias and other questionable practices distort the literature to some extent in every field. We researchers can protect our reputations by practicing science openly.&lt;/p&gt;
&lt;p&gt;Clinical trials are an interesting model for other kinds of research. The lesson is not to replicate less, nor to avoid crowdsourcing. On the contrary, the lesson is to always keep measuring effects and to &lt;a href="http://dx.doi.org/10.1177/1745691614529796"&gt;continuously update cumulative meta-analytic estimates&lt;/a&gt;. These are a few thoughts, which I offer in the hope of contributing to a constructive conversation about replication research.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Gustav Nilsonne, MD, PhD is a researcher at Stockholm University and Karolinska Institutet.  You can reach him at gustav.nilsonne@ki.se&lt;/em&gt;&lt;/p&gt;</summary></entry><entry><title>Facilitating Radical Change in Publication Standards</title><link href="http://osc.centerforopenscience.org/2014/11/12/facilitating-radical-change/" rel="alternate"></link><updated>2014-11-12T12:00:00-05:00</updated><author><name>Denny Borsboom</name></author><id>tag:osc.centerforopenscience.org,2014-11-12:2014/11/12/facilitating-radical-change/</id><summary type="html">&lt;p&gt;This train won't stop anytime soon. &lt;/p&gt;
&lt;p&gt;That's what I kept thinking during the two-day sessions in Charlottesville, where a diverse array of scientific stakeholders worked hard to reach agreement on new journal standards for open and transparent scientific reporting. The aspired standards are intended to specify practices for authors, reviewers, and editors to follow in order to achieve higher levels of openness than currently exist. The leading idea is that a journal, funding agency, or professional organization, could take these standards off-the-shelf and adopt them in their policy. So that when, say, &lt;em&gt;The Journal for Previously Hard To Get Data&lt;/em&gt; starts to turn to a more open data practice, they don't have to puzzle on how to implement this, but may instead just copy the data-sharing guideline out of the new standards and post it on their website. &lt;/p&gt;
&lt;p&gt;The organizers&lt;sup&gt;1&lt;/sup&gt; of the sessions, which were presided by Brian Nosek of the Center for Open Science, had approached methodologists, funding agencies, journal editors, and representatives of professional organizations to achieve a broad set of perspectives on what open science means and how it should be institutionalized. As a result, the meeting felt almost like a political summit. It included high officials from professional organizations like the American Psychological Association (APA) and the Association for Psychological Science (APS), programme directors from the National Institutes of Health (NIH) and the National Science Foundation (NSF), editors of a wide variety of psychological, political, economic, and general science journals (including &lt;em&gt;Science&lt;/em&gt; and &lt;em&gt;Nature&lt;/em&gt;), and a loose collection of open science enthusiasts and methodologists (that would be me). &lt;/p&gt;
&lt;p&gt;The organizers had placed their bets adventurously. When you have such a broad array of stakeholders, you've got a lot of different interests, which are not necessarily easy to align – it would have been much easier to achieve this kind of task with a group of, say, progressive methodologists. But of course if the produced standards succeed, and are immediately endorsed by important journals, funders, and professional organizations, then the resulting impact is such that it might change the scientific landscape forever. The organizers were clearly going for the big fish. And although one can never be sure with this sort of thing, I think the fish went for the bait, even though it isn't caught just yet.&lt;/p&gt;
&lt;p&gt;Before the meeting, subcommittees had been tasked to come up with white papers and proposed standards on five topics: Open standards for data-sharing, reporting of analyses, reporting on research design, replication, and preregistration. During the meeting, these standards were discussed, revised, discussed, and revised again. I don't want to go into the details of exactly what the standards will contain, as they are still being revised at this time, but I think the committees have succeeded in formulating a menu of standards that are digestible for both progressive and somewhat more conservative stakeholders. If so, in the near future we can expect broad changes to take place on these five topics.&lt;/p&gt;
&lt;p&gt;For me, one of the most interesting features of the meeting was that it involved such diverse perspectives. For instance, when you talk about data-sharing, what exactly are the data you want people to share? In psychology, we're typically just talking about a 100kB spreadsheet, but what if the data involve Terabytes of neural data? And what about anthropological research, in which the data may involve actual physical artifacts? The definition of data is an issue that seems trivial from a monodisciplinary perspective, but that might well explode in your face if it is transported to the interdisciplinary realm. Similarly, halfway through the meeting, the people involved in clinical trials turned out to have a totally different understanding of preregistration as compared to the psychologists in the room. It was fascinating to see how fields slice up their research reality differently, and how they wrestle with different issues under the same header (and with the same issue under different ones). &lt;/p&gt;
&lt;p&gt;Despite these differences, however, I felt that we all did have a clear target on the horizon, and I am confident that the new standards will be endorsed by many, if not all, of the stakeholders present at the meeting. Of course, it is a great advantage that leading journals like Science and Nature endorse higher standards op openness, and that funders like NIH and NSF are moving too. I sometimes have to pinch myself to make sure I am not dreaming, but there is real evidence that, after all these years, actual change is finally taking place: see &lt;a href="http://www.nih.gov/about/reporting-preclinical-research.htm"&gt;NIH's endorsement of open science&lt;/a&gt; in preclinical research, the Royal Society's &lt;a href="http://royalsocietypublishing.org/data-sharing"&gt;new guidelines&lt;/a&gt; which make open data mandatory, and the joint editorial on these issues that was simultaneously published by &lt;a href="http://www.sciencemag.org/content/346/6210/679.full"&gt;Science&lt;/a&gt; and &lt;a href="http://www.nature.com/news/journals-unite-for-reproducibility-1.16259?WT.ec_id=NATURE-20141106"&gt;Nature&lt;/a&gt; earlier this week. This effectively means that we live in a new world already.&lt;/p&gt;
&lt;p&gt;Perhaps the most important thing about this meeting was that it proves how important openness and transparency have become. There are very few topics that could command the agendas of so many leaders in the field to align, so that they can all get together at the same time and place, to spend two days sweating on a set of journal standards. Today, open science is such a topic. &lt;/p&gt;
&lt;p&gt;This train won't stop anytime soon. &lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The meeting was organized by Brian Nosek (Center for Open Science), Edward Miguel (University of Berkeley), Donald Green (Columbia University), and Marcia McNutt (Editor-in-Chief of Science Magazine); funded by the Laura and John Arnold Foundation; and sponsored by the Center Open Science, the Berkeley Initiative for Transparency in the Social Sciences, and Science Magazine.&lt;/p&gt;</summary></entry><entry><title>Inflated Grades, Inflated Science</title><link href="http://osc.centerforopenscience.org/2014/11/05/inflated-grades-inflated-science/" rel="alternate"></link><updated>2014-11-05T12:00:00-05:00</updated><author><name>Sean P. Mackinnon</name></author><id>tag:osc.centerforopenscience.org,2014-11-05:2014/11/05/inflated-grades-inflated-science/</id><summary type="html">&lt;p&gt;This year, I’ve been teaching a lot more than usual. With that extra teaching comes a lot more grading – and students with concerns about grades.  With &lt;a href="http://www.psmag.com/education/good-reason-grade-inflation-stay-princeton-harvard-university-71087/"&gt;all the talk about grade inflation lately&lt;/a&gt;, I’ve been thinking about HOW grades come to be inflated. While there are certainly &lt;a href="http://drjamesthompson.blogspot.de/2014/07/leberwusrt-university-somewhere-in.html"&gt;political pressures&lt;/a&gt; from governments and institutions to produce “successful” students that contribute to grade inflation, I’m thinking about these problems like a psychologist -- a data analyst, really.  I’ve learned a lot by working in the open science movement about the psychological processes that underlie questionable research practices and effect size inflation. In this post, I want to draw parallels between grade inflation at universities, and effect size inflation in scientific research.  To some extent, I think there are similar psychological processes that contribute to both problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grades have monetary value&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;High university grades have pragmatic, monetary value to students. At the low end, avoiding failures means not having to pay extra tuition to re-take a course. On the other end, high university grades increase the odds that students will receive scholarships to pay for their education and provides increased access to graduate-level education – which for many individuals, means a &lt;a href="http://www.theglobeandmail.com/news/national/education/tinker-tailor-university-grad-guess-which-one-makes-the-most-money/article17023256/"&gt;higher-paid, more prestigious job&lt;/a&gt;. It makes sense then that students will act in their best interest to improve their own grades, while actively avoiding situations that will negatively impact their grades.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Measurement of Academic Success Contains Error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;University grades are a psychological construct that (theoretically) measure a student’s capacity for critical thought, knowledge of the subject matter, and analytical skill. Like any psychological construct, teachers need to operationalize this construct with specific measures – in university, that usually means essays, exams, quizzes and assignments. Classical test theory suggests that all measurement is imperfect:&lt;/p&gt;
&lt;p&gt;True Score = Measured Score + Error&lt;/p&gt;
&lt;p&gt;So, whenever we grade a student’s exam or essay, it approximates their true level of competence with a certain degree of inaccuracy. Maybe you add up the final grade incorrectly, create a poor multiple choice question, or are just in a bad mood when you graded that essay. All of this is error.&lt;/p&gt;
&lt;p&gt;Another assumption underlying many statistics is that the residuals are normally distributed. So, sometimes you give students grades that are too high, and sometimes you give grades that are too low, but on average these will tend to cancel each other out in terms of the average grades for your class (assuming randomly distributed errors).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stacking the deck to get an advantage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The thing is (despite these statistical truisms) most students think that getting a lower grade than they deserve is terribly unfair.  Students will tend to work in their own self-interest to get better grades – and the tangible rewards that go along with good grades. Through a few consistently applied tactics, students positively bias the error terms – and in doing so, tend to inflate grades overall.  There are at least three primary ways students might do this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contesting grades&lt;/strong&gt;: When students receive a grade that is lower than they deserve – which will happen occasionally due to measurement error in the grading process – students will often try to convince their professor to change that grade.  However, I have yet to have a student argue that they should receive a lower grade even though it’s probable that I’ve made those kinds of errors too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dropping classes&lt;/strong&gt;: When students figure they are going to fail a course, then tend to drop the course so it won’t count against their GPA. Thus, the worst instances of performance can be wiped from their record. Of course, students drop classes for all kinds of reasons – however, I think it’s reasonable to say that poor grades increase the odds a student will drop a class (in fact it’s &lt;a href="https://www.southeastern.edu/admin/ir/about_us/presentations/sair_2003.pdf"&gt;the #1 reason students give for withdrawing from courses&lt;/a&gt; in one survey).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Taking easy classes&lt;/strong&gt;: It’s easier to get a good grade in some classes than others. Sometimes it’s because the material is easier, while other times it’s because the student has prior experience (e.g., a French immersion student taking introductory French in university).  While I don’t have hard evidence to prove this, I think most teachers were students themselves long enough to understand that plenty of students are thinking about this when selecting classes.&lt;/p&gt;
&lt;p&gt;Because students selectively contest poor grades, and drop courses more frequently when their performance is poor, and actively search for classes where it is easier to get a good grade, this produce a selective positive bias in overall grades.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The parallel with research effect sizes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I think there are clear parallels between grade inflation and inflated effect sizes in science. Like students and good grades, statistically significant results have a pragmatic, monetary value for scientists. Statistically significant results are much more likely to be published in scientific journals.  Publications are the currency in academia – numerous, high-impact publications are required to win grants, tenure, and sometimes even to keep your job. It’s not surprising then that scientists will go to great lengths to get statistically significant results to publish in journals – sometimes creating &lt;a href="http://osc.centerforopenscience.org/2013/10/16/thriving-au-naturel-amid-science-on-steroids/"&gt;“manufactured beauties”&lt;/a&gt; in the process. &lt;/p&gt;
&lt;p&gt;In a &lt;a href="http://osc.centerforopenscience.org/2014/07/09/response-to-jason-mitchell/"&gt;previous post&lt;/a&gt;, I talked about how sampling variation can lead some researchers to find null results, even when the experimenter has done everything right. Imagine you get grant money to run an experiment. It’s well-grounded in theory, and you even go to great lengths to ensure you have 95% power. You collect the data and run the experiment and eagerly check your results … only to find that your hypotheses were not supported.  With 95% power, 1 in 20 research studies will end up like this (assuming a frequentist approach which, for better or worse, is dominant in psychology right now). This is a frightening fact of our line of work – there is an element of random chance that permeates statistics. Psychologically speaking, this feels dreadfully unfair. You did good scientific work, but because of outside pressures to prioritize “statistically significant” work with ps &amp;lt; .05, your research will encounter significant challenges getting published.  With this in mind it makes sense why many scientists engage in behaviours to “game” this potentially unfair system – the rationale is not that different from those of our students trying to make their way through university with good grades.&lt;/p&gt;
&lt;p&gt;Like many in the OSC, I think that major structural changes are needed at the level of the scientific journals to incentivize good research practices, rather than simply incentivizing novel, statistically significant results. When I look at the issues using grades as an analogy, it seems to me that asking scientists to change their questionable research practices without changing the underlying structural problems is a lot like asking students to simply accept a bad grade -- good for the credibility of the institution, but bad for the individual. &lt;/p&gt;
&lt;p&gt;Thinking about these two issues together has been an interesting thought experiment both in empathizing with student concerns, and with understanding precisely what it is about the current climate of publishing that feels so unfair sometimes. Like my students, I’d like to believe that hard work should equate to career success – however, the unfortunate truth of science is that there is a bit of luck involved. Even if I disagree with &lt;a href="http://www.psychologicalscience.org/index.php/news/releases/questionable-research-practices-surprisingly-common.html"&gt;questionable research practices&lt;/a&gt;, think I can understand why many people do it. Probability can be cruel.&lt;/p&gt;</summary></entry><entry><title>Reproducible Reporting</title><link href="http://osc.centerforopenscience.org/2014/10/30/reproducible-reporting/" rel="alternate"></link><updated>2014-10-30T12:00:00-04:00</updated><author><name>Ingmar Visser</name></author><id>tag:osc.centerforopenscience.org,2014-10-30:2014/10/30/reproducible-reporting/</id><summary type="html">&lt;h2&gt;Why reproducible reporting?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;You are running an experiment when a conference deadline comes along; you quickly put together an abstract with 'preliminary findings' showing this or that effect. When preparing the talk for the conference, typically months later, you find a different p-value, F-value, t-value or whatever your favorite statistic was; it may still be significant or as expected, but different regardless; this nags you.&lt;/p&gt;
&lt;p&gt;You are preparing a manuscript with multiple experiments, a dozen or so tables and figures and many statistical results such as t-tests, ANOVA's et cetera. During manuscript preparation you decide on using another exclusion criterion for your response time data as well as your participants, all for good reasons. Then, the new semester starts and you are overloaded with teaching duties, and you are only able to start working on your manuscript weeks later. Did you already update figure 5 with the new exclusion criteria? Did you already re-run the t-test on page 17 concerning the response times? &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately, I myself and many others are familiar with such experiences of not being able to reproduce results. Here, reproducibility refers &lt;em&gt;not&lt;/em&gt; to the aspect of experimental results being reproducible. Reproducibility of experimental results has been written about frequently (see &lt;a href="http://osc.centerforopenscience.org/2014/08/07/talk-about-replication/"&gt;discussion here&lt;/a&gt;), and serious effort is now put into testing the reproducibility of common results (eg in the &lt;a href="https://osf.io/ezcuj/wiki/home/"&gt;reproducibililty project&lt;/a&gt;), as well as improving standards when it comes to experimentation (eg by pre-registration, see &lt;a href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/"&gt;discussion here&lt;/a&gt;). Rather, this post focuses on reproducibility of preparing the data, statistical analyses, producing figures and tables, and reporting of results in a journal manuscript. &lt;/p&gt;
&lt;p&gt;During my own training as a psychologist, not much attention was given in the curriculum to standards of bookkeeping, lab-journals, and reporting of results. Standards that are in place focus on the stylistic aspects of reporting, such as that the F in reporting an F-value should be italicized, rather than upright. The APA reporting guidelines concern mostly matters of form and style, such as the exquisitely detailed guidelines for producing references. While such uniformity of manuscripts in form and style are highly relevant when it comes to printing material in journals, those guidelines have not much to say about the contents of what is reported and about how to maintain the reproducibility of analyses.  &lt;/p&gt;
&lt;h2&gt;What is reproducible reporting?&lt;/h2&gt;
&lt;p&gt;When it comes to reproducible reporting, the goal is to have a workflow from (raw) data files to (pre-)print manuscript in such a way that every step along the way is reproducible, by others, including your future self -- for example, by others who have an interest in the results for the purposes of replication or quality control. It may be useful to have someone particular in mind such as &lt;a href="http://wicherts.socsci.uva.nl/"&gt;Jelte Wicherts&lt;/a&gt; or &lt;a href="http://opim.wharton.upenn.edu/~uws/"&gt;Uri Simonsohn&lt;/a&gt;, both famous for tracking down errors (and worse) in published research -- hence, the goal is to make your work, say, Wicherts-proof. Typically, there are at least three phases involved in (re-)producing reports from data: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;transforming raw data files to a format that admits statistical analysis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;making figures and tables, and producing analytical results from the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;putting it all together into a report, either a journal manuscript, a presentation for a conference, or a website&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To accomplish reproducibility in reporting, the following are important desiderata for our toolchain: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scripting&lt;/strong&gt; Using a scripting language (as opposed a point-and-click interface) for the statistical analyses, such as &lt;a href="http://www.r-project.org/"&gt;R&lt;/a&gt;, has the important advantage that analyses can easily be reproduced using the script. Popular statistical analysis packages, such as SPSS, also admit of the possibility of saving scripts of analyses. However, it is not necessary to do so, and hence one can easily omit doing so as it is not the &lt;em&gt;standard&lt;/em&gt; way of operating. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One tool instead of many&lt;/strong&gt; R (and possibly other similar statistical packages) can be used to perform &lt;em&gt;all&lt;/em&gt; the analytical steps from reading raw data files to analysing data, and producing (camera-ready) figures and tables; again, this is a major advantage over using a combination of text-file editors (to reformat raw data), a spreadsheet program, to clean the raw data, SPSS or otherwise to do the analyses, and a graphical program to produce figures. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;High quality output in multiple formats&lt;/strong&gt; A toolchain to produce scientific output should produce high quality output, preferably suitable for immediate publication; that is, it should produce camera-ready copy. It should also produce outputs in different formats, that is, analysis scripts and text should be re-usable to produce either journal manuscripts, web pages or conference presentations. Other important requirements are i) portability (anyone using your source, in plain text format, should be able to reproduce your manuscript, whether they are working on Mac, Linux, Windows or otherwise), ii) flexibility (seamless integration of references and production of the reference list, automatic table of contents, indexing), iii) scalability (the tool should work similarly for small journal manuscripts and multi-volume books), iv) the separation of form and content (scientific reporting should be concerned with content, formatting issues should be left to journals, website maintainers et cetera). &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maintain a single source&lt;/strong&gt; Possibly the most important requirement for reproducible reporting is that the three phases of getting from raw data to pre-print manuscript are kept in a single file; data preprocessing, data analysis, and the text making up a manuscript should be tightly coupled in a single place. A workflow that only comprises a single file and where analytical results are automatically inserted into the text of a manuscript prevents common errors such as forgetting to update tables and figures (did I update this figure with the new exclusion criteria or not?), and, most importantly, simply making typing errors in copying results (the latter is arguably quite a common source of errors and hard to detect, especially so when the results are in favour of your hypotheses). &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fortunately, many tools are available these days that satisfy these requirements to help ensure reproducibility of each of the phases of reporting scientific results. In particular, data pre-processing and data analysis can be made reproducible using &lt;a href="http://www.r-project.org/"&gt;R&lt;/a&gt;, an open source script based statistics package, which currently has close to 6000 add-on packages to do analyses ranging from simple t-tests to multi-level irt models, and from analysing two-by-two tables to intricate Markov models for complex panel designs. &lt;/p&gt;
&lt;p&gt;As for the third requirement, high quality output, LaTeX is the tool of choice. LaTeX has been the de-facto standard for high-quality typesetting in the sciences since its inception in 1986 by &lt;a href="http://en.wikipedia.org/wiki/Leslie_Lamport"&gt;Leslie Lamport&lt;/a&gt;. LaTeX belongs to the family of &lt;a href="http://en.wikipedia.org/wiki/Markup_language"&gt;markup languages&lt;/a&gt;, together with HTML, Markdown, XML, et cetera; the key characteristic of markup languages is the separation of form and content. Writing in LaTeX means assigning semantic labels to parts of your text, such as &lt;em&gt;title&lt;/em&gt;, &lt;em&gt;author&lt;/em&gt;, &lt;em&gt;section&lt;/em&gt;, et cetera. Rather than deciding that a &lt;em&gt;title&lt;/em&gt; should be type-set in 12-point Times New Roman, as an author I just want to indicate what the title &lt;em&gt;is&lt;/em&gt;. Journals, websites and other media can then decide what their preferred format is for &lt;em&gt;title&lt;/em&gt;'s in their medium and use the source LaTeX file to produce say PDF, for a journal, or HTML, for a website, output. Separation of form and content is precisely the feature that allows this flexibility in output formats. &lt;/p&gt;
&lt;p&gt;The superior typographical quality of LaTeX produced output is nicely illustrated &lt;a href="http://www.zinktypografie.nl/latex.php?lang=en"&gt;here&lt;/a&gt;, showing improved hyphenation, spacing, standard use of ligatures and others. Besides typographical quality, portability, automatic indexing, producing tables of contents, citations and referencing, and scalability were built into the design of LaTeX. See &lt;a href="http://www.andy-roberts.net/writing/latex/benefits"&gt;here&lt;/a&gt;, for a -- balanced -- discussion of the benefits of LaTeX over other word-processors.&lt;/p&gt;
&lt;p&gt;The real benefit of these tools -- R and LaTeX -- comes in when they are used in combination. Several tools are available to combine R with LaTeX in a single document and the main advantage of this combination is that all three phases scientific report production are combined in a single document, fulfilling the fourth requirement. In the following I provide some minor examples of using R/LaTeX. &lt;/p&gt;
&lt;h2&gt;How to get started with reproducible reporting?&lt;/h2&gt;
&lt;p&gt;The main tools required for combining statistical analyses in R with LaTeX are Sweave and knitr. Below are the main portals for getting the required -- free! open-source! -- software, as well as some references to appropriate introductory guides. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Getting started with R: the &lt;a href="http://www.r-project.org/"&gt;R home page&lt;/a&gt;, for downloads of the program, add-on packages and manuals; as for the latter, &lt;a href="http://cran.r-project.org/doc/contrib/Baron-rpsych.pdf"&gt;R for psychologists&lt;/a&gt;, is particularly useful with worked examples of many standard analyses. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Getting started with LaTeX: the &lt;a href="http://www.latex-project.org/"&gt;LaTeX-homepage&lt;/a&gt; for downloads, introductory guides and much much more; here's a &lt;a href="http://www.stdout.org/~winston/latex/latexsheet.pdf"&gt;cheat sheet&lt;/a&gt; that helps in avoiding having to read introductory material. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Getting started with &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/utils/doc/Sweave.pdf"&gt;Sweave&lt;/a&gt;, or alternatively &lt;a href="http://yihui.name/knitr/"&gt;knitr&lt;/a&gt;; both provide minimal examples to produce reproducible reports. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If using these programs through their command-line interfaces seems overwhelming to start with, &lt;a href="http://www.rstudio.com/"&gt;Rstudio&lt;/a&gt; provides a nice graphical user interface for R, as well as having options to produce PDF's from Sweave and/or knitr files. &lt;/p&gt;
&lt;p&gt;Needless to say, &lt;a href="http://google.com"&gt;Google&lt;/a&gt; is your friend for finding more examples, troubleshooting et cetera. &lt;/p&gt;
&lt;h3&gt;Minimal R, LaTeX and Sweave example&lt;/h3&gt;
&lt;p&gt;A minimal example of using R and LaTeX using Sweave can be downloaded here: &lt;a href="http://osc.centerforopenscience.org/static/reproducible-reporting.zip"&gt;reproducible-reporting.zip&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get it working do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install Rstudio&lt;/li&gt;
&lt;li&gt;Install LaTeX&lt;/li&gt;
&lt;li&gt;Open the .Rnw file in Rstudio after unpacking and run Compile pdf&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Some Markdown examples&lt;/h3&gt;
&lt;p&gt;Similar to the combination of LaTeX, R in Sweave, &lt;a href="http://en.wikipedia.org/wiki/Markdown"&gt;Markdown&lt;/a&gt; combines simple markup and R code to produce HTML pages as done in this blog. The examples below illustrate some of the possibilities. &lt;/p&gt;
&lt;p&gt;To get the flavour, start with loading some data and use 'head(sleep)' to see what's in it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data&lt;span class="p"&gt;(&lt;/span&gt;sleep&lt;span class="p"&gt;)&lt;/span&gt;
head&lt;span class="p"&gt;(&lt;/span&gt;sleep&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/plot1.png" alt="Fraser Neiman addresses crowd" &gt;&lt;/p&gt;
&lt;p&gt;This data gives the 'case' values for application of two types of drug and its influence on hours of sleep. The following plots the means and standard deviations of the two types of treatments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;extra&lt;span class="o"&gt;~&lt;/span&gt;group&lt;span class="p"&gt;,&lt;/span&gt;data&lt;span class="o"&gt;=&lt;/span&gt;sleep&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/plot2.png" alt="Fraser Neiman addresses crowd" &gt;&lt;/p&gt;
&lt;p&gt;This data set is the one used by Student to introduce his t-test, so it's only fitting to do such a test on these data here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;t.test&lt;span class="p"&gt;(&lt;/span&gt;extra&lt;span class="o"&gt;~&lt;/span&gt;group&lt;span class="p"&gt;,&lt;/span&gt;data&lt;span class="o"&gt;=&lt;/span&gt;sleep&lt;span class="p"&gt;,&lt;/span&gt;paired&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;img src="images/plot3.png" alt="Fraser Neiman addresses crowd" &gt;&lt;/h2&gt;
&lt;p&gt;There is much more to say about reproducibility in reporting and statistical analysis than a blog can contain. There are several recent books on the topic for further reading: &lt;/p&gt;
&lt;p&gt;Gandrud, C. (2013). Reproducible research with R and RStudio. CRC Press.&lt;/p&gt;
&lt;p&gt;Stodden, V., Leisch, F., &amp;amp; Peng, R. D. (Eds.). (2014). Implementing Reproducible Research. CRC Press.&lt;/p&gt;
&lt;p&gt;Xie, Yihui. Dynamic Documents with R and knitr. CRC Press, 2013.&lt;/p&gt;
&lt;p&gt;... and an online course &lt;a href="https://www.coursera.org/course/repdata"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;There is a world to win in acceptance of reproducible research practices, and reproducible reporting should be part and parcel of that effort. Such acceptance not only requires investment on the side of scientists and authors but also on the side of journals, in that they should accept LaTeX/R formatted manuscripts. Many psychometrics and mathematical psychology journals do accept LaTeX, as well as Elsevier, who provide explicit instructions for &lt;a href="http://www.elsevier.com/author-schemas/latex-instructions"&gt;submitting manuscripts in LaTeX&lt;/a&gt;, and so does Frontiers, see &lt;a href="http://www.frontiersin.org/Journal/AuthorInfo/ManuscriptGuidelines.aspx"&gt;here&lt;/a&gt;. Change in journal policy can be brought about by (associate) editors: if you have such a position make sure that your journal subscribes to open-source, reproducible reporting standards as this will also help improve the standards of reviewing and eventually the journal itself. As a reviewer, one may request to see the data and analysis files to aid in judging adequacy of reported results. At least some journals, such as the Journal of Statistical Software, require the source and analysis files to be part of every submission; in fact, JSS, accepts LaTeX submissions only, and this should become the standard throughout psychology and other social sciences as well. &lt;/p&gt;</summary></entry><entry><title>Two Calls to Conscience in the Fight for Open Access</title><link href="http://osc.centerforopenscience.org/2014/10/24/two-calls-to-conscience/" rel="alternate"></link><updated>2014-10-24T12:00:00-04:00</updated><author><name>Shauna Gordon-McKeon, Sheila Miguez</name></author><id>tag:osc.centerforopenscience.org,2014-10-24:2014/10/24/two-calls-to-conscience/</id><summary type="html">&lt;p&gt;In celebration of &lt;a href="http://www.openaccessweek.org/"&gt;Open Access Week&lt;/a&gt;, we'd like to share two pieces of writing from open access advocates who faced or are facing persecution for their efforts towards sharing knowledge.&lt;/p&gt;
&lt;p&gt;The first is a letter from Diego A. Gómez Hoyos.  Gomez is a Colombian graduate student studying biodiversity who is facing up to eight years in prison for sharing a research article.  &lt;a href="http://www.karisma.org.co/compartirnoesdelito/?p=384"&gt;He writes&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The use of FLOSS was my first approach to the open source world. Many times I could not access ecological or statistical software, nor geographical information systems, despite my active interest in using them to make my first steps in research and conservation. As a student, it was impossible for me to cover the costs of the main commercial tools. Today, I value access to free software such as &lt;a href="http://www.r-project.org/"&gt;The R project&lt;/a&gt; and &lt;a href="http://www.qgis.org/"&gt;QGis project&lt;/a&gt;, which keep us away from proprietary software when one does not have the budget for researching.&lt;/p&gt;
&lt;p&gt;But it was definitely since facing a criminal prosecution for sharing information on the Internet for academic purposes, for ignoring the rigidity of copyright law, that my commitment to support initiatives promoting open access and to learn more about ethical, political, and economic foundations has been strengthened.&lt;/p&gt;
&lt;p&gt;I am beginning my career with the conviction that access to knowledge is a global right. The first articles I have published in journals have been under Creative Commons licenses. I use free or open software for analyzing. I also do my job from a social perspective as part of my commitment and as retribution for having access to public education in both Colombia and Costa Rica.&lt;/p&gt;
&lt;p&gt;From the situation I face, I highlight the support I have received from so many people in Colombia and worldwide. Particularly, I thank the valuable support of institutions working for our freedom in the digital world. Among them I would like to acknowledge those institutions that have joined the campaign called &lt;a href="https://act.eff.org/action/unamonos-para-promover-el-acceso-abierto-en-todo-el-mundo"&gt;“Let’s stand together to promote open access worldwide”&lt;/a&gt;: &lt;a href="https://www.eff.org/"&gt;EFF&lt;/a&gt;, &lt;a href="http://www.karisma.org.co/"&gt;Fundación Karisma&lt;/a&gt;, &lt;a href="https://creativecommons.org/"&gt;Creative Commons&lt;/a&gt;, &lt;a href="https://archive.org/index.php"&gt;Internet Archive&lt;/a&gt;, &lt;a href="http://keionline.org/"&gt;Knowledge Ecology International&lt;/a&gt;, &lt;a href="https://www.openaccessbutton.org/"&gt;Open Access Button&lt;/a&gt;, &lt;a href="https://www.derechosdigitales.org/"&gt;Derechos Digitales&lt;/a&gt;, &lt;a href="https://wikimedia.org.uk/wiki/Open_Coalition_Project_Co-ordinator"&gt;Open Coalition&lt;/a&gt;, &lt;a href="https://okfn.org/"&gt;Open Knowledge&lt;/a&gt;, &lt;a href="http://www.righttoresearch.org/"&gt;The Right to Research Coalition&lt;/a&gt;, &lt;a href="https://openmedia.org/"&gt;Open Media&lt;/a&gt;, &lt;a href="https://www.fightforthefuture.org/"&gt;Fight for the Future&lt;/a&gt;, &lt;a href="https://www.usenix.org/"&gt;USENIX&lt;/a&gt;, &lt;a href="https://www.publicknowledge.org/"&gt;Public Knowledge&lt;/a&gt; and &lt;a href="https://act.eff.org/action/unamonos-para-promover-el-acceso-abierto-en-todo-el-mundo"&gt;all individuals that have supported the campaign&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If open access was the default choice for publishing scientific research results, the impact of these results would increase and cases like mine would not exist. There would be no doubt that the right thing is to circulate this knowledge, so that it should serve everyone.&lt;/p&gt;
&lt;p&gt;Thank you all for your support.
Diego A. Gómez Hoyos&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The second document we’re sharing today is the &lt;a href="https://archive.org/details/GuerillaOpenAccessManifesto"&gt;Guerilla Open Access Manifesto&lt;/a&gt;, written by the late Aaron Swartz in 2008:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Information is power. But like all power, there are those who want to keep it for themselves. The world's entire scientific and cultural heritage, published over centuries in books and journals, is increasingly being digitized and locked up by a handful of private corporations. Want to read the papers featuring the most famous results of the sciences? You'll need to send enormous amounts to publishers like Reed Elsevier. &lt;/p&gt;
&lt;p&gt;There are those struggling to change this. The Open Access Movement has fought valiantly to ensure that scientists do not sign their copyrights away but instead ensure their work is published on the Internet, under terms that allow anyone to access it. But even under the best scenarios, their work will only apply to things published in the future. Everything up until now will have been lost. &lt;/p&gt;
&lt;p&gt;That is too high a price to pay. Forcing academics to pay money to read the work of their colleagues? Scanning entire libraries but only allowing the folks at Google to read them? Providing scientific articles to those at elite universities in the First World, but not to
children in the Global South? It's outrageous and unacceptable. &lt;/p&gt;
&lt;p&gt;"I agree," many say, "but what can we do? The companies hold the copyrights, they make enormous amounts of money by charging for access, and it's perfectly legal — there's nothing we can do to stop them." But there is something we can, something that's already being done: we can fight back. &lt;/p&gt;
&lt;p&gt;Those with access to these resources — students, librarians, scientists — you have been given a privilege. You get to feed at this banquet of knowledge while the rest of the world is locked out. But you need not — indeed, morally, you cannot — keep this privilege for yourselves. You have a duty to share it with the world. And you have: trading passwords with colleagues, filling download requests for friends. &lt;/p&gt;
&lt;p&gt;Meanwhile, those who have been locked out are not standing idly by. You have been sneaking through holes and climbing over fences, liberating the information locked up by the publishers and sharing them with your friends. &lt;/p&gt;
&lt;p&gt;But all of this action goes on in the dark, hidden underground. It's called stealing or piracy, as if sharing a wealth of knowledge were the moral equivalent of plundering a ship and murdering its crew. But sharing isn't immoral — it's a moral imperative. Only those blinded by greed would refuse to let a friend make a copy. &lt;/p&gt;
&lt;p&gt;Large corporations, of course, are blinded by greed. The laws under which they operate require it — their shareholders would revolt at anything less. And the politicians they have bought off back them, passing laws giving them the exclusive power to decide who can make copies. &lt;/p&gt;
&lt;p&gt;There is no justice in following unjust laws. It's time to come into the light and, in the grand tradition of civil disobedience, declare our opposition to this private theft of public culture. &lt;/p&gt;
&lt;p&gt;We need to take information, wherever it is stored, make our copies and share them with the world. We need to take stuff that's out of copyright and add it to the archive. We need to buy secret databases and put them on the Web. We need to download scientific journals and upload them to file sharing networks. We need to fight for Guerilla Open Access. &lt;/p&gt;
&lt;p&gt;With enough of us, around the world, we'll not just send a strong message opposing the privatization of knowledge — we'll make it a thing of the past. Will you join us? &lt;/p&gt;
&lt;p&gt;Aaron Swartz 
July 2008, Eremo, Italy&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the past few years, the open access movement has gained momentum as the benefits it provides to individual researchers and to the scientific community as a whole have started to manifest.  But even if open access lacked these benefits, we would still be morally obligated to advocate for it, because &lt;a href="http://www.un.org/en/documents/udhr/index.shtml#a27"&gt;access to knowledge is a human right&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;To learn more about open access efforts, visit &lt;a href="http://www.openaccessweek.org/"&gt;the Open Access Week website&lt;/a&gt;.&lt;/p&gt;</summary></entry><entry><title>Reexamining Reviewer Anonymity - More Costs than Benefits</title><link href="http://osc.centerforopenscience.org/2014/10/22/reexamining-reviewer-anonymity/" rel="alternate"></link><updated>2014-10-22T12:00:00-04:00</updated><author><name>Aaron Goetz</name></author><id>tag:osc.centerforopenscience.org,2014-10-22:2014/10/22/reexamining-reviewer-anonymity/</id><summary type="html">&lt;p&gt;Academic publishing dogma holds that peer reviewers (aka referees) should be anonymous. In the vast majority of cases, however, there are more costs than benefits to reviewer anonymity. Here, I make the case that reviewer identity and written reviews themselves should become publicly accessible information. Until then, reviewers should sign their reviews, as this practice can increase rigor, expose biases, encourage goodwill, and could serve as an honest signal of review quality and integrity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why reviewer anonymity solves nothing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The story goes that anonymity frees the reviewer from any reputational costs associated with providing a negative review. Without the cloak of invisibility, reviewers who provided devastating critiques would then become the target of attacks from those debased authors. Vengeful authors could sabotage the reviewer’s ability to get publications, grants, and tenure.&lt;/p&gt;
&lt;p&gt;It’s imaginable that these vengeful authors who have the clout to sabotage another’s career &lt;em&gt;might&lt;/em&gt; exist, but I’m willing to bet that few careers have been injured or sidelined due primarily to a bullying senior scientist. It’s difficult to say whether the absence of these horror stories is due to a lack of vengeful saboteurs or a lack of named reviewers. If you’re aware of rumored or confirmed instances of a scorned author who exacted revenge, please let me know in the comments section below.&lt;/p&gt;
&lt;p&gt;Let’s appreciate that our default is to be onymous&lt;sup&gt;1&lt;/sup&gt;. Without hiding behind anonymity, we manage to navigate our careers, which often includes being critical and negative. We openly criticize others’ work in commentaries, at conferences, in post-publication reviews, and on Facebook and Twitter. Editorships are not the kiss of death, even though their names appear at the bottom of rejection letters. Sure, editors typically have tenure and so you might think that there are no costs to their onymous criticism. But they also still attempt to publish and get grant funding, and their criticism, in the form of rejection letters, probably doesn’t hinder this. Moreover, for every enemy you make by publicly criticizing their work, in the form of post-publication reviews for example, you probably recruit an ally. Can’t newfound allies influence your ability to get publications, grants, and tenure just as much as adversaries?&lt;/p&gt;
&lt;p&gt;JP de Ruiter, who wrote an excellent &lt;a href="http://osc.centerforopenscience.org/2014/05/15/anonymous-peer-review/"&gt;piece&lt;/a&gt; also questioning anonymous peer review, offered a simple workaround to the problem of the fearful young scientist criticizing the senior scientist: “Reviewers with tenure always sign their reviews.” This is great, but my fear is that most reviews are written by untenured scientists, so the problems associated with reviewer anonymity will remain with this rule in place. My advice to the untenured and those on, or soon to be on, the job market would be the same: sign all reviews. Even negative reviews that recommend rejection should be signed. Needless to say, negative reviews need to be written very carefully. Drawing attention to flaws, fatal or otherwise, can be done with tact. Speaking tentatively will take the sting out of any criticism. In the review that the author (or public) sees, you can suggest a more appropriate analysis or alterative explanation, but in the private comments to the editor, you can emphasize how devastating these shortcomings are. Keep in mind that reviewers do not need to communicate their recommendation (i.e., accept, revise, reject) in the review that the authors see. In fact, most editors prefer the recommendation be kept separate from the review. This allows them more freedom with their decision. Also, newly minted PhDs and postdocs should keep in mind that there are practices and laws in place so that a scorned search committee member cannot make a unilateral decision.&lt;/p&gt;
&lt;p&gt;A second worry is that, without anonymity, reviewers would have to worry about being critical of a known colleague’s (and sometimes a friend’s) work. With anonymity, they’re free to criticize any manuscript and maintain favorable relationships with the authors. But if you’re worried about hurting a colleague’s feelings by delivering an honest and critical review, then you shouldn’t be a reviewer. Recuse yourself. Or maybe you shouldn’t infantilize your colleagues. They’ve probably learned to keep personal and professional relationships separate, and they would surely prefer an honest and constructive review, even if it was accompanied by some short-lived emotional pangs.&lt;/p&gt;
&lt;p&gt;A third worry about reviewer transparency might be that it could produce credulous or timid reviews. I don’t see this as a serious threat. Even in the light of onymity, reviewers will still demand good evidence. Identified reviewers will still provide the occasional dismissive, sarcastic, and insulting review. I’m certain of this because of my history of providing brutal, onymous reviews and because of those few that I’ve received. Even with my name signed at the bottom, I’ve written some things that I would find difficult to say to the author’s face. I’m not inappropriate, but I can be frank.  &lt;/p&gt;
&lt;p&gt;Moreover, the concern that identifying reviewers will lead to overly effusive reviews is alleviated when we appreciate the reputational costs associated with providing uncritical reviews. No one wants their name in the acknowledgements of a worthless paper.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Five benefits of reviewer transparency&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1) Encourage goodwill.&lt;/strong&gt; Obviously, reviewer transparency can curb misbehavior. We’re all well aware that it’s easy to be nasty when anonymity reduces the associated risks. The vileness of many YouTube comments is an obvious example. de Ruiter argues that anonymous peer review not only has the unintended consequence of removing good science from the literature, but it also removes good scientists. I can attest to this, too. One of my former graduate students, having just gone through the peer review process, questioned his future in academia. He expressed that he didn’t want the fate of his career to hinge on the whims of three random people who are loaded with biases and can behave badly without consequence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) Get credit.&lt;/strong&gt; Currently, we don't get credit for reviewing. If it's not related to your research, it's not worth your time to write a review, let alone write a high-quality review. "Opportunity costs!" screams the economist. But if we make reviews archivable, then we can receive credit, and we should be more likely to review. Some retention, tenure, and promotion committees would likely count these archived reviews as forms of scholarship and productivity. Altmetrics—quantitative measures of a researcher’s  impact, other than journal impact factor—are becoming more and more popular, and unless journal impact factors are completely revamped, which is unlikely to happen anytime soon, we’ll all be hearing a lot more about altmetrics in the future. Digital-born journals are in a good position to overhaul the peer review process to make it transparent and archivable. &lt;em&gt;F1000Research&lt;/em&gt; and &lt;em&gt;PeerJ&lt;/em&gt;, for example, have laudable open peer review models.&lt;/p&gt;
&lt;p&gt;The flip side of this “getting credit” benefit is that we’ll be able to see who’s free-riding. In a correspondence &lt;a href="http://www.nature.com/nature/journal/v505/n7484/full/505483a.html"&gt;piece&lt;/a&gt; in &lt;em&gt;Nature&lt;/em&gt;, Dan Graur argued that those scientists who publish the most are least likely to serve as reviewers. “The biggest consumers of peer review seem to contribute the least to the process,” he wrote. This inverse correlation was not found, however, in a &lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0092896"&gt;proper analysis&lt;/a&gt; of four ecology journals over an 8-year period, but the ratio of researchers’ reviews to submissions could be journal or discipline specific. Bottom line: free-riding could be a problem, and reviewer onymity could help to reduce it. &lt;/p&gt;
&lt;p&gt;A journal’s prestige comes primarily from the quality of papers it publishes. And the quality of papers rests largely on the shoulders of the editors and peer reviewers. It follows, then, that the prestige of a journal is owed to its editors and reviewers. Editors get acknowledged. Their names are easily found in the colophon of a print journal and on the journal’s website, but not so for the reviewers’ names. Some journals publish an annual acknowledgment of manuscript reviewers, but because it’s divorced from any content—e.g., you don’t know who reviewed what and how often—it’s largely worthless and probably ignored. Given that the dissemination (and progress?) of science depends on free labor provided by reviewers, they should get credit for doing it. Admittedly, this would introduce complexities, such as including the recommendations of the reviewers. I’d appreciate if I were acknowledged as a reviewer in each paper I review, but only if my recommendation accompanied my name: “Aaron Goetz recommended rejection.” A reviewer’s name, without her accompanying recommendation, in the acknowledgements of a published paper would look like an endorsement, and I know I’m not the only one to recommend rejection to a paper that was subsequently published. Even better, it would not be difficult to link associated reviews to the paper. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) Accountability&lt;/strong&gt;. You’ve surely opened up the pages of your discipline’s flagship journal, saw a laughable article, and wondered who let this nonsense get published. Picking the low-hanging fruit, who reviewed Bem’s precognition &lt;a href="http://psycnet.apa.org/journals/psp/100/3/407/"&gt;paper&lt;/a&gt;?  Some reviewers of that paper, not &lt;a href="http://osc.centerforopenscience.org/2014/06/25/a-skeptics-review/"&gt;Eric-Jan Wagenmakers&lt;/a&gt;, should be held accountable for wasting researcher resources. Try not to calculate how much time and effort was spent on &lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0033423"&gt;these&lt;/a&gt; &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2001721"&gt;projects&lt;/a&gt; that set the record straight. &lt;/p&gt;
&lt;p&gt;Another benefit that comes from shining light on reviewers would be the ability to recognize unsuitable reviewers and conflicts of interest. I bet a nontrivial number of people have reviewed their former students’ or former advisor’s work. I also have a hunch that a few topics within psychology owe their existence to a small network of researchers who are continually selected as the reviewers for these papers. As a hypothetical example, wouldn’t it be important to know that the majority of work on &lt;a href="http://psycnet.apa.org/psycinfo/2003-00369-002"&gt;terror management theory&lt;/a&gt; was reviewed by Greenberg, Solomon, Pyszczynski, and their students? Although I don’t think that G, S, P, and their students conducted the majority of reviews of the hundreds of terror management papers, I am highly skeptical of TMT for &lt;a href="http://www.tandfonline.com/doi/abs/10.1080/10478400701366969#.U3ugJtJdVAI"&gt;theoretical&lt;/a&gt; &lt;a href="http://www.epjournal.net/wp-content/uploads/ep03297325.pdf"&gt;reasons&lt;/a&gt;. But I digress.&lt;/p&gt;
&lt;p&gt;Some colleagues have confessed that, when reviewing a manuscript that has the potential to steal their thunder or undermine their work, they were more critical, were more likely to recommend rejection, and took significantly longer to return their review. This is toxic and is “damaging science” in de Ruiter’s words. &lt;/p&gt;
&lt;p&gt;And for those senior researchers who delegate reviews to graduate students, onymity could alleviate the associated bad practices. Senior researchers will either be forced to write their own reviews or engage in more pedagogy so that their students’ reviews meet basic standards of quality.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4) Clarification&lt;/strong&gt;. Authors would be able to ask reviewers to clarify their comments and suggestions, even if official correspondence between the two is severed due to the manuscript’s rejection. I’ve certainly received comments that I didn’t know quite what to do with. I once got “The authors should consider whether these perceptual shifts are commercial.” Huh? Commercial? Of course, a potential danger is that authors and reviewers could open a back-channel dialog that excludes the editor. I imagine that some editors will read &lt;em&gt;potential danger&lt;/em&gt;, while some will read &lt;em&gt;potential benefit&lt;/em&gt;. If you’re the former, an explicit “Authors and reviewers should refrain from communicating with one another about the manuscript throughout the review process” would probably do the trick.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5) Increased quality&lt;/strong&gt;. This is the primary benefit of review transparency. I know that I’m not the only reviewer who has, at some point, written a hasty or careless review. Currently, there are no costs &lt;em&gt;to reviewers&lt;/em&gt; who provide careless or poor-quality reviews, but there are serious costs associated with careless reviews, the primary being impeding scientific progress and wasting researcher resources. If we tie reputational consequences to reviews, then review quality increases. This practice might also increase review time, but that’s a cost we should be willing to incur to increase quality and accountability, expose biases, give credit where it’s due, and encourage goodwill.&lt;/p&gt;
&lt;p&gt;There’s actually some empirical &lt;a href="http://bjp.rcpsych.org/content/176/1/47.long"&gt;evidence&lt;/a&gt; suggesting that signed reviews are of higher quality than unsigned reviews. Reviewers for the &lt;em&gt;British Journal of Psychiatry&lt;/em&gt; were randomly assigned to signed and unsigned groups and provided real reviews for manuscripts, per business as usual. The researchers then measured the quality of reviews and compared them. By most measures, review quality was modestly but statistically better among the signed reviews. These data, however, aren’t as relevant to my argument, because reviewer identity was only revealed to &lt;em&gt;authors of the manuscripts&lt;/em&gt; rather than the entire scientific community. Any differences noted between the signed and unsigned groups are likely conservative estimates of what would happen if reviewers’ names and recommendations were publically attached to papers where reputational costs and benefits could be incurred. Another &lt;a href="http://www.bmj.com/content/318/7175/23"&gt;study&lt;/a&gt; also examining the effect of reviewer anonymity on review quality did not find any differences between the signed and unsigned groups, but this study suffers from the same limitation as the first: reviewer identity was only revealed to the authors of the manuscripts and did not appear in the subsequent publishing of accepted manuscripts.&lt;/p&gt;
&lt;p&gt;Signed reviews could become what evolutionary scientists call &lt;em&gt;honest signals&lt;/em&gt;. Honest signals—sometimes referred to as hard-to-fake signals, costly signals, or Zahavian signals—refer to traits or behaviors that are metabolically and energetically costly or dangerous to produce, maintain, or express. We all know the peacock’s tail as the standard example. A peacock’s tail honestly signals low parasite load. Only healthy, high quality males can afford to produce large, bright tail feathers. And many of us learned that the &lt;a href="https://www.google.com/search?q=stotting&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ei=KNp7U4i9J473oATqyoG4Cg&amp;amp;ved=0CAkQ_AUoAg&amp;amp;biw=1014&amp;amp;bih=608"&gt;stotting&lt;/a&gt; of many antelope species is best understood as an honest signal of quality.&lt;/p&gt;
&lt;p&gt;Much in same way that large, bright tail feathers honestly signal health, signed reviews can honestly signal review quality and integrity. Only a reviewer who writes a high quality review and is confident that the review is high quality can afford to sign her name at the bottom of her review. And only a reviewer who is confident that her critical review is fair and warranted can afford sign her name.&lt;/p&gt;
&lt;p&gt;It’s easy to write a subpar review; it probably happens every day. It’s not easy, however, to write a subpar review if your name is attached to it. Our desire to maintain our reputation is strong. To illustrate this, I challenge you to tweet or update your Facebook status to this: “CBS is hands down the best network. I could watch it all day.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I recently reread an onymous review I received from a colleague I deeply respect. His review did not pull any punches, and parts of it would probably be considered abrasive by those who haven’t developed a thick skin. When I received it, I recognized that to give my paper such a review—to dig up those obscure references, to run those analyses, to identify all of the strengths and weaknesses, and to entertain those alternative explanations—he had to get intimate with it. He had to let it marinade in his thoughts before he savored each paragraph, citation, and analysis. Although he ultimately devoured it, it was deeply rewarding to have someone else care that much about my work. And it fills me with gratitude to know who it was that give up their weekend, their time on their own work, or their time with their friends and family. Anything that increases rigor, exposes biases, aids scientific progress, and promotes gratitude and goodwill should at least be considered. And beyond mere consideration, journals should think seriously about examining the differences between signed and unsigned reviews and between public and private reviews. Editors have all they need to examine the differences between signed and unsigned reviews, and editors open to testing an open reviewer system that links reviews to published papers can contact me and others at the Open Science Collaboration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Footnotes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Yes, you’re recognizing onymous as the back-formation of anonymous. Onymous is synonymous with named, identified, or signed. 
&lt;sup&gt;2&lt;/sup&gt; JP de Ruiter, whom I mention few times throughout, wrote a piece that argued the same basic point that I’m trying to here: anonymous peer review is toxic and it should be fixed. Andrew Sabisky alerted me to de Ruiter’s &lt;a href="http://osc.centerforopenscience.org/2014/05/15/anonymous-peer-review/"&gt;post&lt;/a&gt;, and it inspired me to finish writing this piece. Many thanks to both. I encourage you to read de Ruiter’s post. Also, Kristy MacLeod wrote a great &lt;a href="http://kjmacleod.wordpress.com/2014/01/11/i-signed-my-review-and-heres-why/"&gt;post&lt;/a&gt; about her decision to sign a review, and her decision to sign seems to be rooted in honest signaling. I recommend you read it, too.
&lt;sup&gt;3&lt;/sup&gt; Geoffrey Miller wrote and signed the review I referenced in the last paragraph.
&lt;sup&gt;4&lt;/sup&gt; Special thanks go to Jessica Ayers, Kayla Causey, JP de Ruiter, Jon Grahe, and Gorge Romero who gave comments on an earlier draft of this post. All errors are my own, of course.&lt;/p&gt;</summary></entry><entry><title>A Psi Test for the Health of Science</title><link href="http://osc.centerforopenscience.org/2014/10/14/health-of-science/" rel="alternate"></link><updated>2014-10-14T11:00:00-04:00</updated><author><name>Alex Holcombe</name></author><id>tag:osc.centerforopenscience.org,2014-10-14:2014/10/14/health-of-science/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Science is sick. How will we know when it's been cured?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Meta-analysis quantitatively combines the evidence from multiple experiments, across different papers and laboratories. It's the best way we have to determine the upshot of a spate of studies.&lt;/p&gt;
&lt;p&gt;Published studies of psi (telepathy, psychokinesis, and other parapsychological phenomena) have been submitted to meta-analysis. The verdict of these meta-analyses is that the evidence for the existence of psi is close to overwhelming. Bosch, Steinkamp, &amp;amp; Boller (2006, &lt;em&gt;Psychological Bulletin&lt;/em&gt;), for example, meta-analyzed studies of the ability of participants to affect the output of random number generators. These experiments stemmed from an older tradition in which participants attempted to influence a throw of dice to yield a particular target number. As for the old dice experiments, many of the studies found that the number spat out by the random number generator was more often the target number that the participant was gunning for than one would expect by chance. In their heroic effort, Bosch et al. combined the results of 380 published experiments, and calculated that if in fact psychokinesis does not exist, the probability of finding the evidence published was less than one in a thousand (for one of their measures, &lt;em&gt;z&lt;/em&gt; = 3.67). In other words, it is extremely unlikely that so much evidence in favor of psychokinesis would have resulted if psychokinesis actually does not exist.&lt;/p&gt;
&lt;p&gt;Like many others, I suspect that this evidence stems not from the existence of psi, but rather from various biases in the way science today is typically conducted. &lt;/p&gt;
&lt;p&gt;"Publication bias" refers to the tendency for a study to be published if it is interesting, while boring results rarely make it out of the lab. "P-hacking" - equally insidious - is the tendency of scientists to try many different statistical analyses until they find a statistically significant result. If you try enough analyses or tests, you're nearly guaranteed to find a statistically significant although spurious result. But despite scientists' suspicion that the seemingly-overwhelming evidence for psi is a result of publication bias and p-hacking, there is no way to prove this, or to establish it beyond a reasonable doubt (we shouldn't expect &lt;em&gt;proof&lt;/em&gt;, as that may be a higher standard than is feasible for empirical studies of a probabilistic phenomenon).&lt;/p&gt;
&lt;p&gt;Fortunately these issues have received plenty of attention, and new measures are being adopted (albeit slowly) to address them. Researchers &lt;a href="http://www.psychologicalscience.org/index.php/publications/journals/psychological_science/badges"&gt;have been encouraged&lt;/a&gt; to publicly announce (simply by posting on a website) a single, specific statistical analysis plan prior to collecting data. This can eliminate p-hacking. Other positive steps, like sharing of code and data, helps other scientists to evaluate the evidence more deeply, to spot signs of p-hacking as well as inappropriate analyses and simple errors. In the case of a recent study of psi by Tressoldi et al., Sam Schwartzkopf has been able to &lt;a href="http://neuro.plos.org/2014/10/01/nothing-spooky-about-decoding-telepathy-a-lesson-in-the-value-of-open-science/"&gt;wade into the arcane details of the study&lt;/a&gt;, revealing possible problems. But even if the Tressoldi et al. study is shown to be seriously flawed, Sam's efforts won't overturn all the previous evidence for psi, nor will it combat publication bias in future studies. We need a combination of measures to address the maladies that afflict science.&lt;/p&gt;
&lt;p&gt;OK, so let's say that preregistration, open science, and other measures are implemented, and together fully remedy the unhealthy traditions that hold back efforts to know the truth. How will we know science has been cured? &lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Psi Test&lt;/strong&gt; for the health of science might be the answer. According to the Psi Test, until it can be concluded that psi does not exist using the same meta-analysis standards as are applied to any other phenomenon in the biomedical or psychological literature, science has not yet been cured. &lt;/p&gt;
&lt;p&gt;Do we really need to eliminate publication bias to pass the Psi Test, or can meta-analyses deal with it? &lt;a href="http://en.wikipedia.org/wiki/Funnel_plot"&gt;Funnel plots&lt;/a&gt; can provide evidence for publication bias. But given that most areas of science are rife with publication bias, if we use publication bias to overturn the evidence for psi, to be consistent we'd end up disbelieving countless more-legitimate phenomena. And my reading of medicine’s standard &lt;a href="http://handbook.cochrane.org/"&gt;meta-analysis guide&lt;/a&gt;, by the Cochrane Collaboration, is that in Cochrane reviews, evidence for publication bias raises concerns but is not used to overturn the verdict indicated by the evidence.&lt;/p&gt;
&lt;p&gt;Of course, instead of concluding that science is sick, we might instead conclude that psi actually exists. But I think this is not the case - mainly because of what I hear from physicists. And I think if psi did exist, there’d likely be even &lt;em&gt;more&lt;/em&gt; overwhelming evidence for it by now than we have. Still, I want us to be able to dismiss psi using the same meta-analysis techniques we use for the run-of-the-mill. Others have made &lt;a href="http://osc.centerforopenscience.org/2014/06/25/a-skeptics-review/"&gt;similar points&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Psi Test for the health of science, even if valid, won't tell us right away that science has been fixed. But in retrospect we’ll know. After the year science is cured, when taking psi studies published that year and after, applying the standard meta-analysis technique will result in the conclusion that psi does not exist.&lt;/p&gt;
&lt;p&gt;Below, I consider two objections to this Psi Test.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objection 1&lt;/strong&gt;: some say that we already can conclude that psi does not exist, based on Bayesian evaluation of the psi proposition. To evaluate the evidence from psi studies, a Bayesian first assigns a probability that psi exists, prior to seeing the studies' data. Most physicists and neuroscientists would say that our knowledge of how the brain works and of physical law very strongly suggests that psychokinesis is impossible. To overturn this Bayesian prior, one would need much stronger evidence than even the one-in-a-thousand chance derived from psi studies that I mentioned above. I agree; it's one reason I don't believe in psi. However, it's pretty hard to pin down in quantitative fashion and partially explains why Bayesian analysis hasn’t taken over the scientific literature more rapidly. Also, there may be expert physicists out there that think some sort of quantum interaction could underlie psi, and it's hard to know how to quantitatively combine the opinions of dissenters with the majority.  &lt;/p&gt;
&lt;p&gt;Rather than relying on a Bayesian argument (although Bayesian analysis is still useful, even with a neutral prior), I'd prefer that our future scientific practice, involving preregistration, unbiased publishing, replication protocols, and so on reach the point where if hundreds of experiments on a topic are available, they should be fairly definitive. Do you think we will get there?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objection 2&lt;/strong&gt;: Some will say that science can never eliminate publication bias. While publication bias is reduced by the advent of journals like PLoS ONE that accept null results, and by the growing number of journals that &lt;a href="https://osf.io/8mpji/wiki/home/"&gt;accept papers prior to the data being collected&lt;/a&gt;, it may forever remain a significant problem. But there are further steps one could take: in &lt;a href="http://en.wikipedia.org/wiki/Open_notebook_science"&gt;open notebook science&lt;/a&gt;, all data is posted on the net as soon as it is collected, eliminating all opportunity for publication bias. But open notebook science might never become standard practice, and publication bias may remain strong enough that substantial doubt will persist for many scientific issues. In that case, the only solution may be a pre-registered, confirmatory large-scale replication of an experiment, similar to what &lt;a href="http://www.psychologicalscience.org/index.php/replication"&gt;we are doing&lt;/a&gt; at &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt; (I'm an associate editor for the new Registered Replication Report article track). Will science always need that to pass the psi test?&lt;/p&gt;</summary></entry><entry><title>What Open Science Framework and Impactstory mean to these scientists' careers</title><link href="http://osc.centerforopenscience.org/2014/10/07/what-osf-and-impactstory-mean/" rel="alternate"></link><updated>2014-10-07T10:00:00-04:00</updated><author><name>Stacy Konkiel</name></author><id>tag:osc.centerforopenscience.org,2014-10-07:2014/10/07/what-osf-and-impactstory-mean/</id><summary type="html">&lt;p&gt;&lt;em&gt;This article was originally posted on &lt;a href="http://blog.impactstory.org/osf-winners-announced/"&gt;the Impactstory blog&lt;/a&gt;.&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;Yesterday, we announced three winners in the &lt;a href="http://cos.io"&gt;Center for Open Science&lt;/a&gt;’s random drawing to win a year’s subscription to Impactstory for users that connected their Impactstory profile to their Open Science Framework (OSF) profile: Leonardo Candela (&lt;a href="http://osf.io/4sxmh"&gt;OSF&lt;/a&gt;, &lt;a href="https://www.impactstory.org/LeonardoCandela"&gt;Impactstory&lt;/a&gt;), Rebecca Dore (&lt;a href="http://osf.io/wvtvz"&gt;OSF&lt;/a&gt;, &lt;a href="https://www.impactstory.org/RebeccaDore"&gt;Impactstory&lt;/a&gt;), and Calvin Lai (&lt;a href="http://osf.io/k5kdu"&gt;OSF&lt;/a&gt;, &lt;a href="https://www.impactstory.org/CalvinLai"&gt;Impactstory&lt;/a&gt;). Congrats, all!  &lt;/p&gt;
&lt;p&gt;We know our users would be interested to hear from other researchers practicing Open Science, especially how and why they use the tools they use. So, we emailed our winners who graciously agreed to share their experiences using the &lt;a href="https://osf.io/"&gt;OSF&lt;/a&gt; (a platform that supports project management with collaborators and project sharing with the public) and &lt;a href="http://impactstory.org"&gt;Impactstory&lt;/a&gt; (a webapp that helps researchers discover and share the impacts of all their research outputs). Read on!  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's your research focus?&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt;: I’m a computer science researcher. My research interests include Data Infrastructures, Virtual Research Environments, Data Publication, Open Science, Digital Library Management Systems and Architectures, Digital Libraries Models, Distributed Information Retrieval, and Grid and Cloud Computing.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rebecca&lt;/strong&gt;: I am a PhD student in Developmental Psychology. Broadly, my research focuses on children’s experiences in pretense, fiction and fantasy. How do children understand these experiences? How might these experiences affect children's behaviors, beliefs and abilities?  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Calvin&lt;/strong&gt;: I'm a doctoral student in Social Psychology studying how to change unconscious or automatic biases. In their most insidious forms, unconscious biases lead to discrepancies between what people value (e.g., egalitarianism) and how people act (e.g., discriminating based on race). My interest is in understanding how to change these unconscious thoughts so that they're aligned with our conscious values and behavior.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do you use the Open Science Framework in the course of your research?&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt;: Rather than an end user of the system for supporting my research tasks, I’m interested in analysing and comparing the facilities offered by such an environment and the concept of Virtual Research Environments.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rebecca&lt;/strong&gt;: At this stage, I use the OSF to keep all of the information about my various projects in one place and to easily make that information available to my collaborators--it is much more efficient to stay organized than constantly exchanging and keeping track of emails. I use the wiki feature to keep notes on what decisions were made and when and store files with drafts of materials and writing related to each project. Version control of everything is very convenient.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Calvin&lt;/strong&gt;: For me, the Open Science Framework (OSF) encompasses all aspects of the research process - from study inception to publication. I use the OSF as a staging ground in the early stages for plotting out potential study designs and analysis plans. I will then register my study shortly before data collection to gain the advantage of pre-registered confirmatory testing. After data collection, I will often refer back to the OSF as a reminder of what I did and as a guide for analyses and manuscript-writing. Finally, after publication, I use the OSF as a repository for public access to my data and study materials.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's your favorite Impactstory feature? Why?&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt;: I really appreciate the effort Impactstory is posing on collecting metrics on the impact my research products have on the web. I like its integration with ORCID and the recently supported “Key profile metrics” since it gives a nice overview of a researcher impact.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rebecca&lt;/strong&gt;: I had never heard of ImpactStory before this promotion, and it has been really neat to start testing out. It took me 2 minutes to copy my publication DOIs into the system, and I got really useful information that shows the reach of my work that I hadn't considered before, for example shares on Twitter and where the reach of each article falls relative to other psychology publications. I'm on the job market this year and can see this being potentially useful as supplementary information on my CV.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Calvin&lt;/strong&gt;: Citation metrics can only tell us so much about the reach of a particular publication. For me, Impactstory's alternative metrics have been important for figuring out where else my publications are having impact across the internet. It has been particularly valuable for pointing out connections that my research is making that I wasn't aware of before.  &lt;/p&gt;
&lt;p&gt;Thanks to all our users who participated in the drawing by connecting their OSF and Impactstory profiles! Both of our organizations are proud and excited to be working to support the needs of researchers practicing Open Science, and thereby changing science for the better.  &lt;/p&gt;
&lt;p&gt;To learn more about our open source non-profits, visit the &lt;a href="http://impactstory.org"&gt;Impactstory&lt;/a&gt; and &lt;a href="https://osf.io/"&gt;Open Science Framework&lt;/a&gt; websites.  &lt;/p&gt;</summary></entry><entry><title>The meaning of replicability across scientific disciplines</title><link href="http://osc.centerforopenscience.org/2014/09/09/meaning-of-replicability/" rel="alternate"></link><updated>2014-09-09T11:00:00-04:00</updated><author><name>Konrad Hinsen</name></author><id>tag:osc.centerforopenscience.org,2014-09-09:2014/09/09/meaning-of-replicability/</id><summary type="html">&lt;p&gt;&lt;a href="http://osc.centerforopenscience.org/2014/08/07/talk-about-replication/"&gt;Recently&lt;/a&gt;, Shauna Gordon-McKeon wrote about the meaning of replicability on this blog, concentrating on examples from psychology. In this post, I summarize for comparison the situation in computational science. These two fields may well be at opposite ends of the spectrum as far as replication and replicability are concerned, so the comparison should be of interest for establishing terminology that is also suitable for other domains of science. For a more detailed discussion of the issues specific to computational science, see &lt;a href="http://khinsen.wordpress.com/2014/08/27/reproducibility-replicability-and-the-two-layers-of-computational-science/"&gt;this post&lt;/a&gt; on my personal blog.&lt;/p&gt;
&lt;p&gt;The general steps in conducting a scientific study are the same in all fields:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Design: define in detail what needs to be done in order to obtain useful insight into a scientific problem. This includes a detailed description of required equipment, experimental samples, and procedures to be applied.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execution: do whatever the design requires to be done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interpretation: draw conclusions from whatever results were obtained.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The details of the execution phase vary enormously from one discipline to another. In psychology, the "experimental sample" is typically a group of volunteers, which need to be recruited, and the "equipment" includes the people interacting with the volunteers and the tools they use, but also the conditions in which the experiment takes place. In physics or chemistry, for which the terms "sample" and "equipment" are most appropriate, both are highly specific to an experiment and acquiring them (by buying or producing) is often the hard part of the work. In computational science, there are no samples at all, and once the procedure is sufficiently well defined, its execution is essentially left to a computer, which is a very standardized form of equipment. Of course what I have given here are caricatures, as reality is usually much more complicated. Even the three steps I have listed are hardly ever done one after the other, as problems discovered during execution lead to a revision of the design. But for explaining concepts and establishing terminology, such caricatures are actually quite useful.&lt;/p&gt;
&lt;p&gt;Broadly speaking, the term "replication" refers to taking an existing study design and repeating the execution phase. The motivation for doing this is mainly verification: the scientists who designed and executed the study initially may have made mistakes that went unnoticed, forgotten to mention an important aspect of their design in their publication, or at the extreme have cheated by making up or manipulating data.&lt;/p&gt;
&lt;p&gt;What varies enormously across scientific disciplines is the effort or cost associated with replication. A literal replication (as defined in Shauna's post) of a psychology experiment requires recruiting another group of volunteers, respecting their characteristics as defined by the original design, and investing a lot of researchers' time to repeat the experimental procedure. A literal replication of a computational study that was designed to be replicable involves minimal human effort and an amount of computer time that is in most cases not important. On the other hand, the benefit obtained from a literal replication varies as well. The more human intervention is involved in a replication, the more chances for human error there are, and the more important it is to verify the results. The variability of the “sample” is also important: repeating an experiment with human volunteers is likely to yield different outcomes even if done with exactly the same subjects, and similar problems apply in principle with other living subjects, even as small as bacteria. In contrast, re-running a computer program is much less useful, as it can only discover rare defects in computer hardware and system software.&lt;/p&gt;
&lt;p&gt;These differences lead to different attitudes toward replication. In psychology, as Shauna describes, literal replication is expensive and can detect only some kinds of potential problems, which are not necessarily expected to be the most frequent or important ones. This makes a less rigid approach, which Shauna calls "direct replication", more attractive: the initial design is not repeated literally, but in spirit. Details of the protocol are modified in a way that, according to the state of knowledge of the field, should not make a difference. This makes replication cheaper to implement (because the replicators can use materials and methods they are more familiar with), and covers a wider range of possible problems. On the other hand, when such an approach leads to results that are in contradiction with the original study, more work must be invested to figure out the cause of the difference.&lt;/p&gt;
&lt;p&gt;In computational science, literal replication is cheap but at first sight seems to yield almost no benefit. The point of &lt;a href="http://khinsen.wordpress.com/2014/08/27/reproducibility-replicability-and-the-two-layers-of-computational-science/"&gt;my original blog post&lt;/a&gt; was to show that this is not true: replication proves replicability, i.e. it proves that the published description of the study design is in fact sufficiently complete and detailed to make replication possible. To see why this is important, we have to look at the specificities of computation in science, and at the current habits that make most published studies impossible to replicate.&lt;/p&gt;
&lt;p&gt;A computational study consists essentially in running a sequence of computer programs, providing each one with the input data it requires, which is usually in part obtained from the output of programs run earlier. The order in which the programs are run is very important, and the amount of input data that must be provided is often large. Typically, changing the order of execution or a single number in the input data leads to different results that are not obviously wrong. It is therefore common that mistakes go unnoticed when individual computational steps require manual intervention. And that is still the rule rather than the exception in computational science. The most common cause for non-replicability is that the scientists do not keep a complete and accurate log of what they actually did, because keeping such a log is a very laborious, time-consuming, and completely uninteresting task. There is also a lack of standards and conventions for recording and publishing such a log, making the task quite difficult as well. For these reasons, replicable computational studies remain the exception to this day. There is of course no excuse for this: it’s a moral obligation for scientists to be as accurate as humanly and technologically possible about documenting their work. While today’s insufficient technology can be partly blamed, most computational scientists (myself included) could do much better than they do. It is really a case of bad habits that we have acquired as a community.&lt;/p&gt;
&lt;p&gt;The good news is that people are becoming aware of the problem (see for example &lt;a href="http://www.nature.com/news/2010/101013/full/467775a.html"&gt;this status report in Nature&lt;/a&gt;) and working on solutions. Early adopters report consistently that the additional initial effort for ensuring replicability quickly pays off over the duration of a study, even before it gets published. As with any new development, potential adopters are faced with a bewildering choice of technologies and recommended practices. I'll mention &lt;a href="http://www.activepapers.org/"&gt;my own technology&lt;/a&gt; in passing, which makes computations replicable by construction. More generally, interested readers might want to look at &lt;a href="http://www.crcpress.com/product/isbn/9781466561595"&gt;this book&lt;/a&gt;, a &lt;a href="https://www.coursera.org/course/repdata"&gt;Coursera course&lt;/a&gt;, two special issues of CiSE magazine (&lt;a href="http://www.computer.org/csdl/mags/cs/2009/01/index.html"&gt;January 2009&lt;/a&gt; and &lt;a href="http://www.computer.org/csdl/mags/cs/2012/04/index.html"&gt;July 2012&lt;/a&gt;), and a &lt;a href="http://groups.google.com/forum/#!forum/reproducible-research"&gt;discussion forum&lt;/a&gt; where you can ask questions.&lt;/p&gt;
&lt;p&gt;An interesting way to summarize the differences across disciplines concerning replication and reproducibility is to look at the major “sources of variation” in the execution phase of a scientific study. At one end of the spectrum, we have uncontrollable and even undescribable variation in the behavior of the sample or the equipment. This is an important problem in biology or psychology, i.e. disciplines studying phenomena that we do not yet understand very well. To a lesser degree, it exists in all experimental sciences, because we never have full control over our equipment or the environmental conditions. Nevertheless, in technically more mature disciplines studying simpler phenomena, e.g. physics or chemistry, one is more likely to blame human error for discrepancies between two measurements that are supposed to be identical. Replication of someone else's published results is therefore attempted only for spectacularly surprising findings (remember &lt;a href="https://en.wikipedia.org/wiki/Cold_fusion"&gt;cold fusion&lt;/a&gt;?), but in-house replication is very common when testing new scientific equipment. At the other end of the spectrum, there is the zero-variation situation of computational science, where study design uniquely determines the outcome, meaning that any difference showing up in a replication indicates a mistake, whose source can in principle be found and eliminated. Variation due to human intervention (e.g. in entering data) is considered a fault in the design, as a computational study should ideally not require any human intervention, and where it does, everything should be recorded.&lt;/p&gt;</summary></entry><entry><title>Call for Papers on Research Transparency</title><link href="http://osc.centerforopenscience.org/2014/08/22/call-for-papers-bitss/" rel="alternate"></link><updated>2014-08-22T12:30:00-04:00</updated><author><name>Alexander Wais</name></author><id>tag:osc.centerforopenscience.org,2014-08-22:2014/08/22/call-for-papers-bitss/</id><summary type="html">&lt;p&gt;The &lt;a href="http://bitss.org/"&gt;Berkeley Initiative for Transparency in the Social Sciences&lt;/a&gt; (BITSS) will be holding its 3rd annual conference at &lt;a href="http://www.berkeley.edu/index.html"&gt;UC Berkeley&lt;/a&gt; on &lt;strong&gt;December 11-12, 2014&lt;/strong&gt;. The goal of the meeting is to bring together leaders from academia, scholarly publishing, and policy which are committed to strengthening the standards of rigor across social science disciplines.&lt;/p&gt;
&lt;p&gt;A select number of papers elaborating new tools and strategies to increase the transparency of research will be presented and discussed. Topics for papers include, but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-registration and the use of pre-analysis plans;&lt;/li&gt;
&lt;li&gt;Disclosure and transparent reporting;&lt;/li&gt;
&lt;li&gt;Replicability and reproducibility;&lt;/li&gt;
&lt;li&gt;Data sharing;&lt;/li&gt;
&lt;li&gt;Methods for detecting and reducing publication bias or data mining.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Papers or long abstracts must be submitted by &lt;strong&gt;Friday, October 10th&lt;/strong&gt; (midnight Pacific time) through &lt;a href="https://cega.submittable.com/submit/33789"&gt;CEGA’s Submission Platform&lt;/a&gt;. Travel funds will be provided for presenters. Submission can be of completed papers or works in progress.&lt;/p&gt;
&lt;p&gt;The 2014 BITSS Conference is sponsored by the &lt;a href="http://www.sloan.org/"&gt;Alfred P. Sloan Foundation&lt;/a&gt; and the &lt;a href="http://www.arnoldfoundation.org/"&gt;Laura and John Arnold Foundation&lt;/a&gt;.&lt;/p&gt;</summary></entry><entry><title>What we talk about when we talk about replication</title><link href="http://osc.centerforopenscience.org/2014/08/07/talk-about-replication/" rel="alternate"></link><updated>2014-08-07T12:00:00-04:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-08-07:2014/08/07/talk-about-replication/</id><summary type="html">&lt;p&gt;If I said, “Researcher A replicated researcher B’s work”, what would you take me to mean?  &lt;/p&gt;
&lt;p&gt;There are many possible interpretations. I could mean that A had repeated precisely the methods of researcher B, and obtained similar results.  Or I could be saying that A had repeated precisely the methods of researcher B, and obtained very different results.  I could be saying that A had repeated only those methods which were theorized to influence the results.  I could mean that A had devised new methods which were meant to explore the same phenomenon.  Or I could mean that researcher B had copied everything down to the last detail.  &lt;/p&gt;
&lt;p&gt;We do have terms for these different interpretations.  A replication of precise methods is a direct replication, while a replication which uses new methods but gets at the same phenomenon is a conceptual replication.  Once a replication has been completed, you can look at the results and call it a “successful replication” if the results are the same, and a “failed replication” if the results are different.  &lt;/p&gt;
&lt;p&gt;Unfortunately, these terms are not always used, and the result is that recent debates over replication have become not only heated, but confused.  &lt;/p&gt;
&lt;p&gt;Take, for instance, nobel laureate Daniel Kahneman’s open letter to the scientific community, &lt;a href="http://www.scribd.com/doc/225285909/Kahneman-Commentary"&gt;A New Etiquette for Replication&lt;/a&gt;.  He writes:  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Even rumors of a failed replication cause immediate reputational damage by raising a suspicion of negligence (if not worse). The hypothesis that the failure is due to a flawed replication comes less readily to mind – except for authors and their supporters, who often feel wronged.” &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here he uses the common phrasing, “failed replication”, to indicate a replication where different results were obtained.  The cause of those different results is unknown, and he suggests that one option is that the methods used in the direct replication were not correct, which he calls a “flawed replication”.  What, then, is the term for a replication where the methods are known to be correct but different results were still found?&lt;/p&gt;
&lt;p&gt;Further on in his letter, Kahneman adds:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“In the myth of perfect science, the method section of a research report always includes enough detail to permit a direct replication. Unfortunately, this seemingly reasonable demand is rarely satisfied in psychology, because behavior is easily affected by seemingly irrelevant factors.”  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We take “direct replication” to mean copying the original researcher’s methods.  As Kahneman points out, perfect copying is impossible.  When a factor that once seemed irrelevant may have influenced the results, is that a “flawed replication”, or simply no longer a “direct replication”?  How can we distinguish between replications which copy as much of the methods as possible, and those which copy only those elements of the methods which the original author hypothesizes should influence the result?&lt;/p&gt;
&lt;p&gt;This terminology is not only imprecise, it differs from what others use.  In their &lt;a href="http://www.psycontent.com/content/311q281518161139/fulltext.html"&gt;Registered Reports: A Method to Increase the Credibility of Published Results&lt;/a&gt;, Brian Nosek and Daniel Lakens write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“There is no such thing as an exact replication. Any replication will differ in innumerable ways from the original. A direct replication is the attempt to duplicate the conditions and procedure that existing theory and evidence anticipate as necessary for obtaining the effect (Open Science Collaboration, 2012, 2013; Schmidt, 2009). Successful replication bolsters evidence that all of the sample, setting, and procedural differences presumed to be irrelevant are, in fact, irrelevant.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This statement contains an admirably clear definition of “direct replication”, which the authors use here to mean a replication copying only those elements of the methods considered relevant.  This is distinct from Kahneman’s usage of the term “direct replication”.  Kahneman, instead, may be conflating “direct replication” with “literal replication”, a much less common term meaning “the precise duplication of the specific design and results of a previous study” (&lt;a href="http://www.psychwiki.com/wiki/What_is_a_literal_replication%3F"&gt;Heiman, 2002&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Nosek and Lakens also use the term “successful replication” in a way which implies that not only were the results replicated, the methods were as well, as they take the replication’s success to be a commentary on the methods.  However, even “successful replications” may not successfully replicate methods, as pointed out by &lt;a href="http://www.spspblog.org/simone-schnall-on-her-experience-with-a-registered-replication-project/"&gt;Simone Schnall&lt;/a&gt; in her critique of the special issue edited by Nosek and Lakens:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Various errors in several of the replications (e.g., in the &lt;a href="http://www.psycontent.com/content/n657m04571w51j70/?p=5709f7ef6aaf4a4ea65b5433c116abbe&amp;amp;pi=1"&gt;“Many Labs”&lt;/a&gt; paper) became only apparent once original authors were allowed to give feedback. Errors were uncovered even for successfully replicated findings. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whether or not there were methodological errors in these particular cases, the possibility of such errors even when results are replicated remains a possibility, one which is elided by the terminology of “successful replication”.  This is not merely a point of semantics, as "successful replications" may be checked less carefully for methodological errors than “failed replications”.&lt;/p&gt;
&lt;p&gt;There are many other examples of researchers using replication terminology in ways that are not maximally clear.  So far I have only quoted from social psychologists.  When we attempt to speak across disciplines we face even greater potential for confusion.&lt;/p&gt;
&lt;p&gt;As such, I propose:&lt;/p&gt;
&lt;p&gt;1)  That we resurrect the term “literal replication”, meaning “the precise duplication of the specific design of a previous study” rather than overload the term “direct replication”.  Direct replication can then mean only the duplication of those methods deemed to be relevant.  Of course, a perfect literal replication is impossible, but using this terminology implies that duplication of as much of the previous study as possible is the goal.&lt;/p&gt;
&lt;p&gt;2)  That we retire the phrases “failed replication” and “successful replication”, which do not distinguish between procedure and results.  In their place, we can use “replication with different results” and “flawed replication” for the former, and “replication with similar results” and “sound replication” for the latter.&lt;/p&gt;
&lt;p&gt;Thus, a replication attempt where the goal was to precisely duplicate materials and where this was successfully done, but different results were found, would be a sound literal replication with different results.  An attempt only to duplicate elements of the design hypothesized to be relevant, leading to some methodological questions, yet where similar results were found, would be a flawed direct replication with similar results.&lt;/p&gt;
&lt;p&gt;These terms may seem unnecessarily wordy, and indeed may not always be needed, but I encourage everyone to use them when precision is important, for instance in published articles or in debates with those who disagree with you.  I know that from now on, when I hear someone use the bare term “replication”, I will ask, “What kind?”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to JP de Ruiter, Etienne LeBel, and Sheila Miguez for their feedback on this post.&lt;/em&gt;&lt;/p&gt;</summary></entry><entry><title>Open-source software for science</title><link href="http://osc.centerforopenscience.org/2014/07/30/open-source-software-for-science/" rel="alternate"></link><updated>2014-07-30T12:00:00-04:00</updated><author><name>Sebastiaan Mathôt</name></author><id>tag:osc.centerforopenscience.org,2014-07-30:2014/07/30/open-source-software-for-science/</id><summary type="html">&lt;p&gt;A little more than three years ago I started working on &lt;a href="http://osdoc.cogsci.nl/"&gt;OpenSesame&lt;/a&gt;, a free program for the easy development of experiments, mostly oriented at psychologists and neuroscientists. The first version of OpenSesame was the result of a weekend-long hacking sprint. By now, OpenSesame has grown into a substantial project, with a small team of core developers, tens of occasional contributors, and about 2500 active users.&lt;/p&gt;
&lt;p&gt;Because of my work on OpenSesame, I've become increasingly interested in open-source software in general. How is it used? Who makes it? Who is crazy enough to invest time in developing a program, only to give it away for free? Well ... quite a few people, because open source is everywhere. Browsers like Firefox and Chrome. Operating systems like Ubuntu and Android. Programming languages like Python and R. Media players like VLC. These are all examples of open-source programs that many people use on a daily basis.&lt;/p&gt;
&lt;p&gt;But what about specialized scientific software? More specifically: Which programs do experimental psychologists and neuroscientists use? Although this varies from person to person, a number of expensive, closed-source programs come to mind first: E-Prime, SPSS, MATLAB, Presentation, Brainvoyager, etc. &lt;em&gt;Le psychonomist moyen&lt;/em&gt; is not really into open source.&lt;/p&gt;
&lt;p&gt;In principle, there are open-source alternatives to all of the above programs. Think of &lt;a href="http://www.psychopy.org/"&gt;PsychoPy&lt;/a&gt;, &lt;a href="http://www.r-project.org/"&gt;R&lt;/a&gt;, &lt;a href="http://www.python.org/"&gt;Python&lt;/a&gt;, or &lt;a href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/"&gt;FSL&lt;/a&gt;. But I can imagine the frown on the reader's face: &lt;em&gt;Come on, really? These freebies are not nearly as good as 'the real thing', are they?&lt;/em&gt; But this, although true to some extent, merely raises another question: Why doesn't the scientific community invest more effort in the development of open-source alternatives? Why do we keep accepting inconvenient licenses (no SPSS license at home?), high costs ($995 for E-Prime 2 professional), and scripts written in proprietary languages that cannot easily be shared between labs. This last point has become particularly relevant with the recent focus on replication and transparency. How do you perform a direct replication of an experiment if you do not have the required software? And what does transparency even mean if we cannot run each other's scripts?&lt;/p&gt;
&lt;p&gt;Despite widespread skepticism, I suspect that most scientists feel that open source is ideologically preferable over proprietary scientific software. But open source suffers from an image problem. For example, a widely shared misconception is that open-source software is buggy, whereas proprietary software is solid and reliable. But even though quality is subjective--and due to cognitive dissonance strongly biased in favor of expensive software!--this belief is not consistent with reality: &lt;a href="http://www.coverity.com/library/pdf/coverity-scan-2011-open-source-integrity-report.pdf"&gt;Reports&lt;/a&gt; have shown that open-source software contains about half as many errors per line of code as proprietary software.&lt;/p&gt;
&lt;p&gt;Another misconception is that developing (in-house) open-source software is expensive and inefficient. This is essentially a prisoners dilemma. Of course, for an individual organization it is often more expensive to develop software than to purchase a commercial license. But what if scientific organizations would work together to develop the software that they all need: &lt;em&gt;You write this for me, I write this for you?&lt;/em&gt; Would open source still be inefficient then?&lt;/p&gt;
&lt;p&gt;Let's consider this by first comparing a few commercial packages: E-Prime, Presentation, and Inquisit. These are all programs for developing experiments. Yet the wheel has been re-invented for each program. All overlapping functionality has been re-designed and re-implemented anew, because vendors of proprietary software dislike few things as much as sharing code and ideas. (This is made painfully clear by numerous patent wars.) Now, let's compare a few open-source programs: Expyriment, OpenSesame, and PsychoPy. These too are all programs for developing experiments. And these too have overlapping functionality. But you can use these programs together. Moreover, they build on each other's functionality, because open-source licenses allow developers to modify and re-use each other's code. The point that I'm trying to make is not that open-source programs are better than their proprietary counterparts. Everyone can decide that for him or herself. The crucial point is that the development process of open-source software is collaborative and therefore efficient. Certainly in theory, but often in practice as well.&lt;/p&gt;
&lt;p&gt;So it is clear that open-source software has many advantages, also--maybe even especially so--for science. Therefore, development of open-source software should be encouraged. How could universities and other academic organizations contribute to this?&lt;/p&gt;
&lt;p&gt;A necessary first step is to acknowledge that software needs time to mature. There are plenty of young researchers, technically skilled and brimming with enthusiasm, who start a software project. Typically, this is software that they developed for their own research, and subsequently made freely available. If you are lucky, your boss allows this type of frivolous fun, as long the 'real' work doesn't suffer. And maybe you can even get a paper out of it, for example in &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;Journal of Neuroscience Methods&lt;/em&gt;, or &lt;em&gt;Frontiers in Neuroinformatics&lt;/em&gt;. But it is often forgotten that software needs to be maintained. Bugs need to be fixed. Changes in computers and operating systems require software updates. Unmaintained software spoils like an open carton of milk.&lt;/p&gt;
&lt;p&gt;And this is where things get awkward, because universities don't like maintenance. Developing new software is one thing. That's innovation, and somewhat resembles doing research. But maintaining software after the initial development stage is over is not interesting at all. You cannot write papers about maintenance, and maintenance does not make an attractive grant proposal. Therefore, a lot of software ends up 'abandonware', unmaintained ghost pages on development sites like GitHub, SourceForge, or Google Code.&lt;/p&gt;
&lt;p&gt;Ideally, universities would encourage maintenance of open-source scientific software. The message should be: &lt;em&gt;Once you start something, go through with it.&lt;/em&gt; They should recognize that the development of high-quality software requires stamina. This would be an attitude change, and would require that universities get over their publication fetish. Because the value of a program is not in the papers that have been written about it, but in the scientists that use it. Open-source scientific software has a very concrete and self-evident &lt;em&gt;impact&lt;/em&gt; for which developers should be rewarded. Without incentives, they won't make the high-quality software that we all need!&lt;/p&gt;
&lt;p&gt;In other words, developers could use a bit of encouragement and support, and this is currently lacking. I recently attended the APS convention, where I met Jeffrey Spies, one of the founders of the &lt;em&gt;&lt;a href="http://www.centerforopenscience.org/"&gt;Center for Open Science&lt;/a&gt; (COS)&lt;/em&gt;. As readers of this blog probably know, the COS is an American organization that (among many other things) facilitates development of open-source scientific software. They provide advice, support promising projects, and build networks. (Social, digital, and a mix of both, like this blog!) A related organization that focuses more specifically on software development is the &lt;em&gt;&lt;a href="http://www.mozillascience.org/"&gt;Mozilla Science Lab&lt;/a&gt; (MSL)&lt;/em&gt;. I think that the COS and MSL do great work, and provide models that could be adopted by other organizations. For example, I currently work for the &lt;a href="http://www.cnrs.fr/"&gt;CNRS&lt;/a&gt;, the French organization for fundamental research. The CNRS is very large, and could easily provide sustained support for the development of high-quality open-source projects. And the European Research Council could easily do so as well. However, these large research organization do not appear to recognize the importance of software development. They prefer to invest all of their budget in individual research projects, rather than invest &lt;em&gt;a small part&lt;/em&gt; of it in the development and maintenance of the software that these research projects need.&lt;/p&gt;
&lt;p&gt;In summary, a little systematic support would do wonders for the quality and availability of open-source scientific software. Investing in the future, is that not what science is about?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A Dutch version of this article initially appeared in De Psychonoom, the magazine of the Dutch psychonomic society. This article has been translated and updated for the OSC blog.&lt;/em&gt;&lt;/p&gt;</summary></entry><entry><title>Digging a little deeper - Understanding workflows of archaeologists</title><link href="http://osc.centerforopenscience.org/2014/07/16/digging-a-little-deeper/" rel="alternate"></link><updated>2014-07-16T11:00:00-04:00</updated><author><name>Sara Bowman, Mallory Kidwell, and Erin Braswell</name></author><id>tag:osc.centerforopenscience.org,2014-07-16:2014/07/16/digging-a-little-deeper/</id><summary type="html">&lt;p&gt;Scientific domains vary by the tools and instruments used, the way data are collected and managed, and even how results are analyzed and presented. As advocates of open science practices, it’s important that we understand the common obstacles to scientific workflow across many domains.  The COS team visits scientists in their labs and out in the field to discuss and experience their research processes first-hand. We experience the day-to-day of researchers and do our own investigating.  We find where data loss occurs, where there are inefficiencies in workflow, and what interferes with reproducibility.  These field trips inspire new tools and features for the &lt;a href="http://osf.io/"&gt;Open Science Framework&lt;/a&gt; to support openness and reproducibility across scientific domains.&lt;/p&gt;
&lt;p&gt;Last week, the team visited the &lt;a href="http://www.monticello.org/site/research-and-collections/monticello-archaeology"&gt;Monticello Department of Archaeology&lt;/a&gt; to dig a little deeper (bad pun) into the workflow of archaeologists, as well as learn about the &lt;a href="http://www.daacs.org/"&gt;Digital Archaeological Archive of Comparative Slavery&lt;/a&gt; (DAACS). Derek Wheeler, Research Archaeologist at Monticello, gave us a nice overview of how the Archaeology Department surveys land for artifacts. Shovel test pits, approximately 1 foot square, are dug every 40 feet on center as deep as anyone has dug in the past (i.e., down to undisturbed clay). If artifacts are found, the shovel test pits are dug every 20 feet on center.  At Monticello, artifacts are primarily man-made items like nails, bricks or pottery. The first 300 acres surveyed contained 12,000 shovel test pits -- and that’s just 10% of the total planned survey area. That’s a whole lot of holes, and even more data. &lt;/p&gt;
&lt;p&gt;&lt;img src="images/monticello_1.jpg" alt="Fraser Neiman addresses crowd" &gt;
&lt;em&gt;Fraser Neiman, Director of Archaeology at Monticello, describes the work being done to excavate on Mulberry Row - the industrial hub of Jefferson’s agricultural industry.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At the &lt;a href="http://www.monticello.org/site/research-and-collections/mulberry-row-reassessment"&gt;Mulberry Row&lt;/a&gt; excavation site, Fraser Neiman, Director of Archaeology, explained the meticulous and painstaking process of excavating &lt;a href="http://en.wikipedia.org/wiki/Quadrat"&gt;quadrats&lt;/a&gt;, small plots of land isolated for study. Within a quadrat, there exist contexts - stratigraphic units. Any artifacts found within a context are carefully recorded on a context sheet - what the artifact is, its location within the quadrat, along with information about the fill (dirt, clay, etc.) in the context. The fill itself is screened to pull out smaller artifacts the eye may not catch. All of the excavation and data collection at the Mulberry Row Reassessment is conducted following the standards of the Digital Archaeological Archive of Comparative Slavery (DAACS). Standards developed by DAACS help archaeologists in the Chesapeake region to generate, report, and compare data from 20 different sites across the region in a systematic way. Without these standards, archiving and comparing artifacts from different sites would be extremely difficult. &lt;/p&gt;
&lt;p&gt;&lt;img src="images/monticello_2.jpg" alt="Researchers measure excavation site" &gt;
&lt;em&gt;Researchers make careful measurements at the Monticello Mulberry Row excavation site, while recording data on a context sheet.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The artifacts, often &lt;a href="http://en.wikipedia.org/wiki/Sherd"&gt;sherds&lt;/a&gt;, are collected by context and taken to the lab for washing, labeling, analysis and storage. After washing, every sherd within a particular context is labeled with the same number and stored together. All of the data from the context sheets, as well as photos of the quadrants and sherds, are carefully input into DAACS following the standards set out in the DAACS &lt;a href="http://www.daacs.org/about-the-database/daacs-cataloging-manual/"&gt;Cataloging Manual&lt;/a&gt;. There is an enormous amount of manual labor associated with preparing and curating each artifact.  Jillian Galle, Project Manager of DAACS, described the extensive training users must undergo in order to deposit their data in the archive to ensure the standards outlined by the Cataloging Manual are kept. This regimented process ensures the quality and consistency of the data- and thus its utility.  The result is a publicly available dataset of the history of Monticello for researchers of all kinds to examine this important site in America’s history.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/monticello_3.jpg" alt="Washed and numbered sherds" &gt;
&lt;em&gt;These sherds have been washed and numbered to denote their context.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Our trip to Monticello Archaeology was eye-opening, as none of us had any practical experience with archaeological research or data. The impressive DAACS protocols and standards represent an important aspect of all scientific research - the ability to accurately capture large amounts of data in a systematic, thoughtful way - and then share it freely with others. &lt;/p&gt;</summary></entry><entry><title>What Jason Mitchell's 'On the emptiness of failed replications' gets right</title><link href="http://osc.centerforopenscience.org/2014/07/10/what-jason-mitchell-gets-right/" rel="alternate"></link><updated>2014-07-10T12:00:00-04:00</updated><author><name>Tom Stafford</name></author><id>tag:osc.centerforopenscience.org,2014-07-10:2014/07/10/what-jason-mitchell-gets-right/</id><summary type="html">&lt;p&gt;Jason Mitchell's essay &lt;a href="http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm"&gt;'On the emptiness of failed replications'&lt;/a&gt; is notable for being against the current effort to publish replication attempts. Commentary on the essay that I saw was pretty negative (e.g. &lt;a href="http://io9.com/i-was-in-an-undergrad-symposium-on-research-where-diffe-1601474824"&gt;"awe-inspiringly clueless"&lt;/a&gt;, &lt;a href="https://twitter.com/JustinWolfers/status/486551815815561216"&gt;“defensive pseudo-scientific, anti-Bayesian academic ass-covering”&lt;/a&gt;, &lt;a href="https://twitter.com/BenLillie/status/486198755385827329"&gt;"Do you get points in social psychology for publicly declaring you have no idea how science works?"&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Although I reject his premises, and disagree with his conclusion, I don't think Mitchell's arguments are incomprehensibly mad. This seems to put me in a minority, so I thought I'd try and explain the value in what he's saying. I'd like to walk through his essay assuming he is a thoughtful rational person. Why would a smart guy come to the views he has? What is he really trying to say, and what are his assumptions about the world of psychology that might, perhaps, illuminate our own assumptions?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Experiments as artefacts, not samples&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First off, key to Mitchell's argument is a view that experiments are complex artefacts, in the construction of which errors are very likely. Effects, in this view, are hard won, eventually teased out via a difficult process of refinement and validation. The value of replication is self-evident to anyone who thinks statistically: sampling error and publication bias will produce lots of false positives, you improve your estimate of the true effect by independent samples (= replications). Mitchell seems to be saying that the experiments are so complex that replications by other labs aren't independent samples of the same effect. Although they are called replications there are, he claims, most likely to be botched, and so informative of nothing more than the incompetence of the replicators.&lt;/p&gt;
&lt;p&gt;When teaching our students many of us will have deployed the saying "The plural of anecdote is not data". What we mean by this is that many weak observations - of ghosts, aliens or psychic powers - do not combine multiplicatively to make strong evidence in favour of these phenomena. If I've read him right, Mitchell is saying the same thing about replication experiments - many weak experiments are uninformative about real effects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tacit practical knowledge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Part of Mitchell's argument rests on the importance of tacit knowledge in running experiments (see his section "The problem with recipe-following"). We all know that tacit knowledge about experimental procedures exists in science. Mitchell puts a heavy weight on the importance of this. This is a position which presumably would have lots of sympathy from Daniel Kahneman, &lt;a href="http://www.scribd.com/doc/225285909/Kahneman-Commentary"&gt;who suggested that all replication attempts should involve the original authors&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There's a tension here between how science should be and how it is. Obviously our job is to make things explicit, to &lt;a href="http://psychsciencenotes.blogspot.co.uk/2014/05/psychologys-real-replication-problem.html"&gt;explain how to successfully run experiments so that anyone can run them&lt;/a&gt; but the truth is, full explanations aren't always possible. Sure, anyone can try and replicate based on a methods section, but - says Mitchell - you will probably be wasting your time generating noise rather than data, and shouldn't be allowed to commit this to the scientific record.&lt;/p&gt;
&lt;p&gt;Most of us would be comfortable with the idea that if a non-psychologist ran our experiments they might make some serious errors (one thinks of the hash some physical scientists made of psi-experiments, failing completely to account for things like demand effects, for example). Mitchell's line of thought here seems to take this one step further, you can't run a social psychologist's experiments without special training in social psychology. Or even, maybe, you can't successfully run another lab's experiment without training from that lab.&lt;/p&gt;
&lt;p&gt;I think happen to think he's wrong on this, and that he neglects to mention the harm of assuming that successful experiments have a &lt;a href="https://twitter.com/hpashler/status/486566352077848577"&gt;"special sauce"&lt;/a&gt; which cannot be easily communicated (it seems to be a road to elitism and mysticism to me, completely contrary to the goals science should have). Nonetheless, there's definitely some truth to the idea, and I think it is useful to consider the errors we will make if we assume the contrary, that methods sections are complete records and no special background is required to run experiments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Innuendo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mitchell makes the claim that targeting an effect for replication amounts to the innuendo that the effects under inspection are unreliable, which is a slur on the scientists who originally published them. Isn't this correct? Several people on twitter admitted, or tacitly admitted, that their prior beliefs were that many of these effects aren't real. There is something disingenuous about claiming, on the one hand, that all effects should be replicated, but, on the other, targeting particular effects for attention. If you bought Mitchell's view that experiments are delicate artefacts which render most replications uninformative, you can see how the result is a situation which isn't just uninformative but actively harmful to the hard-working psychologists whose work is impugned. Even if you don't buy that view, you might think that selection of which effects should be the focus of something like the Many Labs project is an active decision made by a small number of people, and which targets particular individuals. How this processes works out in practice deserves careful consideration, even if everyone agrees that it is a Good Thing overall.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are a number of issues in Mitchell's essay I haven't touched on - this isn't meant to be a complete treatment, just an explanation of some of the reasonable arguments I think he makes. Even if I disagree with them, I think they are reasonable; they aren't as obviously wrong as some have suggested and should be countered rather than dismissed.&lt;/p&gt;
&lt;p&gt;Stepping back, my take on the 'replication crisis' in psychology is that it really isn't about replication. Instead, this is what digital disruption looks like in a culture organised around scholarly kudos rather than profit. We now have the software tools to coordinate data collection, share methods and data, analyse data, and interact with non-psychologists, both directly and via the media, in unprecedented ways and at an unprecedented rate. Established scholarly communities are threatened as "the way things are done" is challenged. Witness John Bargh's &lt;a href="http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/"&gt;incredulous reaction to having his work challenged&lt;/a&gt; (and note that this was 'a replicate and explain via alternate mechanism' type study that Mitchell says is a valid way of doing replication). Witness the recent &lt;a href="http://www.bmj.com/content/348/bmj.g2228/rr/693104"&gt;complaint of medical researcher Jonathan S. Nguyen-Van-Tam&lt;/a&gt; when a journalist included critique of his analysis technique in a report on his work. These guys obviously believe in a set of rules concerning academic publishing which many of us aren't fully aware of or believe no longer apply.&lt;/p&gt;
&lt;p&gt;By looking at other disrupted industries, such as music or publishing, we can discern morals for both sides. Those who can see the value in the old way of doing things, like Mitchell, need to articulate that value and fast. There's no way of going back, but we need to salvage the good things about tight-knit, slow moving, scholarly communities. The moral for the progressives is that we shouldn't let the romance of change blind us to the way that the same old evils will reassert themselves in new forms, by hiding behind a &lt;a href="https://www.jacobinmag.com/2014/01/sharing-and-caring/"&gt;facade of being new, improved and more equitable&lt;/a&gt;.&lt;/p&gt;</summary></entry><entry><title>Response to Jason Mitchell’s “On the Emptiness of Failed Replications”</title><link href="http://osc.centerforopenscience.org/2014/07/09/response-to-jason-mitchell/" rel="alternate"></link><updated>2014-07-09T12:30:00-04:00</updated><author><name>Sean Mackinnon</name></author><id>tag:osc.centerforopenscience.org,2014-07-09:2014/07/09/response-to-jason-mitchell/</id><summary type="html">&lt;p&gt;Jason Mitchell recently wrote an article entitled &lt;a href="http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm"&gt;“On the Emptiness of Failed Replications.”&lt;/a&gt;. In this article, Dr. Mitchell takes an unconventional and extremely strong stance against replication, arguing that: “… studies that produce null results -- including preregistered studies -- should not be published.”  The crux of the argument seems to be that "scientists who get p &amp;gt; .05 are just incompetent." It completely ignores the possibility that a positive result could also (maybe even equally) be due to experimenter error. Dr. Mitchell also appears to ignore the possibility of simply getting a false positive (which is expected to happen under the null in 5% of cases).  &lt;/p&gt;
&lt;p&gt;More importantly, it ignores issues of effect size and treats the outcome of research as a dichotomous "success or fail.” The advantages of examining effect sizes over simple directional hypotheses using null hypothesis significance testing are beyond the scope of this short post, but you might check out &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3444174/"&gt;Sullivan and Feinn (2012)&lt;/a&gt; as an open-access starting point. Generally speaking, the problem is that sampling variation means that some experiments will find null results even when the experimenter does everything right. As an illustration, below is 1000 simulated correlations, assuming that r = .30 in the population, and a sample size of 100 (I used a &lt;a href="http://www.quantpsy.org/rci/rci.htm"&gt;monte carlo method&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src="images/response1.png" alt="" align="center" style="padding-right: 20px;" /&gt; &lt;/p&gt;
&lt;p&gt;In this picture, the units of analysis are individual correlations obtained in 1 of 1000 hypothetical research studies. The x-axis is the value of the correlation coefficient found, and the y-axis is the number of studies reporting that value. The red line is the critical value for significant results at p &amp;lt; .05 assuming a sample size of 100. As you can see from this picture, the majority of studies are supportive of an effect that is greater than zero. However (simply due to chance) all the studies to the left of the red line turned out non-significant. If we suppressed all the null results (i.e., all those unlucky scientists to the left of the red line) as Dr. Mitchell suggests, then our estimate of the effect size in the population would be inaccurate; specifically, it would appear to be larger than it really is, because certain aspects of random variation (i.e., null results) are being selectively suppressed. Without the minority of null findings (in addition to the majority of positive findings) the overall estimate of the effect cannot be correctly estimated. &lt;/p&gt;
&lt;p&gt;The situation is even more grim if there really is no effect in the population. &lt;/p&gt;
&lt;p&gt;&lt;img src="images/response2.png" alt="" align="center" style="padding-right: 20px;" /&gt; &lt;/p&gt;
&lt;p&gt;In this case, a small proportion of studies will produce false positives, with a roughly equal chance of an effect in either direction. If we fail to report null results, false positives may be reified as substantive effects. The reversal of signs across repeated studies might be a warning sign that the effect doesn’t really exist, but without replication, a single false positive could define a field if it happens (by chance) to be in line with prior theory.&lt;/p&gt;
&lt;p&gt;With this in mind, I also disagree that replications are “publicly impugning the scientific integrity of their colleagues.” Some people feel threatened or attacked by replication. The ideas we produce as scientists are close to our hearts, and we tend to get defensive when they’re challenged. If we focus on effect sizes, rather than the “success or fail” logic of null hypothesis significance testing, then I don’t believe that “failed” replications damage the integrity of the original author, but rather simply suggests that we should modulate the estimate of the effect size downwards. In this framework, replication is less about “proving someone wrong” and more about centering on the magnitude of an effect size. &lt;/p&gt;
&lt;p&gt;Something that is often missed in discussion of replication is that the very nature of randomness inherent in the statistical procedures scientists use means that any individual study (even if perfectly conducted) will probably generate an effect size that is a bit larger or smaller than it is in the population. It is only through repeated experiments that we are able to center on an accurate estimate of the effect size.  This issue is independent of researcher competence, and means that even the most competent researchers will come to the wrong conclusions occasionally because of the statistical procedures and cutoffs we’ve chosen to rely on. With this in mind, people should be aware that a failed replication does not necessarily mean that one of the two researchers is incorrect or incompetent – instead, it is assumed (until further evidence is collected) that the best estimate is a weighted average of the effect size from each research study.&lt;/p&gt;
&lt;p&gt;For some more commentary from other bloggers, you might check out the following links:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://tomsett.me.uk/are-replication-efforts-pointless/"&gt;"Are replication efforts pointless?"&lt;/a&gt; by &lt;a href="http://tomsett.me.uk/about/"&gt;Richard Tomsett&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://scientopia.org/blogs/drugmonkey/2014/07/07/being-as-wrong-as-can-be-on-the-so-called-replication-crisis-of-science/"&gt;"Being as wrong as can be on the so-called replication crisis of science"&lt;/a&gt; by drugmonkey at &lt;a href="http://scientopia.org/blogs/"&gt;Scientopia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://janneinosaka.blogspot.jp/2014/07/are-replication-efforts-useless.html"&gt;"Are replication efforts useless?"&lt;/a&gt; by &lt;a href="http://janneinosaka.blogspot.jp/"&gt;Jan Moren&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://filedrawer.wordpress.com/2014/07/07/jason-mitchells-essay/"&gt;"Jason Mitchell’s essay"&lt;/a&gt; by &lt;a href="http://filedrawer.wordpress.com/"&gt;Chris Said&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://neuroconscience.com/2014/07/08/methodswedontreport-brief-thought-on-jason-mitchell-versus-the-replicators/"&gt;"#MethodsWeDontReport – brief thought on Jason Mitchell versus the replicators"&lt;/a&gt; by &lt;a href="http://neuroconscience.com/micah-allen/"&gt;Micah Allen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://blogs.discovermagazine.com/neuroskeptic/2014/07/07/emptiness-failed-replications/#.U7wKgBYyxgp"&gt;"On 'On the emptiness of failed replications'"&lt;/a&gt; by &lt;a href="http://discovermagazine.com/authors?name=Neuroskeptic"&gt;Neuroskeptic&lt;/a&gt;&lt;/p&gt;</summary></entry><entry><title>phack - An R Function for Examining the Effects of P-hacking</title><link href="http://osc.centerforopenscience.org/2014/07/02/phack/" rel="alternate"></link><updated>2014-07-02T15:00:00-04:00</updated><author><name>Ryne Sherman</name></author><id>tag:osc.centerforopenscience.org,2014-07-02:2014/07/02/phack/</id><summary type="html">&lt;p&gt;&lt;em&gt;This article was &lt;a href="http://rynesherman.com/blog/phack-an-r-function-for-examining-the-effects-of-p-hacking/"&gt;originally posted&lt;/a&gt; in the author's personal blog.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Imagine you have a two group between-S study with N=30 in each group. You compute a two-sample t-test and the result is p = .09, not statistically significant with an effect size r = .17. Unbeknownst to you there is really no relationship between the IV and the DV. But, because you believe there is a relationship (you decided to run the study after all!), you think maybe adding five more subjects to each condition will help clarify things. So now you have N=35 in each group and you compute your t-test again. Now p = .04 with r = .21.&lt;/p&gt;
&lt;p&gt;If you are reading this blog you might recognize what happened here as an instance of p-hacking. This particular form (testing periodically as you increase N) of p-hacking was one of the many data analytic flexibility issues exposed by &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704"&gt;Simmons, Nelson, and Simonshon (2011)&lt;/a&gt;. But what are the real consequences of p-hacking? How often will p-hacking turn a null result into a positive result? What is the impact of p-hacking on effect size?&lt;/p&gt;
&lt;p&gt;These were the kinds of questions that I had. So I wrote a little R function that simulates this type of p-hacking. The function – called phack – is designed to be flexible, although right now it only works for two-group between-S designs. The user is allowed to input and manipulate the following factors (argument name in parentheses):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initial Sample Size (initialN): The initial sample size (for each group) one had in mind when beginning the study (default = 30).&lt;/li&gt;
&lt;li&gt;Hack Rate (hackrate): The number of subjects to add to each group if the p-value is not statistically significant before testing again (default = 5).&lt;/li&gt;
&lt;li&gt;Population Means (grp1M, grp2M): The population means (Mu) for each group (default 0 for both).&lt;/li&gt;
&lt;li&gt;Population SDs (grp1SD, grp2SD): The population standard deviations (Sigmas) for each group (default = 1 for both).&lt;/li&gt;
&lt;li&gt;Maximum Sample Size (maxN): You weren’t really going to run the study forever right? This is the sample size (for each group) at which you will give up the endeavor and go run another study (default = 200).&lt;/li&gt;
&lt;li&gt;Type I Error Rate (alpha): The value (or lower) at which you will declare a result statistically significant (default = .05).&lt;/li&gt;
&lt;li&gt;Hypothesis Direction (alternative): Did your study have a directional hypothesis? Two-group studies often do (i.e., this group will have a higher mean than that group). You can choose from “greater” (Group 1 mean is higher), “less” (Group 2 mean is higher), or “two.sided” (any difference at all will work for me, thank you very much!). The default is “greater.”&lt;/li&gt;
&lt;li&gt;Display p-curve graph (graph)?: The function will output a figure displaying the p-curve for the results based on the initial study and the results for just those studies that (eventually) reached statistical significance (default = TRUE). More on this below.&lt;/li&gt;
&lt;li&gt;How many simulations do you want (sims). The number of times you want to simulate your p-hacking experiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To make this concrete, consider the following R code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;res &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; phack&lt;span class="p"&gt;(&lt;/span&gt;initialN&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hackrate&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp1M&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp2M&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp1SD&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  grp2SD&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; maxN&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; alpha&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; alternative&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;greater&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; graph&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; sims&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This says you have planned a two-group study with N=30 (initialN=30) in each group. You are going to compute your t-test on that initial sample. If that is not statistically significant you are going to add 5 more (hackrate=5) to each group and repeat that process until it is statistically significant or you reach 200 subjects in each group (maxN=200). You have set the population Ms to both be 0 (grp1M=0; grp2M=0) with SDs of 1 (grp1SD=1; grp2SD=1). You have set your nominal alpha level to .05 (alpha=.05), specified a direction hypothesis where group 1 should be higher than group 2 (alternative=“greater”), and asked for graphical output (graph=TRUE). Finally, you have requested to run this simulation 1000 times (sims=1000).&lt;/p&gt;
&lt;p&gt;So what happens if we run this experiment?&lt;sup&gt;1&lt;/sup&gt; So we can get the same thing, I have set the random seed in the code below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;source&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://rynesherman.com/phack.r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# read in the p-hack function&lt;/span&gt;
set.seed&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
res &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; phack&lt;span class="p"&gt;(&lt;/span&gt;initialN&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hackrate&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp1M&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp2M&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp1SD&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; grp2SD&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   maxN&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; alpha&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; alternative&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;greater&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; graph&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; sims&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The following output appears in R:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Proportion&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;Original&lt;/span&gt; &lt;span class="n"&gt;Samples&lt;/span&gt; &lt;span class="n"&gt;Statistically&lt;/span&gt; &lt;span class="n"&gt;Significant&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.054&lt;/span&gt;
&lt;span class="n"&gt;Proportion&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;Samples&lt;/span&gt; &lt;span class="n"&gt;Statistically&lt;/span&gt; &lt;span class="n"&gt;Significant&lt;/span&gt; &lt;span class="n"&gt;After&lt;/span&gt; &lt;span class="n"&gt;Hacking&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.196&lt;/span&gt;
&lt;span class="n"&gt;Probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;Stopping&lt;/span&gt; &lt;span class="n"&gt;Before&lt;/span&gt; &lt;span class="n"&gt;Reaching&lt;/span&gt; &lt;span class="n"&gt;Significance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.805&lt;/span&gt;
&lt;span class="n"&gt;Average&lt;/span&gt; &lt;span class="n"&gt;Number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;Hacks&lt;/span&gt; &lt;span class="n"&gt;Before&lt;/span&gt; &lt;span class="n"&gt;Significant&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Stopping&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;28.871&lt;/span&gt;
&lt;span class="n"&gt;Average&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="n"&gt;Added&lt;/span&gt; &lt;span class="n"&gt;Before&lt;/span&gt; &lt;span class="n"&gt;Significant&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Stopping&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;144.355&lt;/span&gt;
&lt;span class="n"&gt;Average&lt;/span&gt; &lt;span class="n"&gt;Total&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="mf"&gt;174.355&lt;/span&gt;
&lt;span class="n"&gt;Estimated&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;without&lt;/span&gt; &lt;span class="n"&gt;hacking&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Estimated&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;hacking&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;
&lt;span class="n"&gt;Estimated&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;hacking&lt;/span&gt; &lt;span class="mf"&gt;0.19&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;non&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;significant&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;included&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first line tells us how many (out of the 1000 simulations) of the originally planned (N=30 in each group) studies had a p-value that was .05 or less. Because there was no true effect (grp1M = grp2M) this at just about the nominal rate of .05. But what if we had used our p-hacking scheme (testing every 5 subjects per condition until significant or N=200)? That result is in the next line. It shows that just about 20% of the time we would have gotten a statistically significant result. So this type of hacking has inflated our Type I error rate from 5% to 20%. How often would we have given up (i.e., N=200) before reaching statistical significance? That is about 80% of the time. We also averaged 28.87 “hacks” before reaching significance/stopping, averaged having to add N=144 (per condition) before significance/stopping, and had an average total N of 174 (per condition) before significance/stopping.&lt;/p&gt;
&lt;p&gt;What about effect sizes? Naturally the estimated effect size (r) was .00 if we just used our original N=30 in each group design. If we include the results of all 1000 completed simulations that effect size averages out to be r = .03. Most importantly, if we exclude those studies that never reached statistical significance, our average effect size r = .19.&lt;/p&gt;
&lt;p&gt;This is pretty telling. But there is more. We also get this nice picture:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/phack.png" alt="Phack" /&gt;&lt;/p&gt;
&lt;p&gt;It shows the distribution of the p-values below .05 for the initial study (upper panel) and for those p-values below .05 for those reaching statistical significance. The p-curves (see &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2256237"&gt;Simonsohn, Nelson, &amp;amp; Simmons, 2013&lt;/a&gt;) are also drawn on. If there is really no effect, we should see a flat p-curve (as we do in the upper panel). And if there is no effect and p-hacking has occurred, we should see a p-curve that slopes up towards the critical value (as we do in the lower panel).&lt;/p&gt;
&lt;p&gt;Finally, the function provides us with more detailed output that is summarized above. We can get a glimpse of this by running the following code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This generates the following output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Initial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;  &lt;span class="n"&gt;Hackcount&lt;/span&gt;     &lt;span class="n"&gt;Final&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;  &lt;span class="n"&gt;NAdded&lt;/span&gt;    &lt;span class="n"&gt;Initial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;       &lt;span class="n"&gt;Final&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
&lt;span class="mf"&gt;0.86410908&lt;/span&gt;         &lt;span class="mi"&gt;34&lt;/span&gt;  &lt;span class="mf"&gt;0.45176972&lt;/span&gt;     &lt;span class="mi"&gt;170&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.14422580&lt;/span&gt;   &lt;span class="mf"&gt;0.006078565&lt;/span&gt;
&lt;span class="mf"&gt;0.28870264&lt;/span&gt;         &lt;span class="mi"&gt;34&lt;/span&gt;  &lt;span class="mf"&gt;0.56397332&lt;/span&gt;     &lt;span class="mi"&gt;170&lt;/span&gt;   &lt;span class="mf"&gt;0.07339944&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.008077691&lt;/span&gt;
&lt;span class="mf"&gt;0.69915219&lt;/span&gt;         &lt;span class="mi"&gt;27&lt;/span&gt;  &lt;span class="mf"&gt;0.04164525&lt;/span&gt;     &lt;span class="mi"&gt;135&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.06878039&lt;/span&gt;   &lt;span class="mf"&gt;0.095492249&lt;/span&gt;
&lt;span class="mf"&gt;0.84974744&lt;/span&gt;         &lt;span class="mi"&gt;34&lt;/span&gt;  &lt;span class="mf"&gt;0.30702946&lt;/span&gt;     &lt;span class="mi"&gt;170&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.13594941&lt;/span&gt;   &lt;span class="mf"&gt;0.025289555&lt;/span&gt;
&lt;span class="mf"&gt;0.28048754&lt;/span&gt;         &lt;span class="mi"&gt;34&lt;/span&gt;  &lt;span class="mf"&gt;0.87849707&lt;/span&gt;     &lt;span class="mi"&gt;170&lt;/span&gt;   &lt;span class="mf"&gt;0.07656582&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.058508736&lt;/span&gt;
&lt;span class="mf"&gt;0.07712726&lt;/span&gt;         &lt;span class="mi"&gt;34&lt;/span&gt;  &lt;span class="mf"&gt;0.58909693&lt;/span&gt;     &lt;span class="mi"&gt;170&lt;/span&gt;   &lt;span class="mf"&gt;0.18669338&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.011296131&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The object res contains the key results from each simulation including the p-value for the initial study (Initial.p), the number of times we had to hack (Hackcount), the p-value for the last study run (Final.p), the total N added to each condition (NAdded), the effect size r for the initial study (Initial.r), and the effect size r for the last study run (Final.r).&lt;/p&gt;
&lt;p&gt;So what can we do with this? I see lots of possibilities and quite frankly I don’t have the time or energy to do them. Here are some quick ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What would happen if there were a true effect?&lt;/li&gt;
&lt;li&gt;What would happen if there were a true (but small) effect?&lt;/li&gt;
&lt;li&gt;What would happen if we checked for significance after each subject (hackrate=1)?&lt;/li&gt;
&lt;li&gt;What would happen if the maxN were lower?&lt;/li&gt;
&lt;li&gt;What would happen if the initial sample size was larger/smaller?&lt;/li&gt;
&lt;li&gt;What happens if we set the alpha = .10?&lt;/li&gt;
&lt;li&gt;What happens if we try various combinations of these things?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll admit I have tried out a few of these ideas myself, but I haven’t really done anything systematic. I just thought other people might find this function interesting and fun to play with.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; By the way, all of these arguments are set to their default, so you can do the same thing by simply running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;res &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; phack&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary></entry><entry><title>Bem is Back: A Skeptic's Review of a Meta-Analysis on Psi</title><link href="http://osc.centerforopenscience.org/2014/06/25/a-skeptics-review/" rel="alternate"></link><updated>2014-06-25T01:00:00-04:00</updated><author><name>EJ Wagenmakers</name></author><id>tag:osc.centerforopenscience.org,2014-06-25:2014/06/25/a-skeptics-review/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;James Randi, magician and scientific skeptic, has compared those who believe in the paranormal to “unsinkable rubber ducks”: after a particular claim has been thoroughly debunked, the ducks submerge, only to resurface again a little later to put forward similar claims.  &lt;/p&gt;
&lt;p&gt;In light of this analogy, it comes as no surprise that Bem and colleagues have produced a new paper claiming that people can look into the future. The paper is titled &lt;a href="http://ssrn.com/abstract=2423692"&gt;“Feeling the Future: A Meta-Analysis of 90 Experiments on the Anomalous Anticipation of Random Future Events”&lt;/a&gt; and it is authored by Bem, Tressoldi, Rabeyron, and Duggan.&lt;/p&gt;
&lt;p&gt;Several of my colleagues have browsed Bem's meta-analysis and have asked for my opinion. Surely, they say, the statistical evidence is overwhelming, regardless of whether you compute a p-value or a Bayes factor. Have you not changed your opinion? This is a legitimate question, one which I will try and answer below by showing you my review of an earlier version of the Bem et al. manuscript.  &lt;/p&gt;
&lt;p&gt;I agree with the proponents of precognition on one crucial point: their work is important and should not be ignored. In my opinion, the work on precognition shows in dramatic fashion that our current methods for quantifying empirical knowledge are insufficiently strict. If Bem and colleagues can use a meta-analysis to demonstrate the presence of precognition, what should we conclude from a meta-analysis on other, more plausible phenomena?  &lt;/p&gt;
&lt;p&gt;Disclaimer: the authors have revised their manuscript since I reviewed it, and they are likely to revise their manuscript again in the future. However, my main worries call into question the validity of the enterprise as a whole.  &lt;/p&gt;
&lt;p&gt;To keep this blog post self-contained, I have added annotations in italics to provide context for those who have not read the Bem et al. manuscript in detail. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;My review of Bem, Tressoldi, Rabeyron, and Duggan&lt;/strong&gt;&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;“Ah, psi. The remainder of my review will use a professional tone, and I will try to outline the problems I have with the authors' analyses. That said, I do think that this line of research tarnishes the reputation of psychology as an academic discipline. I urge the authors to convince themselves of the absence of psi and try and replicate one of Bem's experiment in a purely confirmatory setting, with a preregistered analysis protocol. When they monitor the Bayes factor they will, as N grows large, obtain massive evidence in favor of the truth. One good, preregistered experiment is worth a thousand experiments where the results are based on cherry-picking. To indicate that cherry-picking is a problem, I have never seen a preregistered experiment that monitored the Bayes factor and ended up supporting psi. Never. If the authors are able to produce such evidence in their own lab (after preregistering the analysis on OSF, and collecting data until the one-sided BF in favor of psi reaches, say, 20) then they can challenge me for an adversarial collaboration and I will gladly accept. Anyway, after having made my prior opinion clear, let's move on to the review. I have several major worries:  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 1: In the abstract, Bem and colleagues suggest that the meta-analysis concerns replications of Bem's 2011 studies. However, this is not the case. The meta-analysis largely consists of studies that pre-date the 2011 work. Almost all of the pre-2011 studies were conducted by ESP proponents. The post-2011 studies, many of which were conducted by ESP skeptics, found no effect.&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;Worry 1. The authors wish to study replications of Bem's work. This means they should only consider studies that were inspired by Bem (2011). A quick look at Table A1 shows that very many studies in the meta-analysis preceded Bem, sometimes by as much as 10 years. It is possible that the earlier studies had advance access to Bem’s protocol, but if this is the case it should be made clear from the outset. A related worry is that skeptics only got interested after the publication of Bem (2011). Hence, I believe that there may be a difference between replications “pre-Bem” (conducted and reported by proponents only) and “post-Bem” (conducted by proponents and skeptics alike). This is a factor that should be taken into account. Perhaps the size of the effect suddenly decreased after 2011?  &lt;/p&gt;
&lt;p&gt;Indeed, when I consider only those psi replications that have been &lt;em&gt;published post-Bem&lt;/em&gt;, I find Galak, Ritchie, Robinson, Subbottsky, Traxler, and Wagenmakers (the Hitchman studies seemed to be about creativity and luck, so I did not incorporate that study; including it does not change the pattern of results). Below is a table of their experiments and effect sizes:  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Experiment&lt;/th&gt;
&lt;th align="left"&gt;N&lt;/th&gt;
&lt;th align="left"&gt;ES&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 1&lt;/td&gt;
&lt;td align="left"&gt;112&lt;/td&gt;
&lt;td align="left"&gt;-0.113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 2&lt;/td&gt;
&lt;td align="left"&gt;158&lt;/td&gt;
&lt;td align="left"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 3&lt;/td&gt;
&lt;td align="left"&gt;124&lt;/td&gt;
&lt;td align="left"&gt;0.110&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 4&lt;/td&gt;
&lt;td align="left"&gt;109&lt;/td&gt;
&lt;td align="left"&gt;0.170&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 5&lt;/td&gt;
&lt;td align="left"&gt;211&lt;/td&gt;
&lt;td align="left"&gt;0.050&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 6&lt;/td&gt;
&lt;td align="left"&gt;106&lt;/td&gt;
&lt;td align="left"&gt;-0.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Galak Exp 7&lt;/td&gt;
&lt;td align="left"&gt;2469&lt;/td&gt;
&lt;td align="left"&gt;-0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Ritchie Exp 1&lt;/td&gt;
&lt;td align="left"&gt;50&lt;/td&gt;
&lt;td align="left"&gt;0.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Ritchie Exp 2&lt;/td&gt;
&lt;td align="left"&gt;50&lt;/td&gt;
&lt;td align="left"&gt;-0.222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Ritchie Exp 3&lt;/td&gt;
&lt;td align="left"&gt;50&lt;/td&gt;
&lt;td align="left"&gt;-0.041&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Robinson&lt;/td&gt;
&lt;td align="left"&gt;50&lt;/td&gt;
&lt;td align="left"&gt;-0.120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Subbotsky Exp 1&lt;/td&gt;
&lt;td align="left"&gt;75&lt;/td&gt;
&lt;td align="left"&gt;0.282&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Subbotsky Exp 2&lt;/td&gt;
&lt;td align="left"&gt;25&lt;/td&gt;
&lt;td align="left"&gt;0.302&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Subbotsky Exp 3&lt;/td&gt;
&lt;td align="left"&gt;26&lt;/td&gt;
&lt;td align="left"&gt;-0.412&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Traxler Exp 1&lt;/td&gt;
&lt;td align="left"&gt;48&lt;/td&gt;
&lt;td align="left"&gt;.060&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Traxler Exp 2&lt;/td&gt;
&lt;td align="left"&gt;60&lt;/td&gt;
&lt;td align="left"&gt;-.346&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Wagenmakers&lt;/td&gt;
&lt;td align="left"&gt;100&lt;/td&gt;
&lt;td align="left"&gt;-0.022&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When it comes to a proper assessment of the replication success for Bem’s studies, I think the above table is the correct one. I did not conduct the meta-analysis but from eyeballing the numbers it seems that there is nothing there whatsoever. The fact that this picture is changed by adding the other studies supports the assertion that they are contaminated by researcher bias and a lack of control over the analysis procedure.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 2: A meta-analysis is sometimes likened to the goose who eats garbage and produces golden eggs. In fact, it is more apt to recall the saying “garbage in, garbage out”; when a meta-analysis is unleashed on a set of biased studies, the result is likely to be incorrect and misleading.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Worry 2. A meta-analysis is only reliable when the individual studies are reliable. When the individual studies tend to be biased towards “discovering” psi, the meta-analysis is useless and will just confirm the presence of researcher bias rather than psi. Some of the studies that tried to replicate Bem prevented cherry-picking and data torture by using preregistration. Only with preregistration can we be somewhat certain that the results are clean. Bem’s own studies, for instance, are tainted by all kinds of post-hoc procedures in order to present the results in the most favorable light. This was common practice in the field, and it still is. For a subject that is as contentious as psi, only studies with preregistration should be allowed in the meta-analysis.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 3: Bem et al. deem replication studies suspect when they do not use the same software program as the original study. This is odd.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Worry 3. Never before did I hear of the idea that a precise replication is one that uses the same software program. One reason not to use the original program is that it capitalized on chance by presenting many different sorts of pictures. Another reason is that the design of specific experiments was suboptimal (e.g., not counterbalanced). When the authors argue that the Galak experiments (and my own) are not direct replications they make it quite clear that their purpose is to present evidence for the presence of psi, regardless of any inconvenient facts that they may encounter along the way.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 4: Bayes factors (BFs) for related experiments may not simply be multiplied. This worry is important, because it affects the analysis of more traditional studies as well.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Worry 4. The authors do not indicate how they calculated the Bayes factor. I hazard to guess that they simply multiplied the BFs from the individual studies; this practice is attractive but flawed, as it assumes that the individual experiments are independent and unrelated. Instead, the authors should calculate a BF for the following two models: H0: the mean of the random effects meta-analysis equals zero, versus H1: the mean of the random effects meta-analysis is larger than zero. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 5: Fail-safe N is the number of studies that have to show a null result in order to wash out the reported effect. When fail-safe N is very large, it is tempting to conclude that the reported effect is robust to publication bias. However, meta-analytic fail-safe methods are not informative in the presence of questionable research practices. Again, this worry is important because it also affects the analysis of more traditional studies.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Worry 5. The fail-safe analyses are not informative. If the null hypothesis is exactly true one can still extract a significant p-value by a combination of questionable research practices. In fact, for me this is the main, really important message from the authors’ fail-safe analysis: fails-safe analyses are meaningless in the presence of questionable research practices.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 6: Several researchers have argued that the Bem 2011 findings do not stand up to statistical scrutiny. The meta-analysis does not cite these critical articles.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Worry 6. Can one replicate that which does not exist? The Bem studies have been subjected to critique from many different angles. For instance, Greg Francis has argued that the results are too good to be true, and Judd, Westfall, and Kenny have shown that certain experiments no longer yield significant results once you include items as a random effect.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Background for Worry 7: A meta-analysis is of limited value when the individual studies are biased. This is perhaps the main lesson we can learn from research on ESP.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Worry 7. This is a worry for the editor rather than the authors. If ever there was a paper that showed the futility of meta-analysis, this one is it. Here we have one of the most ridiculous claims that intelligent people have ever dared to make (yes; the hypothesis that aliens built the pyramids is more plausible than people being able to look into the future) – and a meta-analysis supports this claim. The unavoidable conclusion is not that psi exists; rather, it is that meta-analysis is a tool that is fraught with danger. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;I raised some minor concerns as well but I've deleted these here.&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;I always sign my reviews,&lt;br /&gt;
Eric-Jan Wagenmakers”&lt;/p&gt;
&lt;p&gt;In conclusion, the work by Bem and colleagues demonstrates that our bread-and-butter statistical methods can not be relied upon blindly. Meta-analyses, Bayes factors, p-curve analyses: all of these methods crumble to dust in the hands of researchers who are motivated to prove that ESP exists. In fact, it is not an exaggeration to state that ESP is where statistical methods go to die. Of course, Bem and colleagues will disagree with my interpretation of their work. To resolve the conflict, I repeat my challenge:  preregister the experiment on OSF, monitor the Bayes factor until it exceeds 20, and then I will happily participate in an adversarial collaboration, and grudgingly acknowledge defeat if the data force me to. If the ESP proponents truly believe that the effect is real, preregistration and adversarial collaborations are the only way forward. Perhaps the same holds for other psychological phenomena that are currently under scrutiny as well. &lt;/p&gt;</summary></entry><entry><title>Open Science Initiatives promote Diversity, Social Justice, and Sustainability</title><link href="http://osc.centerforopenscience.org/2014/06/18/osi-promote-dsjs/" rel="alternate"></link><updated>2014-06-18T12:30:00-04:00</updated><author><name>Jon Grahe</name></author><id>tag:osc.centerforopenscience.org,2014-06-18:2014/06/18/osi-promote-dsjs/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;As I follow the recent social media ruckus centered on replication science questioning motives and methods, it becomes clear that the open science discussion needs to consider the point made by the title of this blog; maybe repeatedly. For readers who weren’t following this, &lt;a href="https://politicalsciencereplication.wordpress.com/2014/05/25/replication-bullying-who-replicates-the-replicators/"&gt;this blog&lt;/a&gt; by a political scientist and another post from the &lt;a href="http://www.spspblog.org/psychology-news-round-up-may-23rd/"&gt;SPSP Blog&lt;/a&gt; might be of interest. I invite you to join me in evaluating this argument as the discussion progresses. I contend that “Open Science Initiatives promote Diversity, Social Justice, and Sustainability.” Replication science and registered reports are two Open Science Initiatives and by extension should also promote these ideals. If this is not true, I will abandon this revolution and go back to the status quo. However, I am confident that when considering all the evidence, you will agree with me that these idealistic principles benefit from openness generally and by open science specifically.  &lt;/p&gt;
&lt;p&gt;Before suggesting specific mechanisms by which this occurs, I will briefly note that the definitions of &lt;a href="http://en.wikipedia.org/wiki/Open_science"&gt;Open Science&lt;/a&gt;, &lt;a href="http://en.wikipedia.org/wiki/Diversity"&gt;Diversity&lt;/a&gt;, &lt;a href="http://en.wikipedia.org/wiki/Social_justice"&gt;Social Justice&lt;/a&gt;, and &lt;a href="http://en.wikipedia.org/wiki/Sustainability"&gt;Sustainability&lt;/a&gt; that are listed on Wikipedia are sufficient for this discussion since Wikipedia itself is an Open Science initiative. Also, I would like to convey the challenge of advancing each of these simultaneously. My own institution, Pacific Lutheran University (PLU), in our recent long range plan, &lt;a href="http://issuu.com/pacific.lutheran.university/docs/plu-2020?e=1067239/2651397"&gt;PLU2020&lt;/a&gt;, highlighted the importance of uplifting each of these at our own institution as introduced on page 11, “As we discern our commitments for the future, we reaffirm the ongoing commitments to diversity, sustainability, and justice that already shape our contemporary identity, and we resolve to integrate these values ever more intentionally into our mission and institution.” This is easier said than done because at times the goals of these ideals sometimes conflict. For instance, the environmental costs of feeding billions of people and heating their homes are enormous. Sometimes valuing diversity (such as scholarships targeted for people of color) seems unjust because resources are being assigned unevenly. These tensions can be described with many examples across numerous goals in all three dimensions and highlight the need to make balanced decisions.  &lt;/p&gt;
&lt;p&gt;PLU has not yet resolved this challenge in uplifting all three simultaneously, but I hope that we succeed as we continue the vigorous discussion. Why each is important should be considered from is a Venn diagram on the sustainability Wikipedia page showing &lt;a href="http://en.wikipedia.org/wiki/File:Sustainable_development.svg"&gt;sustainable development&lt;/a&gt; as intersections between three pillars of sustainable development, social (people), economic, and environmental because even sustainability itself represents competing interests. Diversity and Social Justice are both core aspects of the social dimension, where uplifting diversity highlights the importance of distinct ideas and cultures and helps us understand why people and their varied ideas, in addition to oceans and forests are important resources of our planet. The ideals of social justice aim to provide mechanisms such that all members of our diverse population receive and contribute our fair share of these resources. Because resources are limited and society complex and flawed, these ideals are often more aspirational rather than practical. However, the basic premise of uplifting all three is that we are better when valuing diversity, providing social justice, and sustainably using the planet’s resources (people, animals, plants, and rocks). Below I provide examples for how OSIs promote each of these principles while illustrating why each is important to science.   &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;&lt;strong&gt;Open Science Initiatives promote Diversity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;All Voices Are Important.&lt;/strong&gt; While diversity is often confused with labels that describe cultural, ethnic, and economic differences, it is much broader as well because it includes diversity of ideas. As humans, our ideas can easily cross these demographic categories, and as individuals our ideas are the most personal and diverse concepts on the planet. There are many reasons to value diversity, and scientific diversity encompasses both the complexity of research questions (between and within disciplines) as well as complexity of approaches to one research question. Science advances as these disparate approaches center on the most correct answer available at the time. Here I note three ways that OSIs promote Diversity. The first is the capacity for everyone’s voice to be heard. The Open Science Framework and similar platforms allow all researchers to document a study and then to share findings publicly. Therefore, if someone has the best new idea or finding, the data can be available without going through a review process. However because not all research is valuable, quality filters are still necessary. Open Access journals provide opportunity for any idea to be published, but only articles and studies that peers deem worthy will be replicated and advanced. The status quo does not encourage diversity because methods and ideas must meet the approval of people in power (editors and reviewers) before they are considered as part of larger scientific discourse.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Data Can Be Re-Evaluated by Anyone.&lt;/strong&gt; Open science principles recommend full disclosure of materials and data when possible. While there are some exceptions why researchers may withhold data to protect participants, the vast majority of our published work should share anonymous data so that “data detectives” can re-evaluate it to challenge claims or make their own assertions. The Status Quo in psychology suggests that data from published work should be made available upon request from qualified peers published work (see APA ethical guidelines; &lt;a href="http://www.apa.org/ethics/code/principles.pdf"&gt;Article 8.14a&lt;/a&gt;, see page 13). However, in practice, this does not seem to be the case as demonstrated by &lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0026828#pone-0026828-g002"&gt;Wickerts, Bakker, and Molenaar (2011)&lt;/a&gt;. In the recent academic discussion about the validity of the Johnson, Cheung, and Donnellan (2014) pre-registered replication of Schnall, Benton, &amp;amp; Harvey (2008), five different researchers (see links below) have shared reanalyses of the data (to date) to examine the assertion that a ceiling effect caused the findings. Because Open Science requires open data, this discussion was possible:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://yorl.tumblr.com/post/87428392426/ceiling-effects"&gt;Yoel Inbar&lt;/a&gt;
&lt;a href="http://pigee.wordpress.com/2014/05/24/additional-reflections-on-ceiling-effects-in-recent-replication-research/"&gt;Chris Fraley&lt;/a&gt;
&lt;a href="http://www.talyarkoni.org/blog/2014/06/01/there-is-no-ceiling-effect-in-johnson-cheung-donnellan-2014/"&gt;Tal Yarkoni&lt;/a&gt;
&lt;a href="http://www.nicebread.de/reanalyzing-the-schnalljohnson-cleanliness-data-sets-new-insights-from-bayesian-and-robust-approaches/"&gt;Felix Schonbrodt&lt;/a&gt;
&lt;a href="http://datacolada.org/2014/06/04/23-ceiling-effects-and-replications/"&gt;Uri Simonsohn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Different Publication Models Provide Empirical Scientific Tests.&lt;/strong&gt; Valuing diversity means that we can agree to disagree. It is not surprising that the status quo has not embraced Open Science without reservation. A core characteristic of having power is the desire to maintain that power. Change is expensive and frightening. The Open Science paradigm shift will take time. During that time we can take up the challenge posited by Brent Roberts in &lt;a href="http://www.personality-arp.org/metablog/author/pigee/"&gt;his blog suggesting an empirical test&lt;/a&gt; of the difference between findings determined by editorial decisions from status quo publications versus Pre-Registered reports such as those published in the recent &lt;a href="http://www.psycontent.com/content/l67413865317/?p=1286033b4236476181c18f1cd422a5ab&amp;amp;pi=0"&gt;Social Psychology special issue&lt;/a&gt;. While his suggestion for a premiere journal to accept only pre-registered reports for a year might be too challenging to embrace, I imagine that some data detective is already planning to test effect sizes of pre-registered vs. status quo publications for a future research project. It would be interesting to randomly assign authors to condition, but that would not be fair. Maybe a within-subject design would be more appropriate and we could counterbalance the order.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Science Initiatives promote Social Justice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Underprivileged Need More Voice.&lt;/strong&gt; When arguing that OSIs promote Diverse opinions because everyone’s scientific contributions should be considered, we should remember that people who currently have the least access to premiere publication outlets are often underprivileged. From a systems perspective, we know that is it very challenging for bright young people to escape from the economic and cultural limitations of their upbringing. Access to higher education is limited to those who can afford it, and not all institutions are similarly resourceful. Subscription costs to access premiere research journals can be excessive. Further, the costs of completing expensive research are similarly daunting. Therefore, OSIs promote social justice in three ways. First, providing access to published research means that scientists can ask better questions. Second, Meta science projects provide access to researchers with limited means to participate in bigger science. Third, OSIs can offer outcomes so that underprivileged non-scientists can better consume and utilize scientific research (e.g., patients with conditions targeted in the research).  The &lt;a href="https://osf.io/ezcuj/"&gt;Reproducibility Project&lt;/a&gt;, Many Labs (&lt;a href="https://osf.io/wx7ck/"&gt;1&lt;/a&gt;, &lt;a href="https://osf.io/8cd4r/"&gt;2&lt;/a&gt;, and &lt;a href="https://osf.io/ct89g/"&gt;3&lt;/a&gt;) projects, and the &lt;a href="https://osf.io/wfc6u/"&gt;Collaborative Replications and Education Project&lt;/a&gt; offer diverse approaches for a diverse group of researchers. While resources to complete research are still required, individual contributions to larger projects enable researchers to participate in bigger science.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Social Justice Requires Freedom of Speech.&lt;/strong&gt; A core principle of freedom is free speech through protections of the press and individuals. This can be eroded through government censorship, and limited speech is experienced by too many people on the planet. However, status quo research practices also limit freedom of speech. Publication bias is often discussed as a major cause of poor scientific practices. This bias favors novel, counter intuitive, exciting papers. It is only with the advent of OSIs such as Open Access journals and computer platforms that publicly document any type of research such that replications can freely enter scientific discourse. In this way, meta-analytic researchers can actually open the “file drawer” containing all those  non-publishable studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Communication Improves Scientific Conscientiousness.&lt;/strong&gt; Public discourse occurs across a dynamic timeline. Publications can take years to complete while Tweets and Status updates take seconds. Though the scientific privileged do not have the same following as media and sports stars do, their expressions of support or disdain for this scientific paradigm revolution are followed by dozens, or hundreds, or thousands of followers. Although I believe we should all be forgiven for public comments made without review (such as an offensive tweet), it is normative pressure that provides the catalysts and mechanisms for us to apologize for offensive remarks. If some computer mediated communications researcher evaluated the back and forth between Open Science proponents and detractors, they will see zealous voices from both sides express potentially offensive comments. In the aftermath, it seemed that Open Science proponents went out of their way to demonstrate that they were not “replication bullies” through apology and clarification. Though mistakes will occur, and angry voices can echo, because Open Science supports open communication, proponents by necessity, address public criticism publicly. As such, it is difficult to see how the Replication Police can allow Data Detectives to engage in bullying behavior without losing support from the scientific public because their findings are only as good as their reputation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Science Initiatives promote Sustainability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sustainability is a Byproduct of Open Science Tools.&lt;/strong&gt; The primary tools of open science initiatives are computer based such as platforms for open publication, platforms to document the research process, and platforms to uniformly collect data across locations. Though these systems were not developed specifically to reduce waste, they can. Online versus paper publications, computerized filing versus a metal cabinet, and computerized data collection versus paper and pencil administration of measures are all examples of reduced waste that can occur in this transition. These systems allow for seamless sharing of materials between collaborators, which further reduces time, printing, and delivery costs and allow for new research questions that could not be asked in the paper and pencil era.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Science Initiatives Sustain the Workforce.&lt;/strong&gt; I started getting involved in open science initiatives because I felt like I could not effectively contribute to scientific dialogue because I came from a small school with limited research capacity. Many colleagues at small teaching and research institutions struggle similarly with limited tools, subject pools, time, and access to colleagues. While the contributions of a replication and a novel experiment are widely different, meta-science projects provide me access to the larger scientific debate. Contributors to the &lt;a href="http://www.internationalsituationsproject.com/about"&gt;International Situation Project&lt;/a&gt; or the subsequent International Personality Project can access the data to answer novel questions beyond those intended by the primary researchers. This is an ideal OSI where contributors to a multiple site project can answer novel, impactful questions. OSIs promote sustainability of the workforce by energizing all members of the scientific team from top tier research institutions to primarily undergraduate institutions. Further, by engaging everyone, there is reduction of lost data because there would be fewer small studies that never enter public scientific discourse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sustainable Decisions Result from Better Science.&lt;/strong&gt; Finally, OSIs promote sustainability of resources by helping scientists better communicate good science. The Open Science paradigm shift is a reaction to poor science that results from publication bias. If &lt;a href="http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124"&gt;Ioannidis (2005)&lt;/a&gt; is correct that most published findings are false, it is not possible for us to make informed decisions based on strong empirical evidence because we lack the evidence. OSIs are specifically designed to improve the quality of science and therefore the quality of the evidence.  When considering people as part of the sustainability equation, this article about open access to science saves lives &lt;a href="http://chronicle.com/article/Open-Access-to-Research-Can/136065/"&gt;(Suber &amp;amp; Cubrinklas, 2012)&lt;/a&gt; from The Chronicle of Higher Education provides another example. Another example is the recently announced &lt;a href="http://www.arl.org/news/arl-news/3257-share-selects-center-for-open-science-as-development-partner-for-notification-service#.U498ZihBmHh"&gt;SHARE system&lt;/a&gt; designed to notify researchers of related publications so that they can more easily navigate multitudes of publications to find all of them that are most pertinent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do OSIs promote Diversity, Social Justice, and Sustainability (if so, is this a good thing)?&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;My bias is clear. I think that Open Science researchers are challenging a status quo system that is powerful and threatening. I think they are risking much professionally when they encourage changes to publication expectations while engaging in replication science. Not only do OSIs promote each of these separately, I further argue that promoting open discussion of the interconnection between Diversity, Social Justice, and Sustainability leads to a better society. However, this question of the interconnectedness must wait for another blog after more challenges are known. &lt;/p&gt;
&lt;p&gt;Just as my home institution is struggling with the conversation about broad integration of these sometimes conflicting principles across the curriculum, Science will struggle to address many conflicting outcomes of the Open Science Revolution. I am a member of the status quo while calling for change. I am an executive editor for a journal that still operates with old habits. I advocate for student contributions to larger science though I know that student contributions can be flawed. I know that conflicts are in the future as we try to increase access to data and the ability to filter those data. This is the beginning of a shift, not the end of it. &lt;/p&gt;
&lt;p&gt;I invite you to join this discussion by helping to identify metrics to measure the impact of OSIs on these principles. I invite suggestions from both sides of the argument. If I am wrong and OSIs lead to limited thinking, increased injustice, and poor resource management, then I will give up. I will return to the status quo and disappear. If I am right, then I look forward to reading journals where I hope my children will make their scientific contributions.  &lt;/p&gt;</summary></entry><entry><title>Thoughts on this debate about social scientific rigor</title><link href="http://osc.centerforopenscience.org/2014/06/11/thoughts-on-this-debate/" rel="alternate"></link><updated>2014-06-11T16:00:00-04:00</updated><author><name>Betsy Levy Paluck</name></author><id>tag:osc.centerforopenscience.org,2014-06-11:2014/06/11/thoughts-on-this-debate/</id><summary type="html">&lt;p&gt;&lt;em&gt;This article was originally posted on Betsy Levy Paluck's &lt;a href="http://www.betsylevypaluck.com/blog/2014/5/25/what-i-stand-for-in-this-discussion-about-scientific-rigor"&gt;website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;On his terrific &lt;a href="http://hardsci.wordpress.com/"&gt;blog&lt;/a&gt;, Professor Sanjay Srivastava &lt;a href="http://hardsci.wordpress.com/2014/05/25/does-the-replication-debate-have-a-diversity-problem/"&gt;points out&lt;/a&gt; that the current (vitriolic) debate about replication in psychology has been "salted with casually sexist language, and historically illiterate" arguments, on both sides. I agree, and thank him for pointing this out.  &lt;/p&gt;
&lt;p&gt;I'd like to add that I believe academics participating in this debate should be mindful of co-opting powerful terms like &lt;em&gt;bullying&lt;/em&gt; and &lt;em&gt;police&lt;/em&gt; (e.g., the "replication police") to describe the replication movement. Why? Bullying behavior describes repeated abuse from a person of higher power and influence. Likewise, many people in the US and throughout the world have a well-grounded terror of police abuse. The terror and power inequality that these terms connote is diminished when we use it to describe the experience of academics replicating one another's studies. Let's keep evocative language in reserve so that we can use it to name and change the experience of truly powerless and oppressed people.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Back to replication.&lt;/strong&gt; Here is the thing: we all believe in the principle of replication. As scientists and as psychologists, we are all here because we wish to contribute to cumulative research that makes progress on important psychological questions. This desire unites us.&lt;/p&gt;
&lt;p&gt;So what's up?  &lt;/p&gt;
&lt;p&gt;It seems to me that some people oppose the current wave of replication efforts because they do not like the tenor of the recent public discussions. As I already mentioned, neither do I. I'm bewildered by the vitriol. Just a few days ago, one of the most prominent modern economists, currently an internationally bestselling author, had his &lt;a href="http://www.ft.com/cms/s/2/e1f343ca-e281-11e3-89fd-00144feabdc0.html#axzz32mFmDac2"&gt;book called into question&lt;/a&gt; over alleged data errors in a spreadsheet that he made public. His &lt;a href="http://blogs.ft.com/money-supply/2014/05/23/piketty-response-to-ft-data-concerns/?Authorised=false"&gt;response&lt;/a&gt; was cordial and curious; his colleagues followed up with &lt;a href="http://www.nytimes.com/2014/05/25/upshot/a-new-critique-of-piketty-has-its-own-shortcomings.html?_r=0"&gt;care&lt;/a&gt;, &lt;a href="http://www.nytimes.com/2014/05/24/upshot/did-piketty-get-his-math-wrong.html?smid=tw-upshotnyt"&gt;equanimity&lt;/a&gt;, and respect.   &lt;/p&gt;
&lt;p&gt;Are we really being taught a lesson in manners from economists? Is that happening?  &lt;/p&gt;
&lt;p&gt;As one of my favorite TV characters said recently ...  &lt;/p&gt;
&lt;p&gt;If we don't like the tenor of the discussion about replication, registration, etc., let's change it.  &lt;/p&gt;
&lt;p&gt;In this spirit, I offer a brief description of what we are doing in my lab to try to make our social science rigorous, transparent, and replicable. It's one model for your consideration, and we are open to suggestions.  &lt;/p&gt;
&lt;p&gt;For the past few years we have &lt;strong&gt;registered analysis plans&lt;/strong&gt; for every new project we start. (They can be found &lt;a href="http://e-gap.org/design-registration/registered-designs/"&gt;here&lt;/a&gt; on the &lt;a href="http://e-gap.org/"&gt;EGAP&lt;/a&gt; website; this is a group to which I belong. EGAP has had great discussions in partnership with &lt;a href="http://cega.berkeley.edu/programs/BITSS/"&gt;BITSS&lt;/a&gt; about transparency.) My lab's analysis registrations are accompanied by a &lt;strong&gt;codebook&lt;/strong&gt; describing each variable in the dataset.  &lt;/p&gt;
&lt;p&gt;I am happy to say that we are just starting to get better at producing &lt;strong&gt;replication code&lt;/strong&gt; and &lt;strong&gt;data &amp;amp; file organization that is sharing-ready&lt;/strong&gt; as we do the research, rather than trying to reconstruct these things from our messy code files and Dropbox disaster areas following publication (for this, I thank my &lt;a href="http://www.betsylevypaluck.com/collaborators/"&gt;brilliant students&lt;/a&gt;, who surpass me with their coding skills and help me to keep things organized and in place. See also &lt;a href="http://faculty.chicagobooth.edu/matthew.gentzkow/research/CodeAndData.pdf"&gt;this&lt;/a&gt;). What a privilege and a learning experience to have graduate students, right? Note that &lt;a href="http://personxsituation.wordpress.com/2014/05/25/im-disappointed-a-graduate-students-perspective/"&gt;they are listening to us&lt;/a&gt; have this debate.  &lt;/p&gt;
&lt;p&gt;Margaret Tankard, Rebecca Littman, Graeme Blair, Sherry Wu, Joan Ricart-Huguet, Andreana Kenrick (awesome grad students), and Robin Gomila and David Mackenzie (awesome lab managers) have all been writing analysis registrations, organizing files, checking data codebooks, and writing replication code for the experiments we've done in the past three years, and colleagues Hana Shepherd, Peter Aronow, Debbie Prentice, and Eldar Shafir are doing the same with me. Thank goodness for all these amazing and dedicated collaborators, because one reason I understand replication to be so difficult is that it is a huge challenge to reconstruct what you thought and did over a long period of time, without careful record keeping (note: analysis registration also serves that purpose for us!).  &lt;/p&gt;
&lt;p&gt;Previously, I &lt;strong&gt;posted data&lt;/strong&gt; at Yale's ISPS archive, and for other datasets made them available on request if I thought I was going to work more on them. But in future we plan to post all published data plus the dataset's codebook. Economist and political scientists friends often post to their personal websites. Another possibility is posting in digital archives (like Yale's, but there are others: I follow @&lt;a href="https://twitter.com/annthegreen"&gt;annthegreen&lt;/a&gt; for updates on digital archiving).  &lt;/p&gt;
&lt;p&gt;I owe so much of my appreciation for these practices to my advisor &lt;a href="https://sites.google.com/site/donaldpgreen/"&gt;Donald Green&lt;/a&gt;. I've also learned a lot from &lt;a href="http://www.columbia.edu/~mh2245/"&gt;Macartan Humphreys&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;I'm interested in how we can be better. I'm listening to the constructive debates and to the suggestions out there. If anyone has questions about our current process, please leave a comment below! I'd be happy to answer questions, provide examples, and to take suggestions. &lt;/p&gt;
&lt;p&gt;It costs nothing to do this--but it slows us down. Slowing down is not a bad thing for research (though I recognize that a bad heuristic of quantity = quality still dominates our discipline). During registration, we can stop to think-- &lt;em&gt;are we sure we want to predict this? With this kind of measurement? Should we go back to the drawing board about this particular secondary prediction?&lt;/em&gt; I know that if I personally slow down, I can oversee everything more carefully. I'm learning how to say no to new and shiny projects. &lt;/p&gt;
&lt;p&gt;I want to end on the following note. I am now tenured. If good health continues, I'll be on hiring committees for years to come. In a hiring capacity, I will appreciate applicants who, though they do not have a ton of publications, can link their projects to an online analysis registration, or have posted data and replication code. Why? I will infer that they were slowing down to do very careful work, that they are doing their best to build a cumulative science. I will also appreciate candidates who have conducted studies that "failed to replicate" and who responded to those replication results with follow up work and with thoughtful engagement and curiosity (I have read about &lt;a href="http://www.talyarkoni.org/blog/2013/12/27/what-we-can-and-cant-learn-from-the-many-labs-replication-project/"&gt;Eugene Caruso's response&lt;/a&gt; and thought that he is a great model of this kind of response).&lt;/p&gt;
&lt;p&gt;I say this because it's true, and also because some academics report that their graduate students are very nervous about how replication of their lab's studies might &lt;a href="http://www.spspblog.org/simone-schnall-on-her-experience-with-a-registered-replication-project/"&gt;ruin their reputations on the job market&lt;/a&gt; (see Question 13). I think the concern is understandable, so it's important for those of us in these lucky positions to speak out about what we value and to allay fears of punishment over non-replication (see Funder: &lt;a href="http://funderstorms.wordpress.com/2012/10/31/the-perilous-plight-of-the-non-replicator/"&gt;SERIOUSLY NOT OK&lt;/a&gt;). &lt;/p&gt;
&lt;p&gt;In sum, I am excited by efforts to improve the transparency and cumulative power of our social science. I'll try them myself and support newer academics who engage in these practices. Of course, we need to have good ideas as well as good research practices (ugh--this business is not easy. Tell that to your friends who think that you've chosen grad school as a shelter from the bad job market). &lt;/p&gt;
&lt;p&gt;I encourage all of my colleagues, and especially colleagues from diverse positions in academia and from underrepresented groups in science, to comment on what they are doing in their own research and how they are affected by these ideas and practices. Feel free to post below, post on (real) blogs, write letters to the editor, have conversations in your lab and department, or &lt;a href="https://twitter.com/betsylevyp"&gt;tweet&lt;/a&gt;. I am listening. Thanks for reading.&lt;/p&gt;
&lt;p&gt;*&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A collection of comments I've been reading about the replication debate, in case you haven't been keeping up. Please do post more links below, since this isn't comprehensive.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://personxsituation.wordpress.com/2014/05/25/im-disappointed-a-graduate-students-perspective/"&gt;I'm disappointed: a graduate student's perspective&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://hardsci.wordpress.com/2014/05/25/does-the-replication-debate-have-a-diversity-problem/"&gt;Does the replication debate have a diversity problem?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://osf.io/98tkv/"&gt;Replications of Important Results in Social Psychology: Special Issue of Social Psychology&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://funderstorms.wordpress.com/2012/10/31/the-perilous-plight-of-the-non-replicator/"&gt;The perilous plight of the (non)-replicator&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://politicalsciencereplication.wordpress.com/2014/05/25/replication-bullying-who-replicates-the-replicators/"&gt;"Replication Bullying": Who replicates the replicators?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://davidjjohnson.wordpress.com/2014/05/25/rejoinder-to-schnall-2014/"&gt;Rejoinder to Schnall (2014) in Social Psychology&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.google.com/document/d/1ew7X0RaClU5_Ev4Ns3Uyn0I7PmjzP_Z1wKlnza_3Fe0/edit"&gt;Context and Correspondence for Special Issue of Social Psychology&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/"&gt;Behavioral Priming: Time to Nut Up or Shut Up&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tweets:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/DanTGilbert/status/470436673697095680"&gt;@DanTGilbert: "Simone Schnall's expose of the replication police..."&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/BrianNosek/status/470563826274807808"&gt;@BrianNosek: "A remarkable set of public comments..."&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/DavidFunder/status/470316176627613696"&gt;@DavidFunder: "3 terms I learned in past 24 hrs..."&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Open Projects - Open Humans</title><link href="http://osc.centerforopenscience.org/2014/06/05/op-open-humans/" rel="alternate"></link><updated>2014-06-05T12:00:00-04:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-06-05:2014/06/05/op-open-humans/</id><summary type="html">&lt;p&gt;&lt;em&gt;This article is the second in &lt;a href="http://osc.centerforopenscience.org/tag/open-projects.html"&gt;a series highlighting open science projects&lt;/a&gt; around the community. You can read the interview this article was based on: &lt;a href="https://docs.google.com/document/d/1tQuOFme5EQbNkcGBCc1rr7FO9SWEEufPwhhJ59JXBUM/edit"&gt;edited for clarity&lt;/a&gt;, &lt;a href="https://docs.google.com/document/d/1c2xMBMEr5m8a3wR7Om5mPAUgsz_JBPudlYsnSEbH0lY/edit"&gt;unedited&lt;/a&gt;.&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;While many researchers encounter no privacy-based barriers to releasing data, those working with human participants, such as doctors, psychologists, and geneticists, have a difficult problem to surmount. How do they reconcile their desire to share data, allowing their analyses and conclusions to be verified, with the need to protect participant privacy? It's a dilemma we've talked about before on the blog (see: &lt;a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/"&gt;Open Data and IRBs&lt;/a&gt;, &lt;a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/"&gt;Privacy and Open Data&lt;/a&gt;). A new project, Open Humans, seeks to resolve the issue by finding patients who are willing - even eager - to share their personal data.  &lt;/p&gt;
&lt;p&gt;Open Humans, which recently won a $500,000 grant &lt;a href="http://blog.personalgenomes.org/2014/01/14/open-humans-network-wins-knight-news-challenge-health-award/"&gt;from the Knight Foundation&lt;/a&gt;, grew out of the &lt;a href="http://www.personalgenomes.org/"&gt;Personal Genome Project&lt;/a&gt;. Founded in 2005 by Harvard genetics professor &lt;a href="http://arep.med.harvard.edu/gmc/"&gt;George Church&lt;/a&gt;, the Personal Genome Project sought to solve a problem that many genetics researchers had yet to recognize. "At the time people didn't really see genomes as inherently identifiable," Madeleine Price Ball explains. Ball is co-founder of OpenHumans, Senior Research Scientist at PersonalGenomes.org, and Director of Research at the Harvard Personal Genome Project.  She quotes from &lt;a href="http://www.1000genomes.org/"&gt;1000 Genomes&lt;/a&gt;' &lt;a href="http://www.1000genomes.org/sites/1000genomes.org/files/docs/Informed%20Consent%20Form%20Template.pdf"&gt;informed consent form&lt;/a&gt;: "'Because of these measures, it will be very hard for anyone who looks at any of the scientific databases to know which information came from you, or even that any information in the scientific databases came from you.'"  &lt;/p&gt;
&lt;p&gt;"So that's sort of the attitude scientists had towards genomes at the time. Also, the Genetic Information Nondiscrimination Act didn't exist yet. And there was GATTACA. Privacy was still this thing everyone thought they could have, and genomes were this thing people thought would be crazy to share in an identifiable manner. I think the scientific community had a bit of unconscious blindness, because they couldn't imagine an alternative."  &lt;/p&gt;
&lt;p&gt;Church found an initial &lt;a href="https://en.wikipedia.org/wiki/Personal_Genome_Project"&gt;ten participants&lt;/a&gt; - the list includes university professors, health care professionals, and Church himself. The IRB interviewed each of the participants to make sure they truly understood the project and, satisfied, allowed it to move forward. The Personal Genome Project now boasts &lt;a href="https://my.pgp-hms.org/users/"&gt;over 3,400 participants&lt;/a&gt;, each of whom have passed an entrance exam showing that they understand what will happen to their data, and the risks involved. Most participants are enthusiastic about sharing. One participant described it as "donating my body to science, but I don't have to die first".   &lt;/p&gt;
&lt;p&gt;The Personal Genome Project's expansion hasn't been without growing pains. "We've started to try to collect data beyond genomes." Personal health information, including medical history, procedures, test results, prescriptions, has been provided by a subset of participants. "Every time one of these new studies was brought before the IRB they'd be like ‘what? that too?? I don't understand what are you doing???' It wasn't scaling, it was confusing, the PGP was trying to collect samples and sequence genomes &lt;em&gt;and&lt;/em&gt; it was trying to let other groups collect samples and do other things."  &lt;/p&gt;
&lt;p&gt;Thus, Open Humans was born. "Open Humans is an abstraction that takes part of what the PGP was doing (the second part) and make it scalable," Ball explains. "It's a cohort of participants that demonstrate an interest in public data sharing, and it's researchers that promise to return data to participants."  &lt;/p&gt;
&lt;p&gt;Open Humans will start out with a number of participants and an array of public data sets, thanks to collaborating projects &lt;a href="http://humanfoodproject.com/americangut/"&gt;American Gut&lt;/a&gt;, &lt;a href="https://flunearyou.org/"&gt;Flu Near You&lt;/a&gt;, and of course, the Harvard Personal Genome Project. Participants share data and, in return, researchers promise to share results. What precisely "sharing results" means has yet to be determined. "We're just starting out and know that figuring out how this will work is a learning process," Ball explains. But she's already seen what can happen when participants are brought into the research process - and brought together:  &lt;/p&gt;
&lt;p&gt;"One of the participants made an online forum, another a Facebook group, and another maintains a LinkedIn group… before this happened it hadn't occurred to me that abandoning the privacy-assurance model of research could empower participants in this manner. Think about the typical study - each participant is isolated, they never see each other. Meeting each other could breach confidentiality! Here they can talk to each other and &lt;em&gt;gasp&lt;/em&gt; complain about you. That's pretty empowering." Ball and her colleague Jason Bobe, Open Humans co-founder and Executive Director of PersonalGenomes.org, hope to see all sorts of collaborations between participants and researchers. Participants could help researchers refine and test protocols, catch errors, and even provide their own analyses.  &lt;/p&gt;
&lt;p&gt;Despite these dreams, Ball is keeping the project grounded. When asked whether Open Humans will require articles published using their datasets to be made open access, she replies that, "stacking up a bunch of ethical mandates can sometimes do more harm than good if it limits adoption". Asked about the effect of participant withdrawals on datasets and reproducibility, she responds, "I don't want to overthink it and implement things to protect researchers at the expense of participant autonomy based on just speculation." (It &lt;em&gt;is&lt;/em&gt; mostly speculation. Less than 1% of Personal Genome Project users have withdrawn from the study, and none of the participants who've provided whole genome or exome data have done so.)  &lt;/p&gt;
&lt;p&gt;It's clear that Open Humans is focused on the road directly ahead. And what does that road look like?  "Immediately, my biggest concern is building our staff. Now that we won funding, we need to hire a good programmer... so if you are or know someone that seems like a perfect fit for us, please pass along &lt;a href="http://openhumans.org/#now_hiring"&gt;our hiring opportunities&lt;/a&gt;". She adds that anyone can &lt;a href="http://openhumans.org/"&gt;join the project's mailing list&lt;/a&gt; to get updates and find out when Open Humans is open to new participants - and new researchers. "And just talk about us. Referring to us is an intangible but important aspect for helping promote awareness of participant-mediated data sharing as a participatory research method and as a method for creating open data."  &lt;/p&gt;
&lt;p&gt;In other words: start spreading the news.  Participant mediated data isn't the only solution to privacy issues, but it's an enticing one - and the more people who embrace it, the better a solution it will be.  &lt;/p&gt;</summary><category term="open-projects"></category></entry><entry><title>Questions and Answers about the Förster case</title><link href="http://osc.centerforopenscience.org/2014/05/29/forster-case/" rel="alternate"></link><updated>2014-05-29T15:30:00-04:00</updated><author><name>Denny Borsboom, Han van der Maas, Eric-Jan Wagenmakers, Department of Psychological Methods, UvA</name></author><id>tag:osc.centerforopenscience.org,2014-05-29:2014/05/29/forster-case/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;By now, everyone is probably familiar with the recent investigation of the work of Dr. Förster, in which the Landelijk Orgaan Wetenschappelijke Integriteit (LOWI) concluded that data reported in a paper by Dr. Förster had been manipulated. In his reaction to the newspaper article NRC Dr. Förster suggested that our department would be involved in a witch-hunt. This is incorrect.  &lt;/p&gt;
&lt;p&gt;However, we have noticed that there are many questions about both the nature of the case and the procedure followed. We have compiled the following list of questions and answers to explain what happened. If any other questions arise, feel free to email them to us so we can add them to this document.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: What was the basis of the allegations against Dr. Förster?&lt;/strong&gt;&lt;br /&gt;
A: In every single one of 40 experiments, reported across three papers, the means of two experimental conditions (“local focus” and “global focus”) showed almost exactly opposite behavior with respect to the control condition. So whenever the local focus condition led to a one-point increase of the mean level of the dependent variable compared to the control condition, the global condition led almost exactly to a one-point decrease. Thus, the samples exhibit an unrealistic level of linearity.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Couldn’t the effects actually be linear in reality?&lt;/strong&gt;&lt;br /&gt;
A: Yes, that is unlikely but possible. However, in addition to the perfect linearity of the effects themselves, there is far too little variance in the means of the conditions, given the variance that is present within the conditions. In other words: the means across the conditions follow the linear pattern (much) too perfectly. To show this, the whistleblower’s complaint computed the probability of finding this level of linearity (or even more perfect linearity) in the samples researched, under the assumption that, in reality, the effect is linear in the population. That probability equals 1/508,000,000,000,000,000,000.   &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;&lt;strong&gt;Q: But that’s just a probability. That something is improbable doesn’t mean it’s impossible, right?&lt;/strong&gt;&lt;br /&gt;
A: That is correct. However, the probability of this happening is so small that it justified an investigation: it is roughly equivalent to winning the Dutch lottery on five consecutive drawings. That is also possible in theory, but it would certainly justify an investigation. Also, note that the probability is computed under the most favorable scenario for Dr. Förster: namely, the computation assumes that the linear version of Dr. Förster’s theory is perfectly true, and shows that under this assumption the effects are very unlikely. In reality, that linear version of the theory is highly unlikely to be true; at best, we could expect some ordinal theory to hold. That would lead the probability in question to become at least an order of magnitude lower. It is not an overstatement to say that, given realistic assumptions, Dr. Förster’s results are equivalent to winning the lottery on dozens of successive drawings. The datacolada website provides accessible illustrations of how unrealistic these effects really are.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: What about the statistical validity of the whistleblower’s method?&lt;/strong&gt;&lt;br /&gt;
A: By now, the whistleblower's method has been thoroughly checked and stands undisputed. In his May 11 response, Dr. Förster lists some questions about the method but provides no compelling rebuttal. No one has proposed a plausible QRP or set of QRPs that could explain the results. Note that exactly the same patterns have been detected in 40 experiments in three papers, two of which single-authored by Dr. Förster. Different statistical methods may be applied, but, in our opinion, all of them will inevitably lead to conclusions that are qualitatively identical to those drawn by the LOWI.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: So what happened in the investigation?&lt;/strong&gt;&lt;br /&gt;
A: The University of Amsterdam investigated the case for roughly a year. The relevant committee concluded that the data could not possibly have arisen from actual sampling, as stated in Förster’s papers, and thus endorsed the main conclusion of the whistleblower’s report. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: But I thought the University of Amsterdam concluded there was nothing wrong with the papers?&lt;/strong&gt;&lt;br /&gt;
A: That is not correct. The University of Amsterdam committee endorsed the conclusion of the whistleblower that the data could not have arisen from standard sampling. However, although the committee considered the data to be practically impossible, they also concluded that data manipulation could not be considered proven, because the statistical expert they consulted said that the data might have arisen as a result of the use of Questionable Research Practices (QRPs). Thus, the committee stated they could not conclude that data manipulation had taken place, because they could not rule out the alternative explanation that Dr. Förster had engaged in QRPs. However, Dr. Förster denied having used these practices. Moreover, the whistleblower argued that no known set of QRPs, however excessively used, could ever lead to the patterns in Dr. Förster’s data, and took his complaint to the Landelijk Orgaan Wetenschappelijke Integriteit (LOWI). The LOWI agreed with the whistleblower and considered data manipulation proven.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: So this verdict was reached entirely on the basis of the analysis of the whistleblower?&lt;/strong&gt;&lt;br /&gt;
A: No. During the investigation, the statistical expert consulted by LOWI had access to data files provided by Dr. Förster. In analyzing these, the expert discovered that the linearity of the manipulation did not show up within males or females separately. However, whenever the males deviated from linearity in the positive direction, the females showed a precisely equivalent deviation from linearity in the negative direction, so that the resulting overall means were perfectly linear again. It is hard to imagine how any actual experiment could have produced such an anomalous result.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: OK, so that’s a lot of statistical work. But was the verdict then reached entirely on the basis of a statistical calculation?&lt;/strong&gt;&lt;br /&gt;
A: No. Dr. Förster proved unable to give basic details of where the data came from, who gathered them, or when they were gathered. All verifiable traces of the data gathering process had evaporated as a result of dumping of raw data to win office space, and lapses of Dr. Förster’s memory. The combination of the practical impossibility of the results, and Dr. Förster’s inability to provide any details about the origin of the data, led to the LOWI’s verdict that the data were manipulated and that scientific integrity was violated..  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Does that mean Förster has been found guilty?&lt;/strong&gt;&lt;br /&gt;
A: The data have been manipulated, but the committee does not know by whom. However, the LOWI concludes that Dr. Förster, as lead researcher and first author, is nevertheless responsible. LOWI excluded the possibility that the many research assistants, who were involved in the data collection in these experiments and were typically unaware of the research hypotheses, could have created such exceptional statistical relations.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: But didn’t the LOWI investigate only one paper while the whistleblower’s report mentioned three studies?&lt;/strong&gt;&lt;br /&gt;
A: Yes, LOWI investigated only the Förster and Denzler (2012) paper. Two other papers featured in the original complaint, single-authored by Förster, were not investigated.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Does that mean that the data of the other two papers were judged to be trustworthy?&lt;/strong&gt;&lt;br /&gt;
A: No. They were simply not investigated. We do not know why LOWI chose to investigate only one paper, but there does not appear to be a good reason why the verdict for the other two papers would be different.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Do you know who the whistleblower is?&lt;/strong&gt;&lt;br /&gt;
A: Yes. But because the whistleblower has asked to remain anonymous (which is the legal privilege of any whistleblower), we will not give information that would lead to his or her identification. Also, we cannot react to the accusation voiced by Dr. Förster in his letter to Retraction Watch, in which he stated that the case originated in our department. Note that this does not mean that Dr. Förster’s statement is correct.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Did you or the whistleblower leak the information to the press that led to the preliminary exposure of the case in NRC Handelsblad?&lt;/strong&gt;&lt;br /&gt;
A: No.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: In his letter to Retraction Watch, Dr. Förster says he is the victim of a witch-hunt. What do you think about that?&lt;/strong&gt;&lt;br /&gt;
A: A witch-hunt could be defined as an organized systematic attempt to incriminate researchers on the basis of vague hints and weak circumstantial evidence for misconduct. In our view, the patterns in Dr. Förster’s data clearly and directly pointed to irregularities in the process of data gathering and data analysis. Thus, these patterns were not vague or circumstantial, and were sufficiently implausible to justify an investigation. As far as we know, the investigation stood on its own and was never part of a larger investigative process. In fact, we do not know of any other investigations into misconduct that ran while the case of Dr. Förster was being investigated. Also, we do not know of any investigations into misconduct that are currently taking place, and we do not know of any systematically attempts to search the literature for evidence of fraud. Hence, as far as we know, the investigation of Dr. Förster’s work was an isolated investigation that closely followed the official procedures as laid down in the integrity policies of the University of Amsterdam.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: What needs to happen now?&lt;/strong&gt;&lt;br /&gt;
A: We recommend further investigation of the work of Dr. Förster to determine whether other papers show similar improbable data patterns that are too good to be true.  &lt;/p&gt;</summary></entry><entry><title>The etiquette of train wreck prevention</title><link href="http://osc.centerforopenscience.org/2014/05/28/train-wreck-prevention/" rel="alternate"></link><updated>2014-05-28T13:30:00-04:00</updated><author><name>JP de Ruiter, Bielefeld University</name></author><id>tag:osc.centerforopenscience.org,2014-05-28:2014/05/28/train-wreck-prevention/</id><summary type="html">&lt;p&gt;In a famous &lt;a href="http://www.nature.com/polopoly_fs/7.6716.1349271308!/suppinfoFile/Kahneman%20Letter.pdf"&gt;open letter&lt;/a&gt; to scientists , Daniel Kahneman, seeing “a train wreck looming”, argued that social psychologists (and presumably, especially those who are publishing social priming effects) should engage in systematic and extensive replication studies to avoid a loss of credibility in the field. The fact that a Nobel Prize winning psychologist made such a clear statement gave a strong boost of support to systematic replication efforts in social psychology (see Pashler &amp;amp; Wagenmakers 2012, and their special issue in &lt;em&gt;Psychological Science&lt;/em&gt;).  &lt;/p&gt;
&lt;p&gt;But in a more recent &lt;a href="http://www.scribd.com/doc/225285909/Kahneman-Commentary"&gt;commentary&lt;/a&gt;, Kahneman appears to have changed his mind, and argues that “current norms allow replicators too much freedom to define their study as a direct replication of previous research”, and that the “seemingly reasonable demand” of requiring method sections to be so precise that they enable direct replications is “rarely satisfied in psychology, because behavior is easily affected by seemingly irrelevant factors”. A similar argument was put forth by Simone Schnall, who recently &lt;a href="http://www.psychol.cam.ac.uk/cece/blog"&gt;wrote&lt;/a&gt; that “human nature is complex, and identical materials will not necessarily have the identical effect on all people in all contexts”.  &lt;/p&gt;
&lt;p&gt;While I wholeheartedly agree with Kahneman’s original letter on this topic, I strongly disagree with his commentary, for reasons that I will outline here.  &lt;/p&gt;
&lt;p&gt;First, he argues (as Schnall did too) that there always are potentially influential differences between the original study and the replication attempt. But this would imply that any replication study, no matter how meticulously performed, would be meaningless. (Note that this also holds for &lt;em&gt;successful&lt;/em&gt; replication studies.) This is a clear case of a &lt;em&gt;reductio ad absurdum&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;The main reason why this argument is flawed is that there is a fundamental relationship between the theoretical claim based on a finding and its proper replication, which is the topic of an interesting discussion about the degree to which a replication should be similar to the study it addresses (see Stroebe &amp;amp; Strack, 2014; Simons, 2014; Pashler &amp;amp; Harris, 2012). My position in this debate is the following. The more general the claim that the finding is claimed to support, the more “conceptual” the replication of the supporting findings can (and should) be. Suppose we have a finding F that we report in order to claim evidence for scientific claim C. In the case that C is &lt;em&gt;identical&lt;/em&gt; to F, such that C is a claim of the type “The participants in our experiment did X at time T in location L”, it is indeed impossible to do any type of replication study, because the exact circumstances of F were unique and therefore by definition irreproducible. But in this case (that F = C), C has obviously no generality at all, and is therefore scientifically not very interesting. In such a case, there would also be no point in doing inferential statistics. If, on the other hand, C is more general than F, the level of methodological detail that is provided should be sufficient to enable readers to attempt to replicate the finding, allowing for variation that the authors do not consider important. If the authors remark that this result arises under condition A but acknowledge that it might not arise under condition A' (let's say, with participants who are aged 21-24 rather than 18-21), then clearly a follow-up experiment under condition A' isn't a valid replication. But if their claim (explicitly or implicitly) is that it doesn't matter whether condition A or A' is in effect, then a follow-up study involving condition A' might well be considered a replication. The failure to specify any particular detail might reasonably be considered an implicit claim that this detail is not important.  &lt;/p&gt;
&lt;p&gt;Second, Kahnemann is worried that even the rumor of a failed replication could damage the reputation of the original authors. But if researchers attempt to do a replication study, this does not imply that they believe or suggest that the original author was cheating. Cheating does occasionally happen, sadly, and replication studies are a good way to catch these cases. But, assuming that cheating is not completely rampant, it is much more likely that a finding cannot be replicated successfully because variables or interactions have been overlooked or not controlled for, that there were unintentional errors in the data collection or analysis, or because the results were simply a fluke, caused by our standard statistical practices severely overestimating evidence against the null hypothesis (Sellke, Bayarri &amp;amp; Berger, 2001; Johnson, 2013).  &lt;/p&gt;
&lt;p&gt;Furthermore, replication studies are not hostile or friendly. People are. I think it is safe to say that we all dislike uncollegial behavior and rudeness, and we all agree that it should be avoided. If Kahneman wants to give us a stern reminder that it is important for replicators to contact the original authors, then I support that, even though I personally suspect that the vast majority of replicators already do that. There already is etiquette in place in experimental psychology, and as far as I can tell, it’s mostly being observed. And for those cases where it is not, my impression is that the occasional unpleasant behavior originates not only from replicators, but also from original authors.  &lt;/p&gt;
&lt;p&gt;Another point I would like to address is the asymmetry of the relationship between author and replicator. Kahneman writes: “The relationship is also radically asymmetric: the replicator is in the offense, the author plays defense.” This may be true in some sense, but it is counteracted by other asymmetries that work in the opposite direction: The author has already successfully published the finding in question and is reaping the benefits of it. The replicator, however, is up against the strong reluctance of journals to publish replication studies, is required to have a much higher statistical power (hence invest far more resources), and is often arguing against a moving target, as more and more newly emerging and potentially relevant details of the original study can be brought forward by the original authors.  &lt;/p&gt;
&lt;p&gt;A final point: the problem that started the present replication discussion was that a number of findings that were deemed both important and implausible by many researchers failed to replicate. The defensiveness of the original authors of these findings is understandable, but so is the desire of skeptics to investigate if these effects are in fact reliable. I, both as a scientist and as a human being, &lt;em&gt;really want to know&lt;/em&gt; if I can boost my creativity by putting an open box on my desk (Leung et al., 2012) or if the fact that I frequently take hot showers could be caused by loneliness (Bargh &amp;amp; Shalev, 2012). As Kahneman himself rightly put it in his original open letter: “The unusually high openness to scrutiny may be annoying and even offensive, but it is a small price to pay for the big prize of restored credibility.”  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bargh, J. A., &amp;amp; Shalev, I. (2012). The substitutability of physical and social warmth in daily life. &lt;em&gt;Emotion&lt;/em&gt;, 12(1), 154. doi:10.1037/a0023527&lt;/p&gt;
&lt;p&gt;Johnson, V. E. (2013). Revised standards for statistical evidence. &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, 110(48), 19313-19317. doi: doi/10.1073/pnas.1313476110&lt;/p&gt;
&lt;p&gt;Leung, A. K.-y., Kim, S., Polman, E., Ong, L. S., Qiu, L., Goncalo, J. A., et al. (2012). Embodied metaphors and creative "acts". &lt;em&gt;Psychological Science&lt;/em&gt;, 23(5), 502-509. doi:10.1177/0956797611429801&lt;/p&gt;
&lt;p&gt;Pashler, H., &amp;amp; Harris, C. R. (2012). Is the replicability crisis overblown? Three arguments examined. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 7(6), 531-536. doi:10.1177/1745691612463401&lt;/p&gt;
&lt;p&gt;Pashler, H., &amp;amp; Wagenmakers, E.-J. (2012). Editors' Introduction to the Special Section on Replicability in Psychological Science A Crisis of Confidence? &lt;em&gt;Perspectives on Psychological 
Science&lt;/em&gt;, 7(6), 528-530. doi:10.1177/1745691612465253&lt;/p&gt;
&lt;p&gt;Sellke, T., Bayarri, M., &amp;amp; Berger, J. O. (2001). Calibration of p values for testing precise null hypotheses. &lt;em&gt;The American Statistician&lt;/em&gt;, 55(1), 62-71. doi:10.1198/000313001300339950&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The Value of Direct Replication. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 9(1), 76-80. doi:10.1177/1745691613514755&lt;/p&gt;
&lt;p&gt;Stroebe, W., &amp;amp; Strack, F. (2014). The alleged crisis and the illusion of exact replication. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 9(1), 59-71. doi:10.1177/1745691613514450 &lt;/p&gt;</summary></entry><entry><title>Support Publication of Clinical Trials for International Clinical Trials Day</title><link href="http://osc.centerforopenscience.org/2014/05/20/clinical-trials-day/" rel="alternate"></link><updated>2014-05-20T11:00:00-04:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-05-20:2014/05/20/clinical-trials-day/</id><summary type="html">&lt;p&gt;Today is International Clinical Trials Day, held on May 20th in honor of George Lind, the famous Scottish physician who began one of the world's first clinical trials on May 20th, 1747.  This trial discovered that vitamin C deficiency was the cause of scurvy.  While it and the other life-saving trials that have been conducted in the last two hundred and sixty seven years are surely worth celebration, International Clinical Trials Day is also a time to reflect on the problems that plague the clinical trials system.  In particular, the lack of reporting on nearly half of all clinical trials has potentially deadly consequences.&lt;/p&gt;
&lt;p&gt;The AllTrials campaign, launched in January 2013, aims to have all past and present clinical trials registered and reported.  From the AllTrials campaign website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Doctors and regulators need the results of clinical trials to make informed decisions about treatments.&lt;/p&gt;
&lt;p&gt;But companies and researchers can withhold the results of clinical trials even when asked for them. The best available evidence shows that about half of all clinical trials have never been published, and trials with negative results about a treatment are much more likely to be brushed under the carpet.&lt;/p&gt;
&lt;p&gt;This is a serious problem for evidence based medicine because we need all the evidence about a treatment to understand its risks and benefits. If you tossed a coin 50 times, but only shared the outcome when it came up heads and you didn’t tell people how many times you had tossed it, you could make it look as if your coin always came up heads. This is very similar to the absurd situation that we permit in medicine, a situation that distorts the evidence and exposes patients to unnecessary risk that the wrong treatment may be prescribed.&lt;/p&gt;
&lt;p&gt;It also affects some very expensive drugs. Governments around the world have spent billions on a drug called Tamiflu: the UK alone spent £500 million on this one drug in 2009, which is 5% of the total £10bn NHS drugs budget. But Roche, the drug’s manufacturer, published fewer than half of the clinical trials conducted on it, and continues to withhold important information about these trials from doctors and researchers. So we don’t know if Tamiflu is any better than paracetamol.  (&lt;em&gt;Author's note: in April 2014 &lt;a href="http://www.cochrane.org/features/tamiflu-relenza-how-effective-are-they"&gt;a review based on full clinical trial data&lt;/a&gt; determined that Tamiflu was almost entirely ineffective.&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Initiatives have been introduced to try to fix this problem, but they have all failed. Since 2008 in the US the FDA has required results of all trials to be posted within a year of completion of the trial. However an audit published in 2012 has shown that 80% of trials failed to comply with this law. Despite this fact, no fines have ever been issued for non-compliance. In any case, since most currently used drugs came on the market before 2008, the trial results that are most important for current medical practice would not have been released even if the FDA’s law was fully enforced.&lt;/p&gt;
&lt;p&gt;We believe that this situation cannot go on. The AllTrials initiative is campaigning for the publication of the results (that is, full clinical study reports) from all clinical trials – past, present and future – on all treatments currently being used.&lt;/p&gt;
&lt;p&gt;We are calling on governments, regulators and research bodies to implement measure to achieve this.&lt;/p&gt;
&lt;p&gt;And we are calling for all universities, ethics committees and medical bodies to enact a change of culture, recognise that underreporting of trials is misconduct and police their own members to ensure compliance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can learn more about the problem of missing clinical trial data in &lt;a href="http://www.alltrials.net/wp-content/uploads/2013/01/Missing-trials-briefing-note.pdf"&gt;this brief&lt;/a&gt;.  AllTrials also provides slides on this issue to incorporate into talks and presentations as well as a &lt;a href="http://www.alltrials.net/2013/get-involved/"&gt;petition you can sign&lt;/a&gt;.&lt;/p&gt;</summary></entry><entry><title>How anonymous peer review fails to do its job and damages science.</title><link href="http://osc.centerforopenscience.org/2014/05/15/anonymous-peer-review/" rel="alternate"></link><updated>2014-05-15T11:30:00-04:00</updated><author><name>JP de Ruiter, Bielefeld University</name></author><id>tag:osc.centerforopenscience.org,2014-05-15:2014/05/15/anonymous-peer-review/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;Churchill believed that democracy was the “worst form of government except all those other forms that have been tried from time to time.” Something analogous is often said about anonymous peer review (APR) in science: “it may have its flaws, but it’s the ‘least bad’ of all possible systems.” In this contribution, I present some arguments to the contrary. I believe that APR is threatening scientific progress, and therefore that it urgently needs to be fixed.  &lt;/p&gt;
&lt;p&gt;The reason we have a review system in the first place is to uphold basic standards of scientific quality. The two main goals of a review system are to minimize both the number of bad studies that are accepted for publication and the number of good studies that are rejected for publication. Borrowing terminology of signal detection theory, let’s call these false positives and false negatives respectively.  &lt;/p&gt;
&lt;p&gt;It is often implicitly assumed that minimizing the number of false positives is the primary goal of APR. However, signal detection theory tells us that reducing the number of false positives inevitably leads to an increase in the rate of false negatives. I want to draw attention here to the fact that the cost of false negatives is both invisible and potentially very high. It is invisible, obviously, because we never get to see the good work that was rejected for the wrong reasons. And the cost is high, because it removes not only good papers from our scientific discourse, but also entire scientists. I personally know a number of very talented and promising young scientists who first sent their work to a journal, fully expecting to be scrutinized, but then receiving reviews that were so personal, rude, scathing, and above all, unfair, that they decided to look for another profession and never looked back. I also know a large number of talented young scientists who are still in the game, but who suffer intensely every time they attempt to publish something and get trashed by anonymous reviewers. I would not be surprised if they also leave academia soon. The inherent conservatism in APR means that people with new, original approaches to old problems run the risk of being shut out, humiliated, and consequently chased away from academia. In the short term, this is to the advantage of the established scientists who do not like their work to be challenged. In the long run, this is obviously very damaging for science. This is especially true of the many journals that will only accept papers that receive unanimously positive reviews. These journals are not facilitating scientific progress, because work with even the faintest hint of controversy is almost automatically rejected.  &lt;/p&gt;
&lt;p&gt;With all this in mind, it is somewhat surprising that APR &lt;em&gt;also&lt;/em&gt; fails to keep out many obviously bad papers.  &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;There are many problems with APR, but I suspect the two biggest reasons for its failure are:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reviewers have little time (note that they are not paid for their work) leading to sloppy and superficial reviews.  &lt;/li&gt;
&lt;li&gt;Reviewers often do not understand the difference between studies with results that they don’t like and studies that have scientific flaws. This effectively leads to reviewers using their power to eliminate or hamper competitors.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is important to realize that if we look at peer review as a strategic game, rejecting everything is a strong strategy, as this will always reduce the influence of the (reviewer’s) competition. This strategy, if generally applied, would be catastrophic for scientific progress. Indeed, I know of at least one scientific field where grant reviewers nearly always mutually reject the proposals of their peers, to the bemusement and advantage of other, more cooperative fields, who then have more funding available.  &lt;/p&gt;
&lt;p&gt;I believe that the most fundamental problem in APR is accountability. Reviewers can basically say whatever they want, because they are protected by anonymity. To illustrate, here are a few comments (used to justify outright rejections) that I have received over the years. Note that I’m not saying that these papers didn’t deserve to be rejected; only that these specific arguments to reject them are not valid and/or not fair.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These empirical results are wrong, because they contradict influential theory X.&lt;/li&gt;
&lt;li&gt;Those experimental subjects must have been doing it on purpose.&lt;/li&gt;
&lt;li&gt;This work is only interesting for cocktail parties at the Max Planck Institute.&lt;/li&gt;
&lt;li&gt;The phenomenon that you have studied actually doesn’t exist. &lt;/li&gt;
&lt;li&gt;I cannot see any flaws in these experiments, and the author has addressed all my concerns, but I have to say I still just don’t like this study.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I suspect we all have a personal top ten list of these types of reviewer statements. But would these reviewers have written those things if they had been identified by name, and if their reviews had been accessible to the scientific community at large? Do we find arguments of this sort in open peer reviews, like for instance the commentaries in &lt;em&gt;Behavioral &amp;amp; Brain Sciences&lt;/em&gt;? Of course not. It is anonymity that enables reviewers to write comments like that. It is dysfunctional APR culture that allows reviewers to then persuade editors (who, let’s not kid ourselves, don’t have the time to read and check everything in detail) to reject papers on such weak grounds. If these types of argument were used to reject the first paper by a young scientist who submitted a dissertation project, for example, it would be very demotivating and unfair.  &lt;/p&gt;
&lt;p&gt;The party line on why we need anonymity for reviewers is exemplified by the scenario of a young, untenured scientist being asked to review the work of a famous, politically powerful peer. If these young scientists were not anonymous, so the argument goes, they could not freely criticize the work of more famous scientists without fearing consequences for their career. Good point. Note, however, that if we assume that those famous scientists are potentially vengeful towards a young reviewer who “wronged” them, we also have to assume that they would be equally unpleasant when reviewing work by younger scientists that criticized their own. So we employ APR to protect the young scientists from the wrath of their more famous colleagues, but by the same token we allow the famous scientists to block the work of the younger colleagues whenever they find it unpalatable, and, moreover, to do so anonymously. This is a clear case of well-intentioned social engineering backfiring in a disastrous way.  &lt;/p&gt;
&lt;p&gt;There are ways of improving this situation considerably. I am specifically thinking of two rules. The first one I have implemented for myself personally, and I urge others to do so as well. The second one I’ve tried to sell to journal editors, but with very little success so far.&lt;/p&gt;
&lt;p&gt;Rule a) Reviewers with tenure always sign their reviews.  &lt;/p&gt;
&lt;p&gt;Rule b) Reviews are stored, and all researchers have the explicit right to look up and cite reviews. If the author of a certain review is anonymous, so be it. Call them “reviewer 3 in submission so-and-so to journal X”, but at least this allows researchers to address and discuss their &lt;em&gt;arguments&lt;/em&gt; in the papers. I often notice that reviewers have a very strong influence on papers, by requesting that certain points be addressed before they advise acceptance. This epistemic tug-of-war between reviewers and authors often results in needless meandering and bad rhetorical flow.  &lt;/p&gt;
&lt;p&gt;Under the current system, the very arguments used by gatekeepers to decide whether a paper lives or dies, and the influence reviewers have on the paper’s content in case it lives, are unaccountable, unknown, and therefore not open to scrutiny. Having reviews stored, accessible, and addressable would make gatekeepers officially part of the discourse. This would reduce both the false positives and the false negatives, because reviewers who reject good papers and reviewers who accept bad papers, for whatever reason, can be held accountable for what they write. Also, on a more positive note, reviewers would get more credit for their work. Under the current system, the difference between being a constructive reviewer and a careless one is invisible to all except journal editors.  &lt;/p&gt;
&lt;p&gt;Having a review system is probably necessary to keep our standards high. What we need to change is reviewers’ accountability for what they write; we need their arguments to become an integral part of the scientific discourse. As long as reviewers are shielded by anonymity, and their arguments hidden in editors’ file drawers, the review system is highly vulnerable to incompetence, abuse of power, and corruption.  &lt;/p&gt;</summary></entry><entry><title>When Science Selects for Fraud</title><link href="http://osc.centerforopenscience.org/2014/05/07/selecting-for-fraud/" rel="alternate"></link><updated>2014-05-07T12:30:00-04:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-05-07:2014/05/07/selecting-for-fraud/</id><summary type="html">&lt;p&gt;&lt;em&gt;This post is in response to &lt;a href="http://osc.centerforopenscience.org/2014/05/02/avoiding-a-witch-hunt/"&gt;Jon Grahe's recent article&lt;/a&gt; in which he invited readers to propose metaphors that might help us understand why fraud occurs and how to prevent it.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Natural selection is the process by which populations change as individual organisms succeed or fail to adapt to their environments.  It is also an apt metaphor for how human cultures form and thrive.  The scientific community, broadly speaking, selects for a number of personality traits, and those traits are more common among scientists than in the general population.  In some cases, this is necessary and beneficial.  In other cases, it is tragic.  &lt;/p&gt;
&lt;p&gt;The scientific community selects for curiosity.  Not every scientist is driven by a deep desire to understand the natural world, but so many are.  How boring would endless conferences, lab meetings, and lectures be if one didn’t delight in asking questions and figuring out answers.  It also selects for a certain kind of analytical thinking.  Those who can spot a confound or design a carefully controlled experiment are more likely to succeed.  And it selects for perseverance.  Just ask the researchers who work late into the night running gels, observing mice, or analyzing data.  &lt;/p&gt;
&lt;p&gt;The scientific community, like the broader culture of which it is a part, sometimes selects unjustly.  It selects for the well-off: those who can afford the kind of schools where a love of science is cultivated rather than ignored or squashed, those who can volunteer in labs because they don’t need to work to support themselves and others, those who can pay $30 to read a journal article.  It selects for white men: those who don’t have to face conscious and unconscious discrimination, cultural stereotyping, and microaggressions.  &lt;/p&gt;
&lt;p&gt;Of particular relevance right now is the way the scientific community selects for fraud.  If asked, most scientists would say that the ideal scientist is honest, open-minded, and able to accept being wrong.  But we do not directly reward these attributes.  Instead, success - publication of papers, grant funding, academic positions and tenure, the approbation of our peers - is too often based on a specific kind of result.  We reward those who can produce novel and positive results.  We don’t reward based on how they produce them.  &lt;/p&gt;
&lt;p&gt;This does give an advantage to those with good scientific intuitions, which is a reasonable thing to select for.  It also gives an advantage to risk-takers, those willing to risk their careers on being right.  The risk averse?  They have two options: to drop out of scientific research, as I did, or to commit fraud in order to ensure positive results, as Diederik Stapel, Marc Hauser and Jens Foster did.  Among the risk-averse, those who are unwilling to do shoddy or unethical science are selected against.  Those who are willing are selected for, and often reach the tops of their fields.  &lt;/p&gt;
&lt;p&gt;One of the more famous examples of natural selection is the peppered moth of England.  Before the Industrial Revolution, these moths were lightly colored, allowing them to blend in with the light gray bark of the average tree.  During the Industrial Revolution, extreme pollution painted the trees of England black with soot.  To adapt, peppered moths evolved dark, soot-colored wings.  &lt;/p&gt;
&lt;p&gt;We can censure the individuals who commit fraud, but this is like punishing the peppered moth for its dirty wings.  As long as success in the scientific community is measured by results and not process, we will continue to select for those willing to violate process in order to ensure results.  Our species, the scientists, need to change our environment if we want to evolve past fraud.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/moth.jpg" alt="Photo of Jon Grahe" align="center" width="600px" /&gt;
&lt;a href="https://www.flickr.com/photos/dhobern/7522055588/in/photolist-bwwkzi-a2zqz8-a2CkHA-4RecUF-6uwtBr-a9tjNF-dyGUvV-6Mhm2U-2tw4Qf-cTZ46w-fzqdc-fzqdj-9QBTbx-f5YrkT-csGu1y-csGv4A-bV36Wk-c4U7JS-fpNHoh"&gt;Biston betularia by Donald Hobern&lt;/a&gt;, CC BY 2.0&lt;/p&gt;</summary></entry><entry><title>Avoiding a Witch Hunt: What is the Next Phase of our Scientific Inquisition?</title><link href="http://osc.centerforopenscience.org/2014/05/02/avoiding-a-witch-hunt/" rel="alternate"></link><updated>2014-05-02T17:00:00-04:00</updated><author><name>Jon Grahe</name></author><id>tag:osc.centerforopenscience.org,2014-05-02:2014/05/02/avoiding-a-witch-hunt/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;Earlier this week, I learned about another case of fraud in psychological science (&lt;a href="http://retractionwatch.com/2014/04/29/new-dutch-psychology-scandal-inquiry-cites-data-manipulation-calls-for-retraction/"&gt;Retraction Watch, 4.29.2014&lt;/a&gt;). The conclusions from the evidence in the case against him after an extended investigation are hard to ignore. The probability that the findings could have occurred by chance are so minute that it is hard to believe that they didn’t result from falsified data. In an email to the scientific community (&lt;a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/"&gt;Retraction Watch, 4.30.2014&lt;/a&gt;), the target of this investigation strongly asserted that he never faked any data, while assuring us that the coauthor target never worked on the data, it was all his. Some comments from the Retraction Watch post use the term “witch hunt.” It was the first term I used in response as well, suggesting caution before judgment. A colleague pointed out that the difference was that there were no witches, and that there are clearly dishonest scientists. I have no choice but to agree; I think a better analogy is that of the Inquisition. We are entering the era of the Scientific Inquisition. A body of experts (LOWI in this case) will use a battery of sophisticated tools to examine the likelihood that the findings’ irregularities occurred by chance. In this case it is hard to believe his denial, but thankfully I am not a judge in the Scientific Inquisition.  &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;It was suggested that I should consider other similar inquiries, such as doping scandals; but I prefer the Inquisition. I invite the reader to consider other metaphors to guide us through this phase of our scientific correction. However, I would like to avoid scientific witch hunts where people are prosecuted and eventually terminated (professionally) even though they are innocent. Therefore, I think it important to consider the Inquisition as a metaphor. For a brief description, visit the &lt;a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/"&gt;Wikipedia entry&lt;/a&gt;. For our analogy, consider that there were actually a series of inquisitions. They started as trials to deter increasing secularism and heresy against the Catholic church in the middle age in the late 1100s. Given that my life is guided by science as much as a believer’s life might be guided by Catholicism or some other religion, I can think of no heresy more offensive than falsifying data and reports. While we are not nearly as centralized as the Catholic church, the only way to find academic heretics is through an inquiry such as the one used in this case. A long detailed report using many statistical procedures to determine likelihood of fake data, a series of meetings and hearings ending with committee recommendations. The major difference I see is the absence of torture, although maybe that was included and not reported.&lt;/p&gt;
&lt;p&gt;If Phase I of our Scientific Correction was and continues to be recognizing the problem of publication bias and fraud, Phase II is the beginning of the correction. I will not waste space discussing the structural problems that lead to academic dishonesty as there are many voices that have already discussed them. Instead, I would like to consider Phase II: Beginning of Correction. Here again, I will ignore some very positive next steps such as changes in publication expectations, crowdsourcing science projects, and computer systems that make openness easier as we reach toward the process of bringing forth Scientific Utopia as envisions by Nosek and colleagues (&lt;a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/"&gt;Nosek &amp;amp; Bar Anan, 2012&lt;/a&gt;; &lt;a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/"&gt;Nosek, Spies, and Moytl, 2012&lt;/a&gt;). Changes in publications are announced by the journals and societies embracing them, and there are some early indications that crowd-sourcing science projects are successfully replicating many, if not all, published effects in psychology (&lt;a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/"&gt;Yong, 2013&lt;/a&gt;). Naturally, there will be disappointments there as well, but that is topic for a future blog. Instead, consider the Scientific Inquisition portion of Phase II.  &lt;/p&gt;
&lt;p&gt;Consider the fate of transgressors against science. Are you one? Did you fake data to get published? Did you change some data points to get published, but not many? Did you use a priori tests when you were in fact HARKing (&lt;a href="http://retractionwatch.com/2014/04/30/social-psychologist-forster-denies-misconduct-calls-charge-terrible-misjudgment/"&gt;Kerr, 1998&lt;/a&gt;)? Did you actively p-hack? Did you fail to retract? What is the minimal level of violation that should have consequences during the Scientific Inquisition? I would imagine that a number of data fakers are huddling in their houses hoping to escape reality. Detecting scientific fraud is the hot new field and our Scientific Correction requires justice. What is the necessary criteria for preponderance of evidence to be found guilty by the scientific community or a host institution?  &lt;/p&gt;
&lt;p&gt;One we determine guilt, how should we sanction dishonest scientists? Job loss, retraction of degrees, and social ostracism from the the field all seem reasonable consequences for egregious violations. Are there more severe sanctions we could consider? Public shaming at a professional conference might be more impactful to ward off other offenses. Maybe an offender should be exiled not only from academia, but also from his or her neighborhood so as to avoid spreading horrible ethics with the good people who live nearby.   &lt;/p&gt;
&lt;p&gt;The title of this blog invokes witch hunts because the Inquisition morphed across a series of trials aiming to stamp out different types of heresy. Regardless of the reader’s personal religious convictions, one might forgive church leaders at a time before science was firmly rooted for believing that their enemy, the devil, had possessed their targets of accusation: the heretic. Wikipedia describes a series of inquisitions, responding early to secular movements and progressing eventually to the prosecution sorcery and witchcraft. Wikipedia also quotes a source that reminds us that the purpose of the inquisition was to create fear in the public so as to avoid further violations of church law.  &lt;/p&gt;
&lt;p&gt;We have not remotely approached this level of dysfunction in our Scientific Inquisition, but we should be cautious. This change from a real threat to the church (secularism) to imagery and fantastical threats such as sorcery and demonic possession occurred later after it became clear that the tools and outcomes of the inquisition became useful for political gain and personal agendas. In the end, the power was overwhelming and the victims didn’t even need to appear guilty anymore. The inquisitors had a direct line to God, and accusations were sufficient for conviction.  &lt;/p&gt;
&lt;p&gt;Scientists have for decades engaged in slippery statistical practices in order to succeed in a field of biased publication expectations and promotion rewards. How many stepped past the line and engaged in outright fraud? Some estimates are quite high. How many tenured and established researchers are going to be found guilty of academic fraud? What judge or jury will determine their fate? Will they be objective and unbiased as they sanction transgressors? Will the Scientific Inquisition be an independent organization or organizations developed to root out academic fraud?  &lt;/p&gt;
&lt;p&gt;Ideally, they should also have a direct line to truth. Well not yet; the Inquisition didn’t turn evil immediately. Sure the methods of inquisition were brutal, but not particularly for the time, and they got worse as the spectacles progressed. It took a little over 70 years before the torture as part of the inquiry was no longer considered a sin or a crime, as long as it was part of the inquisition. The metaphor is less clear at this point, for it is not clear what is the appropriate modern, scientific version of torture. Instead, I hope to caution us to be wary of the Scientific Inquisition; the determiners of truth. They will still be human, and the tools they will use can still be manipulated.  &lt;/p&gt;
&lt;p&gt;This case suggests fraud, not false accusation. But there will be more accusations and the preponderance of evidence will not always be as clear. In the US judicial system the burden of proof in criminal cases is on the prosecutor and the preponderance of evidence for guilt is “beyond a reasonable doubt”. However, the burden is much less rigorous in civil cases and much closer to “at least 51 % responsible”. In other judicial systems, the burden of proof is on the accused. What will be the burden of proof in the next phase of the Scientific Inquisition? Who will shoulder the burden of proof? We already know that I will not sit on this board; I don’t have the will to punish. But I would like to call for leniency; forgiveness; empathy.  &lt;/p&gt;
&lt;p&gt;The Catholic church went to war against heretics; academics have new tools to go to war against academic heretics. However, instead of progressing through stages and ending with witch hunts, follow the ideal of pardons. At the end of bloody revolutions, it is ideal when the victor pardons the loser. Let us welcome our academic heretics to bring in and lay down their arms. If they agree to point to papers where they committed fraud before they are discovered, offer a pardon. With new publishing standards that value scientific openness, there will be fewer transgressions. Instead of crafting torture tools, focus on training new scientists how to use public dispensaries to share their data and materials. Transgressors know they will live in shame, and might be stripped of their academic accolades. They lost their way, but we also know that social forces impact behavior. Now that the environment is prohibitive of future academic heresy, the same transgressors might become zealots of good science. It would be the best way to gain any respect from their angry peers. &lt;/p&gt;</summary></entry><entry><title>Memo From the Office of Open Science</title><link href="http://osc.centerforopenscience.org/2014/04/23/memo-from-the-office-of-open-science/" rel="alternate"></link><updated>2014-04-23T14:30:00-04:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-04-23:2014/04/23/memo-from-the-office-of-open-science/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;Dear Professor Lucky,&lt;/p&gt;
&lt;p&gt;Congratulations on your new position as assistant professor at Utopia University. We look forward to your joining our community and are eager to aid you in your transition from Antiquated Academy. It’s our understanding that Antiquated Academy does not have an Office of Open Science, so you may be unfamiliar with who we are and what we do.&lt;/p&gt;
&lt;p&gt;The Office of Open Science was created to provide faculty, staff and students with the technical, educational, social and logistical support they need to do their research openly. We recognize that the fast pace of research and the demands placed on scientists to be productive make it difficult to prioritize open science. We collaborate with researchers at all levels to make it easier to do this work.&lt;/p&gt;
&lt;p&gt;Listed below are some of the services we offer.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;&lt;strong&gt;Data Management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our office partners with the &lt;a href="https://osf.io/"&gt;Open Science Framework&lt;/a&gt; to help our community members conduct secure, accessible, version controlled research.  A member of our team runs a training every other Monday covering the basics of this technology. It’s important that anyone in your lab interacting with this system, including yourself, attend a training before starting to use it. The team also runs a mailing list where you can ask technical questions and receive answers from the general community.  &lt;/p&gt;
&lt;p&gt;If you work with human subjects data, patentable data, or other kinds of sensitive data, we will arrange a meeting with you, one of our team members, and a representative of the Internal Review Board and/or university legal team to discuss the appropriate access level for your data. If you work with large data sets (more than a half-terabyte of data generated per month) please inform us in advance.&lt;/p&gt;
&lt;p&gt;Note: University policy is that all data referenced in publications be hosted on the school’s OSF instance, and most departments require that all non-exploratory hypotheses be registered there as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparency&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Utopia University strongly supports open access. We require that copies of all articles and supplemental materials published during your time at Utopia be made available on the school’s website within six months of publication. We strongly encourage you to add previous research as well. A small team of professors and students from the law school run a monthly clinic through our office. If you have questions about your legal options, or would like help opening up previous research, please do contact them to schedule an appointment.&lt;/p&gt;
&lt;p&gt;We believe that all materials, not just papers, should be openly accessible both within our school community and outside of it. We promote the use of &lt;a href="http://figshare.com/"&gt;figshare&lt;/a&gt; as a tool for making such resources public and citable. To encourage use of this tool, our office automatically tracks citations and gives a monthly Impact Award for the resource with the most citations.  The process of uploading materials to figshare is one of several that our Open Science undergraduate interns are trained in; please contact us to request their help.&lt;/p&gt;
&lt;p&gt;A small group of researchers practicing &lt;a href="http://en.wikipedia.org/wiki/Open_notebook_science"&gt;Open Notebook Science&lt;/a&gt; have created a working group which meets regularly to share advice and resources. You are not required to adopt open notebook methodologies, but we recommend you check them out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Career and Educational Advancement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We recognize the unfortunate fact that pursuing open science can sometimes be at odds with career and educational advancement. To mitigate these cultural tendencies, a member of the office of open science takes part in all hiring and tenure discussions in science departments at Utopia University. Their role is to advocate for the consideration of openness of a candidate’s work over more traditional factors. If they feel that a candidate has been penalized for prioritizing openness, they are encouraged to file formal complaints.&lt;/p&gt;
&lt;p&gt;The office also provides travel grants for conferences and workshops related to open science.  We also work with a number of outside groups, such as &lt;a href="http://software-carpentry.org/"&gt;Software Carpentry&lt;/a&gt;, to provide additional methodological, technical and statistical training. To see our current schedule of workshops, you can visit the calendar on our website. If you are interested in hosting or running a new workshop, please contact us.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Community Engagement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have recently partnered with the journalism school to bring students majoring in science communications into our labs.  This typically takes the form of an “embedded reporter” attending lab meetings and interviewing lab members over the course of a semester or half-semester.  If you’re interested in participating in this program, please contact us no later than two weeks before the start of the semester.&lt;/p&gt;
&lt;p&gt;At the end of each semester, the Office of Open science hosts a day-long science fair for principal investigators, post-docs, graduate students, undergraduate students, staff members, and members of the public.  Entries are judged on their educational content and ability to engage as well as their scientific content.  The top three winners receive a cash prize and, if they are members of the public, institutional affiliation with Utopia University.&lt;/p&gt;
&lt;p&gt;For two years the Astronomy Department, with the help of organizations such as &lt;a href="http://science.nasa.gov/citizen-scientists/"&gt;NASA&lt;/a&gt; and &lt;a href="https://www.zooniverse.org/"&gt;Zooniverse&lt;/a&gt;, has run a regular event called “Citizen Science Saturdays”.  This event teaches local community members how to do astronomy research with low-cost, low-expertise tools and has been wildly successful, attracting an average of 50 community members per event and generating 6 publications to date. Departments across the university are in the process of starting similar event series in collaboration with projects such as &lt;a href="http://publiclab.org/"&gt;the Public Lab&lt;/a&gt; (environmental science), &lt;a href="http://diybio.org/"&gt;DIYbio&lt;/a&gt; (biology), &lt;a href="https://www.backyardbrains.com/"&gt;Backyard Brains&lt;/a&gt; (neuroscience), &lt;a href="http://www.sagebase.org/about-2/"&gt;Sage Bionetworks&lt;/a&gt; (medicine), &lt;a href="http://www.inaturalist.org/"&gt;iNaturalist&lt;/a&gt; (biodiversity), &lt;a href="http://openrov.com/"&gt;OpenROV&lt;/a&gt; (oceanography) and more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adherence&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Part of our office’s mandate is to identify and address potentially problematic patterns within the University.  To this end, our office keeps track of:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Replications&lt;/em&gt;:  University policy mandates a ratio of no less than one published replication per novel experiment, at both the university and the department level. Most departments require this ratio at the laboratory level as well. The Office of Open Science monitors replication rates to ensure adherence. Many departments now run courses on replication for undergraduates, for which members of the Office are often mentors and guest lecturers.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Results&lt;/em&gt;:  We monitor publication statistics across the university for unlikely distributions, for instance unusually high ratios of positive to null results. We publish these results to the community and, if they persist for a period of greater than six months, convene meetings to discuss their potential impact.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Retractions&lt;/em&gt;:  We monitor retractions and corrections across the scientific literature, with the help of groups such as &lt;a href="http://retractionwatch.com/"&gt;Retraction Watch&lt;/a&gt;. We coordinate with department administrators to make sure that individuals are aware of retractions and corrections which may impact their research.&lt;/p&gt;
&lt;p&gt;Community members may propose additional metrics to be tracked.&lt;/p&gt;
&lt;p&gt;*&lt;/p&gt;
&lt;p&gt;Open Science, like all science, is a constant process of improvement.  We welcome feedback about our existing services as well as suggestions for more we can do to support you.&lt;/p&gt;
&lt;p&gt;Congratulations again, and welcome to Utopia University!&lt;/p&gt;
&lt;p&gt;Professor Non Pareil&lt;br /&gt;
Director, Office of Open Science&lt;/p&gt;</summary></entry><entry><title>Expectations of replicability and variability in priming effects, Part II: When should we expect replication, how does this relate to variability, and what do we do when we fail to replicate?</title><link href="http://osc.centerforopenscience.org/2014/04/16/expectations-2/" rel="alternate"></link><updated>2014-04-16T12:00:00-04:00</updated><author><name>Joseph Cesario, Kai Jonas</name></author><id>tag:osc.centerforopenscience.org,2014-04-16:2014/04/16/expectations-2/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;&lt;em&gt;Continued from &lt;a href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/"&gt;Part 1&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now that some initial points and clarifications have been offered, we can move to the meat of the argument. Direct replication is essential to science. What does it mean to replicate an effect? All effects require a set of contingencies to be in place. To replicate an effect is to set up those same contingencies that were present in the initial investigation and observe the same effect, whereas to fail to replicate an effect is to set up those same contingencies and fail to observe the same effect. Putting aside what we mean by "same effect" (i.e., directional consistency versus magnitude), we don't see any way in which people can reasonably disagree on this point. This is a general point true of all domains of scientific inquiry.&lt;/p&gt;
&lt;p&gt;The real question becomes, &lt;em&gt;how can we know what contingencies produced the effect in the original investigation&lt;/em&gt;? Or more specifically, &lt;em&gt;how can we separate the important contingencies from the unimportant contingencies&lt;/em&gt;? There are innumerable contingencies present in a scientific investigation that are totally irrelevant to obtaining the effect: the brand of the light bulb in the room, the sock color of the experimenter, whether the participant got a haircut last Friday morning or Friday afternoon. Common sense can provide some guidance, but in the end &lt;em&gt;the theory used to explain the effect&lt;/em&gt; specifies the necessary contingencies and, by omission, the unnecessary contingencies. Therefore, if one is operating under the wrong theory, one might think some contingencies are important when really they are unimportant, and more interestingly, one might &lt;em&gt;miss&lt;/em&gt; some necessary contingencies because the theory did not mention them as being important.&lt;/p&gt;
&lt;p&gt;Before providing an example, it might be useful to note that, as far as we can tell, no one has offered any criticism of the logic outlined above. Many sarcastic comments have been made along the lines of, "apparently we can never learn anything because of all these mysterious moderators." And it is true that the argument can be misused to defend poor research practices. But at core, there is no criticism about the basic point that contingencies are necessary for all effects and a theory establishes those contingencies.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;As an example, consider some research showing social category priming resulting in behavioral effects, such as increased hostility following priming of &lt;em&gt;young black male&lt;/em&gt;. If one is operating under a classic spreading activation model then the theory dictates the following contingencies: a prime event, the association between the prime and the target behavior, and the opportunity to express the target behavior. This is the account given by Bargh et al. (1996) in their earliest article on the topic: the pictures activate the category, the associate &lt;em&gt;hostile&lt;/em&gt; is activated, and being provoked by the experimenter is the opportunity to express the behavior of hostility. &lt;em&gt;Under this model&lt;/em&gt;, if you provide those contingencies to participants then there is no reason to fail to obtain the effect, other than the effect not being real. Hence, it makes sense to question the original effect following a failure if you primed participants (who associate young black males with hostility&lt;sup&gt;2&lt;/sup&gt;) and provoked them. Note also that the theory tells you not only what is needed, but that &lt;em&gt;everything else is irrelevant to obtaining the effect&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But what if this model is wrong? If "direct expression" models are wrong, then there may be additional contingencies that are needed to produce the effect that were present in the original investigation but went unidentified by the experimenter/researcher. For instance, if you understand this priming effect not as a simple expression of the activated concept "hostile" but instead as a self-regulatory response to a physically formidable outgroup male, then you can look to the large literature on defensive threat regulation to identify many other contingencies that should influence the results. For example, presence of escape is a known moderator of threat responses in rodents, such that when escape is available rodents are more likely to flee, relative to when escape is not available and defensive attack is likely. The contingency, then, concerns the animal's ability to escape, and research in defensive threat regulation has found the same contingency to be important for human threat responses (e.g., Blanchard et al., 2001). When we manipulated this variable while priming &lt;em&gt;young black male&lt;/em&gt; (Cesario et al., 2010), we found flight responses to be more likely (when participants were in an open space) than fight responses (when participants were in a sound-attenuating booth).&lt;/p&gt;
&lt;p&gt;As another example, Macrae &amp;amp; Johnston (1998) studied the importance of competing goals on the effects of priming &lt;em&gt;helpfulness&lt;/em&gt;, and found that when people had a competing goal, behavioral priming effects were eliminated. Hence, prior tasks in the study might activate competing goals (perhaps perspective taking or establishing a social connection with outgroups) that could eliminate the aggressive response. Or sampling from a population of people high in agreeableness, who might have the competing goal of getting along chronically accessible, could eliminate the aggressive response.&lt;/p&gt;
&lt;p&gt;Of course, as many people have correctly pointed out, if a replication failure occurs it is completely inappropriate to defensively yell, "unknown moderator!" Indeed, this was stated explicitly in the very article that many priming researchers have cited when engaging in exactly this kind of defensive behavior: "priming researchers cannot appeal endlessly to “unknown moderation” without doing the work to provide evidence for such moderation" (Cesario, 2014, p. 44).&lt;/p&gt;
&lt;p&gt;To send a clear message to researchers, then: Please stop reflexively citing this article when responding to failures to replicate! You are missing the point of that article. Instead, the correct response is to productively work with other researchers &lt;em&gt;to systematically establish that such moderation did in fact occur&lt;/em&gt;! When a failure occurs, the best course forward is to register a replication involving both locations/populations of interest, ideally with an extension of the original study that measures or manipulates the hypothesized moderator.&lt;/p&gt;
&lt;p&gt;To take a final example, consider that there is now low evidentiary value for the original elderly priming-slow walking example, with evidence that some of these effects have been p-hacked (Lakens, 2014). Given this, the proper course of action is to try and replicate the effect, with preregistered replications, to provide more evidence relevant to this judgment. If priming researchers will not do this, then we forfeit the right to continue to talk about this as if it were an established, reliable effect. Of course there are limits: practical considerations prevent most of us from spending the next 20 years priming the same category in order to obtain the most accurate population effect size estimate. But we cannot continue to direct attention to the one or two "successful" publications as if getting something published establishes is truth once and for all.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;Note that the assumption of an association between black males and hostility is critical, and while it might be a reasonable assumption in some regions it may not hold in others, pointing to yet another reason why variability in priming effects may exist across regions. For instance, this assumption would most likely not hold in a country such as the Netherlands, where its black population has a Caribbean background and is mostly associated with parties, fun, and good food. At the same time, lighter skin-toned North African males, e.g. from Morocco, would most likely elicit the effect since they are associated with hostility (see also Dotsch &amp;amp; Wigboldus 2008).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bargh, J.A., Chen, M., &amp;amp; Burrows, L. (1996). Automaticity of social behavior: Direct effects of trait construct and stereotype activation on action. &lt;em&gt;Journal of Personality and Social Psychology&lt;/em&gt;, 71, 230-244. doi: 10.1037/0022-3514.71.2.230&lt;/p&gt;
&lt;p&gt;Blanchard, D.C., Hynd, A.L., Minke, K.A., Minemoto, T., &amp;amp; Blanchard, R.J. (2001). Human defensive behaviors to threat scenarios show parallels to fear- and anxiety-related defense patterns of non-human mammals. &lt;em&gt;Neuroscience and Biobehavioral Reviews&lt;/em&gt;, 25, 761-770.&lt;/p&gt;
&lt;p&gt;Cesario, J. (2014). Priming, replication, and the hardest science. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 9, 40-48. doi: 10.1177/1745691613513470&lt;/p&gt;
&lt;p&gt;Cesario, J., Plaks, J.E., Hagiwara, N., Navarrete, C.D., &amp;amp; Higgins, E.T. (2010). The ecology of automaticity: How situational contingencies shape action semantics and social behavior. &lt;em&gt;Psychological Science&lt;/em&gt;, 21, 1311-1317. doi: 10.1177/0956797610378685&lt;/p&gt;
&lt;p&gt;Dotsch, R., &amp;amp; Wigboldus, D.H.J. (2008). Virtual prejudice. Journal of Experimental Social Psychology, 44, 1194-1198. doi: doi:10.1016/j.jesp.2008.03.003&lt;/p&gt;
&lt;p&gt;Lakens, Daniel, Professors are Not Elderly: Evaluating the Evidential Value of Two Social Priming Effects Through P-Curve Analyses (January 20, 2014). Available at SSRN: &lt;a href="http://ssrn.com/abstract=2381936"&gt;http://ssrn.com/abstract=2381936&lt;/a&gt; or &lt;a href="http://dx.doi.org/10.2139/ssrn.2381936"&gt;http://dx.doi.org/10.2139/ssrn.2381936&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Macrae, C.N., &amp;amp; Johnston, L. (1998). Help, I need somebody: Automatic action and inaction. &lt;em&gt;Social Cognition&lt;/em&gt;, 16, 400-417.&lt;/p&gt;</summary><category term="social-priming-and-reproducibility"></category></entry><entry><title>Expectations of replicability and variability in priming effects, Part I: Setting the scope and some basic definitions</title><link href="http://osc.centerforopenscience.org/2014/04/09/expectations-1/" rel="alternate"></link><updated>2014-04-09T11:00:00-04:00</updated><author><name>Joseph Cesario, Kai Jonas</name></author><id>tag:osc.centerforopenscience.org,2014-04-09:2014/04/09/expectations-1/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;We are probably thought of as "defenders" of priming effects and along with that comes the expectation that we will provide some convincing argument for why priming effects are real. We will do no such thing. The kinds of priming effects under consideration (priming of social categories which result in behavioral priming effects) is a field with relatively few direct replications&lt;sup&gt;1&lt;/sup&gt; and we therefore lack good estimates of the effect size of any specific effect. Judgments about the nature of such effects can only be made after thorough, systematic research, which will take some years still (assuming priming researchers change their research practices). And of course, we must be open to the possibility that further data will show any given effect to be small or non-existent.&lt;/p&gt;
&lt;p&gt;One really important thing we could do to advance the field to that future ideal state is to &lt;strong&gt;stop calling everything priming&lt;/strong&gt;. It appears now, especially with the introduction of the awful term "social priming," that any manipulation used by a social cognition researcher can be called priming and, if such a manipulation fails to have an effect, it is cheerfully linked to this nebulous, poorly-defined class of research called "social priming." &lt;strong&gt;There is no such thing as "social priming."&lt;/strong&gt; There is priming of social categories (elderly, professor) and priming of motivational terms (achievement) and priming of objects (flags, money) and so on. And there are priming &lt;em&gt;effects&lt;/em&gt; at the level of cognition (increased activation of concepts) or affect (valence, arousal, or emotions) or behavior (walking, trivial pursuit performance) or physiology, and some of these priming &lt;em&gt;effects&lt;/em&gt; will be automatic and some not (and even then recognizing the different varieties of automaticity; Bargh, 1989). These are all different things and need to be treated separately.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;In other words, we have to distinguish between a prime and the effect of a prime (Higgins, 1996) and even then we have to distinguish between different kinds of primes and different kinds of effects (as Lakens has done in his excellent recent SSRN paper). Priming &lt;em&gt;professors&lt;/em&gt; may result in greater intelligence-related behavior whereas priming &lt;em&gt;elderly&lt;/em&gt; may have no effect. And, priming &lt;em&gt;professors&lt;/em&gt; may show effects at the behavioral level but not physiological level. But we will never understand these nuances if everything under the sun is called "social priming." For example, embodiment effects are supposed to be different than priming effects (Meier et al., 2012), yet researchers routinely link embodiment effects (and, especially, failures to replicate embodiment effects) to the same field dealing with a wide range of priming effects.&lt;/p&gt;
&lt;p&gt;The utility of any discussion about "priming," then, is minimal if we are not being specific about &lt;em&gt;which primes&lt;/em&gt; and &lt;em&gt;which effects&lt;/em&gt; we're talking about. To be clear: the type of "priming" to which we will refer in these posts are priming of social categories and the behavioral effects that result from such priming. This is best illustrated in the classic Bargh, Chen, and Burrows (1996) studies on priming &lt;em&gt;elderly&lt;/em&gt; and priming &lt;em&gt;young Black male&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We next note two important clarifications of our position because both "sides" of the discussion on priming have misunderstood some of our arguments (Cesario, 2014; Cesario &amp;amp; Jonas, in press). Some have lumped our perspective with Stroebe and Strack's (2014) position and claimed that we believe direct replication is an unattainable illusion. At the same time, priming researchers have cited our work when confronted by replication failures as a means of undermining the value of the failed replication. (Our recommendation for what a researcher should do when a result fails to replicate illustrates why this use of our argument is inappropriate. We describe this at the end of the next blog post.)  &lt;/p&gt;
&lt;p&gt;We are in disagreement with both these interpretations. To clarify, we start with the understanding that (1) direct replication is both attainable and absolutely essential and (2) priming researchers do not do enough direct replications of their own work. (Both these points were made clear in our earlier writings, Cesario, 2014). Priming researchers need to stop doing low powered, small n studies with no direct replications. This does not produce a state of science that instills much confidence.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;em&gt;There are important exceptions that are often overlooked, for example, Correll's first-person shooter task and Payne's weapon misidentification task.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bargh, J.A. (1989). Conditional automaticity: Varieties of automatic influence in social perception and cognition. In J. S. Uleman &amp;amp; J. A. Bargh (Eds.), &lt;em&gt;Unintended Thoughts&lt;/em&gt; (pp. 3-51). New York: Guilford.&lt;/p&gt;
&lt;p&gt;Bargh, J.A., Chen, M., &amp;amp; Burrows, L. (1996). Automaticity of social behavior: Direct effects of trait construct and stereotype activation on action. &lt;em&gt;Journal of Personality and Social Psychology&lt;/em&gt;, 71, 230-244. doi: 10.1037/0022-3514.71.2.230&lt;/p&gt;
&lt;p&gt;Cesario, J. (2014). Priming, replication, and the hardest science. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 9, 40-48. doi: 10.1177/1745691613513470&lt;/p&gt;
&lt;p&gt;Cesario, J. &amp;amp; Jonas, K.J. (in press). Replicability and models of priming: What a resource computation framework can tell us about expectations of replicability. &lt;em&gt;Social Cognition: Invited submission for the special issue on priming.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Higgins, E.T. (1996). Knowledge activation: Accessibility, applicability, and salience. In E. T. Higgins &amp;amp; A. W. Kruglanski (Eds.), &lt;em&gt;Social psychology: Handbook of basic principles&lt;/em&gt; (pp. 133-168). New York: Guilford Press.&lt;/p&gt;
&lt;p&gt;Meier, B.P., Schnall, S., Schwarz, N., &amp;amp; Bargh, J.A. (2012). Embodiment in social psychology. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, 4, 705-716. doi: 10.1111/j.1756-8765.2012.01212.x&lt;/p&gt;
&lt;p&gt;Stroebe, W. &amp;amp; Strack, F. (2014). The alleged crisis and the illusion of exact replication. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 9, 59-71.&lt;/p&gt;</summary><category term="social-priming-and-reproducibility"></category></entry><entry><title>The Deathly Hallows of Psychological Science</title><link href="http://osc.centerforopenscience.org/2014/04/02/deathly-hallows/" rel="alternate"></link><updated>2014-04-02T12:30:00-04:00</updated><author><name>Brent W. Roberts</name></author><id>tag:osc.centerforopenscience.org,2014-04-02:2014/04/02/deathly-hallows/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;&lt;em&gt;This piece was &lt;a href="http://pigee.wordpress.com/2014/03/10/the-deathly-hallows-of-psychological-science/"&gt;originally posted&lt;/a&gt; to the Personality Interest Group and Espresso (PIG-E) web blog at the University of Illinois.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As of late, psychological science has arguably done more to address the ongoing &lt;a href="http://wp.me/p1b8ZP-3l"&gt;believability crisis&lt;/a&gt; than most other areas of science.  Many notable efforts have been put forward to improve our methods.  From the Open Science Framework (&lt;a href="https://osf.io/"&gt;OSF&lt;/a&gt;), to changes in &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/JRPEditorial2013.pdf"&gt;journal reporting practices&lt;/a&gt;, to &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Cumming%20-%20The%20New%20Statistics%20Why%20and%20How%20-%20PS2014.pdf"&gt;new statistics&lt;/a&gt;, psychologists are doing more than any other science to rectify practices that allow far too many unbelievable findings to populate our journal pages.&lt;/p&gt;
&lt;p&gt;The efforts in psychology to improve the believability of our science can be boiled down to some relatively simple changes.  We need to replace/supplement the typical reporting practices and statistical approaches by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Providing more information with each paper so others can double-check our work, such as the study materials, hypotheses, data, and syntax (through the OSF or journal reporting practices).&lt;/li&gt;
&lt;li&gt;Designing our studies so they have adequate power or precision to evaluate the theories we are purporting to test (i.e., use larger sample sizes).&lt;/li&gt;
&lt;li&gt;Providing more information about effect sizes in each report, such as what the effect sizes are for each analysis and their respective confidence intervals.&lt;/li&gt;
&lt;li&gt;Valuing direct replication.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It seems pretty simple.  Actually, the proposed changes are simple, even mundane.&lt;/p&gt;
&lt;p&gt;What has been most surprising is the consistent push back and protests against these seemingly innocuous recommendations.  When confronted with these recommendations it seems many psychological researchers balk. Despite calls for transparency, most researchers avoid platforms like the OSF.  A striking number of individuals argue against and are quite disdainful of reporting effect sizes. Direct replications are disparaged. In response to the various recommendations outlined above, prototypical protests are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Effect sizes are unimportant because we are “testing theory” and effect sizes are only for “applied research.”&lt;/li&gt;
&lt;li&gt;Reporting effect sizes is nonsensical because our research is on constructs and ideas that have no natural metric, so that documenting effect sizes is meaningless.&lt;/li&gt;
&lt;li&gt;Having highly powered studies is cheating because it allows you to lay claim to effects that are so small as to be uninteresting.&lt;/li&gt;
&lt;li&gt;Direct replications are uninteresting and uninformative.&lt;/li&gt;
&lt;li&gt;Conceptual replications are to be preferred because we are testing theories, not confirming techniques.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While these protestations seem reasonable, the passion with which they are provided is disproportionate to the changes being recommended.  After all, if you’ve run a t-test, it is little trouble to estimate an effect size too. Furthermore, running a direct replication is hardly a serious burden, especially when the typical study only examines 50 to 60 odd subjects in a 2×2 design. Writing entire &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Stroebe%20%26%20Strack%202014.pdf"&gt;treatises&lt;/a&gt; arguing against direct replication when direct replication is so easy to do falls into the category of “the lady doth protest too much, methinks.” Maybe it is a reflection of my repressed Freudian predilections, but it is hard not to take a Depth Psychology stance on these protests.  If smart people balk at seemingly benign changes, then there must be something psychologically big lurking behind those protests.  What might that big thing be?  I believe the reason for the passion behind the protests lies in the fact that, though mundane, the changes that are being recommended to improve the believability of psychological science undermine the incentive structure on which the field is built.&lt;/p&gt;
&lt;p&gt;I think this confrontation needs to be more closely examined because we need to consider the challenges and consequences of deconstructing our incentive system and status structure.  This, then begs the question, what is our incentive system and just what are we proposing to do to it?  For this, I believe a good analogy is the dilemma faced by Harry Potter in the last book of the eponymously titled book series.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;&lt;strong&gt;The Deathly Hallows of Psychological Science&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/deathly-hallows.jpg" alt="deathly hallows" align="left" style="padding-right: 20px;" /&gt;&lt;/p&gt;
&lt;p&gt;In the last book of the Harry Potter series “The Deathly Hallows,” Harry Potter faces a dilemma.  Should he pursue the destruction of the Horcruxes or gather together the Deathly Hallows. The Horcruxes are pieces of Voldemort’s soul encapsulated in small trinkets, jewelry, and such.  If they were destroyed, then it would be possible to destroy Voldemort.  The Deathly Hallows are three powerful magical objects, which are alluring because by possessing all three, one becomes the “master of death.”  The Deathly Hallows are the Cloak of Invisibility, the Elder Wand, and the Resurrection Stone. The dilemma Harry faced was whether to pursue and destroy the Horcruxes, which was a painful and difficult path; or Harry could choose to pursue the Deathly Hallows, with which he could quite possibly conquer Voldemort, and, if not conquer him, live on despite him.  He chose to destroy the Horcruxes.&lt;/p&gt;
&lt;p&gt;Like Harry Potter, the field of psychological science (and many other sciences) faces a similar dilemma. Pursue changes in our approach to science that eliminate problematic practices that lead to unreliable science—a “destroy the Horcrux” approach. Or, continue down the path of least resistance, which is nicely captured in the pursuit of the Deathly Hallows.&lt;/p&gt;
&lt;p&gt;What are the Deathly Hallows of psychological science? I would argue that the Deathly Hallows of psychological science, which I will enumerate below, are 1) p values less than .05, 2) experimental studies, and 3) counter-intuitive findings.&lt;/p&gt;
&lt;p&gt;Why am I highlighting this dilemma at this time? I believe we are at a critical juncture.  The nascent efforts at reform may either succeed or fade away like they have done so many times before.  For it is a fact that we’ve confronted this dilemma many times before and have failed to overcome the allure of the Deathly Hallows of psychological science. Eminent methodologists such as &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Cohen%2C%201990.pdf"&gt;Cohen&lt;/a&gt;, &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Meehl%201960.pdf"&gt;Meehl&lt;/a&gt;, &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Lykken1968.pdf"&gt;Lykken&lt;/a&gt;, &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Sedlmeier%20%26%20Gigerenzer%201989.pdf"&gt;Gigerenzer&lt;/a&gt;, &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Schmidt1996.pdf"&gt;Schmidt&lt;/a&gt;, &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Fraley%20NHST%20reformatted.pdf"&gt;Fraley&lt;/a&gt;, and lately &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Cumming%20-%20The%20New%20Statistics%20Why%20and%20How%20-%20PS2014.pdf"&gt;Cumming&lt;/a&gt;, have told us how to do things better since the 1960s to no avail. Revising our approach to science has never been a question of knowing the right the thing to do, but rather it has been whether we were willing to do the thing we knew was right.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Deathly Hallows of Psychological Science: p-values, experiments, and counter-intuitive/surprising findings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/smiley.png" alt="smiley" align="left" style="padding-right: 20px;" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The cloak of invisibility: p&amp;lt;.05.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;The first Deathly Hallow of psychological science is the infamous p-value. You must attain a p-value less than .05 to be a success in psychological science.  Period.  If your p-value is greater than .05, you have no finding and nothing to say. Without anything to say, you cannot attain status in our field. Find a p-value below .05 and you can wrap it around yourself and hide from the contempt aimed at those who fail to cross that magical threshold.&lt;/p&gt;
&lt;p&gt;Because the p-value is the primary key to the domain of scientific success, we do almost anything we can to find it.  We root around in our data, digging up p-values either by cherry picking studies, selectively reporting outcomes, or through some arcane statistical wizardry.  One only has to read &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Writing_the_Empirical_Journal_Article_BEM.pdf"&gt;Bem&lt;/a&gt;’s classic article on how to write an article in psychological science to see how we approach p-values as a field:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“…the data.  Examine them from every angle. Analyze the sexes separately.  Make up new composite indices.  If a datum suggests a new hypothesis, try to find further evidence for it elsewhere in the data.  If you see dim traces of interesting patterns, try to reorganize the data to bring them into bolder relief.  If there are participants you don’t like, or trials, observers, or interviewers who gave you anomalous results, drop them (temporarily).  Go on a fishing expedition for something–anything–interesting.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What makes it worse is that when authors try to report null effects they are beaten down because we as reviewers and editors do everything in our power to hide the null effects. Null effects make for a messy narrative. Our most prestigious journals almost never publish null effects because reviewers and editors act as gatekeepers and mistakenly recommend against publishing null effects.  Consider the following personal example. In one study, reviewer 2 argued that our study was not up for publication in JPSP because one of our effects was null (there were other reasons too). Consider the fact that the null effect in question was a test of a hypothesis drawn from my own theory. I was trying to show that my theory did not work all of the time and the reviewer was criticizing me for showing that my own ideas might need revision. This captures quite nicely the tyranny of the p-value. The reviewer was so wedded to my ideas that he or she wouldn’t even let me, the author of said ideas, offer up some data that would argue for revising them.&lt;/p&gt;
&lt;p&gt;In the absence of simply rejecting null effects, we often recommend cutting the null effects. I have seen countless recommendations in reviews of my papers and the papers of colleagues to simply drop studies or results that show null effects.  It is not then surprising that &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Fanelli%2020121%20rise%20of%20positive%20results.pdf"&gt;psychology confirms 95%&lt;/a&gt; of its hypotheses.&lt;/p&gt;
&lt;p&gt;Even worse, we often commit the fundamental attribution error by thinking that the person trying to publish null effects is an incompetent researcher—especially if they fail to replicate an already published effect that has crossed the magical p&amp;lt; .05 threshold. Not to be too cynical, but the reviewers may have a point.  If you are too naïve to understand “the game”, which is to produce something with p &amp;lt; .05, then maybe you shouldn’t succeed in our field.  Setting sarcasm aside, what the gatekeepers don’t understand is that they are sending a clear message to graduate students and assistant professors that they must compromise their own integrity in order to succeed in our field. Of course, this leads to the winnowing of the field of researchers who don’t want to play the game.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The Elder Wand: Running Experiments&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Everyone wants to draw a causal conclusion, even observational scientists. And, of course, the best way to draw a causal conclusion, if you are not an economist, is to run an experiment.  The second Deathly Hallow for psychological science is doing experimental research at all costs.  As one of my past colleagues told a first year graduate student, “if you have a choice between a correlational or an experimental study, run an experiment.”&lt;/p&gt;
&lt;p&gt;Where things go awry, I suspect, is when you value experiments so much, you do anything in your power to avoid any other method. This leads to levels of artificiality that can get perverse. Rather than studying the effect of racism, we study the idea of racism.  Where we go wrong is that, as &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Cialdini2009.pdf"&gt;Cialdini has noted before&lt;/a&gt;, we seldom work back and forth between the fake world of our labs and the real world where the phenomenon of interest exists. We become methodologists, rather than scientists.  We prioritize lab-based experimental methods because they are most valued by our guild not because they necessarily help us illuminate or understand our phenomenon but because they putatively lead to causal inferences. One consequence of valuing experiments so highly is that we get caught up in a world of hypothetical findings that have unknown relationships to the real world because we seldom if ever touch base with applied or field research.  As Cialdini so poignantly pointed out, we simply don’t value field research enough to pursue it with equal vigor to lab experiments.&lt;/p&gt;
&lt;p&gt;And, though some great arguments have been made that we should all relax a bit about our use of exploratory techniques and dig around in our data, what these individuals don’t realize is that half the reason we do experiments is not to do good science but to look good.  To be a “good scientist” means being a confirmer of hypotheses and there is no better way to be a definitive tester of hypotheses than to run a true experiment.&lt;/p&gt;
&lt;p&gt;Of course, now that we know &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Begley%20%26%20Ellis%202012%20reproducibility%20in%20cancer%20research.pdf"&gt;many researchers run as many experiments as they need&lt;/a&gt; to in order to &lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Francis2014PBR.pdf"&gt;construct what findings they want&lt;/a&gt;, we need to be even more skeptical of the motive to look good by running experiments. Many of us publishing experimental work are really doing exploratory work under the guise of being a wand wielding wizard of an experimentalist simply because that is the best route to fame and fortune in our guild.&lt;/p&gt;
&lt;p&gt;Most of my work has been in the observational domain, and admittedly, we have the same motive, but lack the opportunity to implement our desires for causal inference.  So far, the IRB has not agreed to let us randomly assign participants to the “divorced or not divorced” or the “employed, unemployed” conditions. In the absence of being able to run a good, clean experiment, observational researchers, like myself, bulk up on the statistics as a proxy for running an experiment. The fancier, more complex, and indecipherable the statistics, the closer one gets to the status of an experimenter. We even go so far as to mistake our statistical methods, such as cross-lag panel, longitudinal designs, for ones that would afford us the opportunity to make causal inferences (hint: they don’t). Reviewers are often so befuddled by our fancy statistics that they fail to notice the inappropriateness of that inferential leap.&lt;/p&gt;
&lt;p&gt;I’ve always held my colleague &lt;a href="http://internal.psychology.illinois.edu/~ediener/"&gt;Ed Diener&lt;/a&gt; in high esteem.  One reason I think he is great is that as a rule he works back and forth between experiments and observational studies, all in the service of creating greater understanding of well-being.  He prioritizes his construct over his method. I have to assume that this is a much better value system than our long standing obsession with lab experiments.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The Resurrection Stone: Counter-intuitive findings&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The final Deathly Hallow of psychological science is to be the creative destroyer of widely held assumptions. In fact, the foundational writings about the field of social psychology lay it out quite clearly. One of the primary routes to success in social psychology, for example, is to be surprising.  The best way to be surprising is to be the counter-intuitive innovator—identifying ways in which humans are irrational, unpredictable, or downright surprising (&lt;a href="https://dl.dropboxusercontent.com/u/46388790/methods%20issues/Ross%2C%20Lepper%2C%20%26%20Ward%20%282010%29%20-%20History%20of%20Social%20Psych.pdf"&gt;Ross, Lepper, &amp;amp; Ward, 2010&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It is hard to argue with this motive.  We hold those scientists who bring unique discoveries to their field in the highest esteem.  And, every once in a while, someone actually does do something truly innovative. In the mean time, the rest of us make up little theories about trivial effects that we market with cute names, such as the “End of History Effect”, or the “Macbeth Effect” or, whatever.  We get caught up in the pursuit of cutesy counter-intuitiveness all under the hope that our little innovation will become a big innovation.  To the extent that our cleverness survives the test of time, we will, like the resurrection stone, live on in our timeless ideas even if they are incorrect.&lt;/p&gt;
&lt;p&gt;What makes the pursuit of innovation so formidable an obstacle to reform is that it sometimes works. Every once in a while someone does revolutionize a field. The aspiration to be the wizard of one’s research world is not misplaced.  Thus, we have an incentive system that produces a variable-ratio schedule of reinforcement—one of the toughest to break according to those long forgotten behaviorists (We need not mention behavioral psychologists, since their ideas are no longer new, innovative, or interesting–even if they were right).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reasons for Pessimism&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The problem with the current push for methodological reform is that, like pursuing the Horcruxes, it is hard and unrewarding in comparison to using the Deathly Hallows of psychological science. As one of our esteemed colleagues has &lt;a href="http://funderstorms.wordpress.com/2014/02/25/nsf-gets-an-earful-about-replication/"&gt;noted&lt;/a&gt;, no one will win the APA Distinguished Scientific Award by failing to replicate another researcher’s work. Will a person who simply conducts replications of other researcher’s work get tenure?  It is hard to imagine. Will researchers do well to replicate their own research? Why? It will simply slow them down and handicap their ability to compete with the other aspiring wizards who are producing the conceptually-replicated, small N lab-based experimental studies at a frightening rate. No, it is still best to produce new ideas, even if it comes at the cost of believability. And, everyone is in on the deal. We all disparage null findings in reviews because we want errors of commission rather than omission.&lt;/p&gt;
&lt;p&gt;Another reason why the current system may be difficult to fix is that it provides a weird p-value driven utopia. With the infinite flexibility of Deathly Hallows of psychological science we can pretty much prove any idea is a good one. When combined with our antipathy toward directly replicating our own work or the work of others, everyone can be a winner in the current system. All it takes is a clever idea applied to enough analyses and every researcher can be the new hot wizard. Without any push to replicate, everyone can co-exist in his or her own happy p-value driven world.&lt;/p&gt;
&lt;p&gt;So, there you have it.  My Depth Psychology analysis of why I fear that the seemingly benign recommendations for methodological change are falling on deaf ears.  The proposed changes contradict the entire status structure that has served our field for decades.  I have to imagine that the ease with which the Deathly Hallows can be used is one reason why reform efforts have failed in the past. Since, as many have indicated, the same recommendations to revise our methods have been made for over 50 years. Each time, the effort has failed.&lt;/p&gt;
&lt;p&gt;In sum, while there have been many proposed solutions to our problems, I believe we have not yet faced our real issue, which is how are we going to re-structure our incentive structure?  Many of us have stated, as &lt;a href="http://www.personality-arp.org/html/newsletter06/problem.html"&gt;loudly&lt;/a&gt; and &lt;a href="http://wp.me/p1b8ZP-2I"&gt;persistently&lt;/a&gt; as we can that there are Horcruxes &lt;a href="https://dl.dropboxusercontent.com/u/46388790/my%20pubs/Asendorpf%20et%20al%202013.pdf"&gt;all around us&lt;/a&gt; that need to be destroyed. The move to improve our methods and to conduct direct replications can be seen as an effort to eliminate our believability Horcruxes. But, I believe the success of that effort rides on how clearly we see the task ahead of us. Our task is to convince a skeptical majority of scientists to dismantle an incentive structure that has worked for them for many decades. This will be a formidable task.&lt;/p&gt;</summary></entry><entry><title>Behavioral Priming: Time to Nut Up or Shut Up</title><link href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/" rel="alternate"></link><updated>2014-03-26T12:00:00-04:00</updated><author><name>EJ Wagenmakers</name></author><id>tag:osc.centerforopenscience.org,2014-03-26:2014/03/26/behavioral-priming/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;In the epic movie "Zombieland", one of the main protagonists –Tallahassee, played by Woody Harrelson– is about to enter a zombie-infested supermarket in search of Twinkies. Armed with a banjo, a baseball bat, and a pair of hedge shears, he tells his companion it is "time to nut up or shut up". In other words, the pursuit of happiness sometimes requires that you expose yourself to grave danger. Tallahasee could have walked away from that supermarket and its zombie occupants, but then he would never have discovered whether or not it contained the Twinkies he so desired.   &lt;/p&gt;
&lt;p&gt;At its not-so-serious core, Zombieland is about leaving one's comfort zone and facing up to your fears. This I believe is exactly the challenge that confronts the proponents of behavioral priming today. To recap, the phenomenon of behavioral priming refers to unconscious, indirect influences of prior experiences on actual behavior. For instance, presenting people with words associated with old age ("Florida", "grey", etc.) primes the elderly stereotype and supposedly makes people walk more slowly; in the same vein, having people list the attributes of a typical professor ("confused", "nerdy", etc.) primes the concept of intelligence and supposedly makes people answer more Trivia questions correctly.   &lt;/p&gt;
&lt;p&gt;In recent years, the phenomenon of behavioral priming has been scrutinized with increasing intensity. Crucial to the debate is that many (if not all) of the behavioral priming effects appear to vanish like thin air in the hands of other researchers. Many of these researchers –from now on, the skeptics– have reached the conclusion that behavioral priming effects are elusive, brought about mostly by confirmation bias, the use of questionable research practices, and selective reporting.   &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;In &lt;a href="http://www.nature.com/polopoly_fs/7.6716.1349271308!/suppinfoFile/Kahneman%20Letter.pdf"&gt;an open letter to priming researchers&lt;/a&gt;, Daniel Kahneman summarized the situation as follows (I am quoting liberally because I believe Kahneman is spot on):  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"As all of you know, of course, questions have been raised about the robustness of priming results (...) your field is now the poster child for doubts about the integrity of psychological research. Your problem is not with the few people who have actively challenged the validity of  some priming results. It is with the much larger population of colleagues who in the past accepted your surprising results as facts when they were published. These people have now attached a question mark to the field, and it is your responsibility to remove it. (...) My reason for writing this letter is that I see a train wreck looming. (...) I believe that you should collectively do something about this mess. To deal effectively with the doubts you should acknowledge their existence and confront them straight on, because a posture of defiant denial is self-defeating. Specifically, I believe that you should have an association, with a board that might include prominent social psychologists from other fields. The first mission of the board would be to organize an effort to examine the replicability of priming results, following a protocol that avoids the questions that have been raised and guarantees credibility among colleagues outside the field."  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As Kahneman suggests, priming proponents have a choice between two options: (1) defiant denial; (2) Tallahassee's "nut up or shut up" approach, that is, concrete empirical action to convince the world that the phenomenon is real. Kahneman wrote his letter on September 26, 2012, and so far the field has chosen to take the first option. Kahneman's plea for replication has been all but ignored. I have personally experienced the reluctance of priming proponents to enter any sort of collaboration, or commit to any sort of preregistration that may be conceived as a critical test of the priming phenomenon. One exception to the rule is Fritz Strack, who has kindly agreed to help out with a preregistered replication attempt of his famous facial feedback study for &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;. Another exception to the rule is Sascha Topolinski, who was extremely helpful when my lab tried to replicate one of his experiments on clockwise movements inducing preference for novelty. A final exception is the mega-study by Brent Donnellan, Richard Lucas, and Joe Cesario (in press), who failed to replicate the finding that people compensate for feeling lonely by taking warm baths and showers.  &lt;/p&gt;
&lt;p&gt;As is vividly illustrated by several articles published in a recent issue of &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, priming proponents have argued that the concept of direct replicability is a mirage (see for instance Cesario, 2014; Stroebe and Strack, 2014). Specifically, any direct replication attempt may be foiled by unrecognized moderator variables. For instance, the professor priming effect may hold up in Bloomington but it may fail when replicated in Purdue – not because the effect is unreliable or absent, but because Purdue students may not associate professors with intelligence for some reason (but see Simons, 2014). Priming proponents do have a logically valid point: moderators may be important, for some studies, in some situations. But, logically valid or not, this completely misses the mark. As Kahneman argued in his letter, defiant denial will not silence the growing chorus of skeptics. The train has left the station a few years ago, and the only way for priming proponents to regain lost ground is to engage in adversarial collaborations, preregistred replications, and other rigourous methods to re-establish scientific credibility.&lt;/p&gt;
&lt;p&gt;One may argue that a healthy scientific field, when challenged at its core empirical assumptions, should have heeded Kahneman's advice immediately. But it is not too late. Perhaps priming proponents will begin to realize that defiant denial is indeed self-defeating. Skeptics will remain active, infecting more and more researchers as times goes by. To prevent a priming apocalypse, the proponents will ultimately be forced to face up to their fears and take concrete empirical action. Perhaps they will be zombie lunch but there is no other way to regain credibility. Time to nut up or shut up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cesario, J. (2014). Priming, replication, and the hardest science. Perspectives on Psychological Science, 9, 40-48.&lt;/p&gt;
&lt;p&gt;Donnellan, M. B., Lucas, R. E., &amp;amp; Cesario, J. (in press). On the association between loneliness and bathing habits: Nine replications of Bargh and Shalev (2012) Study 1. Emotion.&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The value of direct replication. Perspectives on Psychological Science, 9, 76-80.&lt;/p&gt;
&lt;p&gt;Stroebe, W. &amp;amp; Strack, F. (2014). The alleged crisis and the illusion of exact replication. Perspectives on Psychological Science, 9, 59-71.&lt;/p&gt;</summary><category term="social-priming-and-reproducibility"></category></entry><entry><title>If You Have Data, Use It When Theorizing</title><link href="http://osc.centerforopenscience.org/2014/03/19/if-you-have-data/" rel="alternate"></link><updated>2014-03-19T12:00:00-04:00</updated><author><name>Daniel Lakens</name></author><id>tag:osc.centerforopenscience.org,2014-03-19:2014/03/19/if-you-have-data/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;There is a reason data collection is part of the empirical cycle. If you have a good theory that allows for what Platt (1964) called ‘strong inferences’, then statistical inferences from empirical data can be used to test theoretical predictions. In psychology, as in most sciences, this testing is not done in a Popperian fashion (where we consider a theory falsified if the data does not support our prediction), but we test ideas in Lakatosian lines of research, which can either be progressive or degenerative (e.g., Meehl, 1990). In (meta-scientific) &lt;em&gt;theory&lt;/em&gt;, we judge (scientific) theories based on whether they have something going for them.  &lt;/p&gt;
&lt;p&gt;In scientific &lt;em&gt;practice&lt;/em&gt;, this means we need to evaluate research lines. One really flawed way to do this is to use ‘vote-counting’ procedures, where you examine the literature, and say: "Look at all these significant findings! And there are almost no non-significant findings! This theory is the best!” Read Borenstein, Hedges, Higgins, &amp;amp; Rothstein (2006) who explain “Why Vote-Counting Is Wrong” (p. 252 – but read the rest of the book while you’re at it).  &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;One good way the evaluate lines of research is to perform a meta-analysis. However, there is one way in which meta-analyses suck big time: they suffer from publication bias. Since you only have access to the significant results, you have to try to find non-significant results by asking people whether they have them lying around in file-drawers, and convince these people to share the data (and invest the time to find the data and make them understandable for you) so that there is a substantial possibility you will be able to write an article about how the effect they worked so hard to get published actually does not exist. I’m not saying it is impossible, but there’s a challenge.  &lt;/p&gt;
&lt;p&gt;Luckily, Simonsohn, Nelson, and Simmons (2013) worked out a new meta-analytic procedure, called ‘&lt;em&gt;p&lt;/em&gt;-curve analysis’, which does not suffer from publication bias (hurray for science!). In a &lt;em&gt;p&lt;/em&gt;-curve analysis you look at the distribution of &lt;em&gt;p&lt;/em&gt;-values below .05 in the published literature, and test whether the distribution is right-skewed (as it should be if the line of research has something going for it), or uniform (a straight line – and we all know from hospital series on TV that straight lines are not good). There’s a third possibility, namely a left-skewed distribution, which also means there is no evidential value for a line of research (but in addition, that there’s a higher number of just significant results than expected by chance, which can indicate &lt;em&gt;p&lt;/em&gt;-hacking, Simmons, Nelson, &amp;amp; Simonsohn, 2011).  &lt;/p&gt;
&lt;p&gt;The new thing in &lt;em&gt;p&lt;/em&gt;-curve analyses is that inferences are drawn from a test performed on the distribution. There is nothing new about how &lt;em&gt;p&lt;/em&gt;-values should be distributed below the .05 level – that was all pretty much known previously (see for example Sellke, Bayarri, &amp;amp; Berger, 2001). But the tests are useful, in that they allow us to judge whether a set of studies (e.g., a line of research) has something going for it, or not. This makes it a perfect tool to evaluate lines of research in the published literature, and thus, evaluate how well theories are doing.  &lt;/p&gt;
&lt;p&gt;Stroebe &amp;amp; Strack (2014) recently argued that conceptual replications are the best test of a theory. Subsequently, they point to many successful conceptual replications of elderly priming and professor priming (see below), and suggest this is support for the theoretical idea that inspired these studies. This is basically a vote-counting meta-analytic procedure. Dijksterhuis (2014) does the same thing when he says that "hundreds of papers cannot be erased by the mere flick of a skeptic magic wand". This is also vote-counting, in that you argue for the presence of an effect based on a count of the number of significant effects in the literature. Again: vote-counting is a flawed meta-analytic procedure and should never be used to infer support for a theoretical idea, not even implicitly in theoretical reviews.  &lt;/p&gt;
&lt;p&gt;In a recent illustration of how &lt;em&gt;p&lt;/em&gt;-curve analyses can differentiate between theoretical ideas that have something going for them, and ideas that have not much going for them, &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2381936"&gt;I &lt;em&gt;p&lt;/em&gt;-curved lines of studies&lt;/a&gt; examining two theoretical ideas. The first, elderly priming, predicts that activating the stereotype of elderly will make people behave more slowly. The second, professor priming, predicts that activating the stereotype of a professor will make people work harder and perform better on a trivial pursuit game. My analyses are not yet peer-reviewed (but the data on which it is based is &lt;a href="https://osf.io/3urp2/"&gt;available on the Open Science Framework&lt;/a&gt;, see also Lakens, 2014) so be critical and draw your own conclusions. The analysis revealed elderly priming had no evidential value (and displayed signs of &lt;em&gt;p&lt;/em&gt;-hacking, as indicated by the higher percentage of &lt;em&gt;p&lt;/em&gt;-values just below .05 in the figure), while professor priming had something going for it (as indicated by the relatively higher percentage of &lt;em&gt;p&lt;/em&gt;-values between .00 and .01 in the figure). This nicely illustrates how it can be inferred that two lines of research that were both discussed in an equivocal manner based on implicit vote-counting procedures by Stroebe and Strack (2014) actually differ in their likelihood when correct meta-analytic procedures are applied.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/elderly_priming.png" alt="elderly priming" align="center" style="padding-right: 20px;" width="603px" /&gt;&lt;br /&gt;
&lt;img src="images/professor_priming.png" alt="professor priming" align="center" style="padding-right: 20px;" width="603px" /&gt;  &lt;/p&gt;
&lt;p&gt;I think it is important to realize that theoretic conclusions that do not use the correct meta-analytic procedures have a high risk of being false. I notice that a lot of researchers who review the literature take a vote-counting meta-analytic perspective. They cite and discuss a lot of significant findings, and assume it says something about the probability that the ideas tested in a line of studies are true. You should not be convinced about the likelihood a theoretical idea is true by such theoretical reviews. And if you write a theoretical review, make use of the meta-analytical procedures (such as &lt;em&gt;p&lt;/em&gt;-curve analyses) you have at your disposal to draw more accurate, data-driven inferences about the likelihood theoretical ideas are true.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Borenstein, M., Hedges, L. V., Higgins, J. P., &amp;amp; Rothstein, H. R. (2011). Introduction to meta-analysis. Hoboken, NJ: Wiley.  &lt;/p&gt;
&lt;p&gt;Dijksterhuis, A. (2014). Welcome back theory! Perspectives on Psychological Science, 9, 72-75.  &lt;/p&gt;
&lt;p&gt;Lakens, D. (2014). Professors are not elderly: Evaluating the evidential value of two social priming effects through p-curve analyses. Available at SSRN: &lt;a href="http://ssrn.com/abstract=2381936"&gt;http://ssrn.com/abstract=2381936&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Meehl, P. E. (1990). Appraising and amending theories: The strategy of Lakatosian defense and two principles that warrant it. Psychological Inquiry, 1(2), 108-141.  &lt;/p&gt;
&lt;p&gt;Platt, J. R. (1964). Strong inference. Science, 146(3642), 347-353.  &lt;/p&gt;
&lt;p&gt;Sellke, T., Bayarri, M. J., &amp;amp; Berger, J. O. (2001). Calibration of p values for testing precise null hypotheses. The American Statistician, 55(1), 62-71.  &lt;/p&gt;
&lt;p&gt;Simmons, J. P., Nelson, L. D., &amp;amp; Simonsohn, U. (2011). False positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22, 1359–1366.  &lt;/p&gt;
&lt;p&gt;Simonsohn, U., Nelson L, Simmons, J. (2014). P-curve: A key to the file drawer. Journal of Experimental Psychology: General.  &lt;/p&gt;
&lt;p&gt;Stroebe, W., &amp;amp; Strack, F. (2014). Welcome back theory! Perspectives on Psychological Science, 9, 59-71. DOI: 10.1177/1745691613514450  &lt;/p&gt;</summary><category term="social-priming-and-reproducibility"></category></entry><entry><title>In the Previous Episodes of the Tale of Social Priming and Reproducibility</title><link href="http://osc.centerforopenscience.org/2014/03/12/previous-episodes/" rel="alternate"></link><updated>2014-03-12T11:00:00-04:00</updated><author><name>Åse Innes-Ker</name></author><id>tag:osc.centerforopenscience.org,2014-03-12:2014/03/12/previous-episodes/</id><summary type="html">&lt;p&gt;We have lined up a nice set of posts responding to the recent special section in PoPS on social priming and replication/reproducibility, which we will publish in the coming weeks. It has proven easier to find critics of social priming than to find defenders of the phenomenon, and if there are primers out there who want to chime in they are most welcome and may contact us at oscblog@googlegroups.com.&lt;/p&gt;
&lt;p&gt;The special section in PoPS was immediately prompted by this wonderful &lt;a href="http://pps.sagepub.com/content/7/6.toc"&gt;November 2012 issue&lt;/a&gt; from PoPS on replicability in psychology  (open access!), but the Problems with Priming started prior to this. For those of you who didn’t seat yourself in front of the screen with a tub of well-buttered pop-corn every time behavioral priming made it outside the trade journals, I’ll provide some back-story, and links to posts and articles that frames the current response.&lt;/p&gt;
&lt;p&gt;The mitochondrial Eve of behavioral priming is Bargh’s &lt;a href="http://psycnet.apa.org/journals/psp/71/2/230/"&gt;Elderly Prime&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;. The unsuspecting participants were given scrambled sentences, and were asked to create proper sentences out of four of the five words in each. Some of the sentences included words like Bingo or Flordia – words that may have made you think of the elderly, if you were a student in New York in the mid nineties. Then, they measured the speed with which the participant walked down the corridor to return their work, and, surprising to many, those that unscrambled sentences that included “Bingo” and “Florida” walked slower than those that did not. Conclusion: the construct of “elderly” had been primed, causing participants to adjust their behavior (slower walk) accordingly. You can check out sample sentences in &lt;a href="http://marginalrevolution.com/marginalrevolution/2012/03/walking-fast-and-slow.html"&gt;this Marginal Revolution post&lt;/a&gt; – yes, priming made it to this high-traffic economy blog.&lt;/p&gt;
&lt;p&gt;This paper has been cited 2571 times, so far (according to Google Scholar). It even appears in Kahneman’s &lt;a href="http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555"&gt;Thinking, Fast and Slow&lt;/a&gt;, and has been high on the wish-list for replication on Pashler’s &lt;a href="http://www.psychfiledrawer.org/view_article_list.php"&gt;PsychFile Drawer&lt;/a&gt;. (No longer in the top 20, though).&lt;/p&gt;
&lt;p&gt;Finally, in January 2012, Doyen, Klein, Pichon &amp;amp; Cleeremans (a Belgian group) &lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029081."&gt;published a replication attempt&lt;/a&gt; in PLOSone where they suggest the effect was due to demand.  Ed Yong did &lt;a href="http://blogs.discovermagazine.com/notrocketscience/2012/01/18/primed-by-expectations-why-a-classic-psychology-experiment-isnt-what-it-seemed/#.UuimfLRwG70"&gt;this nice write-up&lt;/a&gt; of the research.&lt;/p&gt;
&lt;p&gt;Bargh was not amused, and wrote a scathing rebuttal on his blog in the Psychology Today domain.  He took it down after some time (for good reason – I think it can be found, but I won’t look for it.). &lt;a href="http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/#.UuimZ7RwG70"&gt;Ed commented on this too&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A number of good posts from blogging psychological scientists also commented on the story. A sampling are &lt;a href="http://hardsci.wordpress.com/2012/03/12/some-reflections-on-the-bargh-doyen-elderly-walking-priming-brouhaha/"&gt;Sanjay Srivastava&lt;/a&gt; on his blog Hardest Science, &lt;a href="http://neurochambers.blogspot.se/2012/03/you-cant-replicate-concept.html"&gt;Chris Chambers&lt;/a&gt; on NeuroChambers, and &lt;a href="http://cedarsdigest.wordpress.com/2012/03/21/put-your-head-up-to-the-meta-a-peer-reviews-post-post-publication-peer-review-a-bargh-full-of-links/"&gt;Cedar Riener&lt;/a&gt; on his Cedarsdigest. &lt;/p&gt;
&lt;p&gt;The British Psychological Society published &lt;a href="http://www.thepsychologist.org.uk/blog/11/blogpost.cfm?threadid=2196&amp;amp;catid=48"&gt;a notice about it&lt;/a&gt; in The Psychologist which links to additional commentary.  In May, Ed Yong had &lt;a href="http://www.nature.com/news/replication-studies-bad-copy-1.10634"&gt;an article in Nature&lt;/a&gt; discussing the status of non-replication in psychology in general, but where he also brings up the Doyen/Bargh controversy. On January 13, the Chronicle published &lt;a href="http://chronicle.com/article/Power-of-Suggestion/136907/"&gt;a summary of what had happened&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But, prior to that, Daniel Kahneman made a call for psychologists to clean up their act as far as behavioral priming goes. Ed Yong (again) published two pieces about it. One in &lt;a href="http://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535"&gt;Nature&lt;/a&gt; and one on &lt;a href="http://blogs.discovermagazine.com/notrocketscience/2012/10/04/daniel-kahneman-daisy-chain-replications-priming-psychology/#.UuTRArRwHIU"&gt;his blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The controversies surrounding priming continued in the spring of 2013. This time it was David Shanks who, as a hobby (&lt;a href="http://www.ucl.ac.uk/psychlangsci/"&gt;from his video&lt;/a&gt; - scroll down below the fold) had taken to attempting to replicate priming of intelligence, work originally done by &lt;a href="http://psycnet.apa.org/index.cfm?fa=search.displayrecord&amp;amp;uid=1998-01060-003"&gt;Dijksterhuis and van Knippenberg&lt;/a&gt; in 1998. He had his students perform a series of replications, all of which showed no effect, and was then collected in &lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0056515"&gt;this PLOSone paper&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Dijksterhuis retorted in &lt;a href="http://www.plosone.org/annotation/listThread.action?root=64751"&gt;the comment section&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;. Rolf Zwaan &lt;a href="http://rolfzwaan.blogspot.nl/2013/04/social-priming-in-theory.html"&gt;blogged about it&lt;/a&gt;. Then, Nature posted &lt;a href="http://www.nature.com/news/disputed-results-a-fresh-blow-for-social-psychology-1.12902#/correction1"&gt;a breathless article&lt;/a&gt; suggesting that this was a fresh blow for us who are Social Psychologists.&lt;/p&gt;
&lt;p&gt;Now, most of us who do science thought instead that this was science working just like it ought to be working, and blogged up a storm about it – with some of the posts (including one of mine) linked in Ed Yong’s &lt;a href="http://phenomena.nationalgeographic.com/2013/05/04/ive-got-your-missing-links-right-here-4-may-2013/"&gt;“Missing links” feature&lt;/a&gt;. The links are all in the fourth paragraph, above the scroll, and includes additional links to discussions on replicability, and the damage done by a certain Dutch fraudster.&lt;/p&gt;
&lt;p&gt;So here you are, ready for the next set of installments.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Ancestral to this is &lt;a href="http://psycnet.apa.org/index.cfm?fa=search.displayRecord&amp;amp;uid=1981-01290-001"&gt;Srull &amp;amp; Wyer’s&lt;/a&gt; (1979) story of Donald, who is either hostile or kind, depending on which set of sentences the participant unscrambled in that earlier experiment that had nothing to do with judging Donald.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; A nice feature.  No waiting years for the retorts to be published in the dead tree variant we all get as PDF’s anyway.&lt;/p&gt;</summary><category term="social-priming-and-reproducibility"></category></entry><entry><title>Confidence Intervals for Effect Sizes from Noncentral Distributions</title><link href="http://osc.centerforopenscience.org/2014/03/06/confidence%20intervals/" rel="alternate"></link><updated>2014-03-06T12:00:00-05:00</updated><author><name>Russ Clay</name></author><id>tag:osc.centerforopenscience.org,2014-03-06:2014/03/06/confidence intervals/</id><summary type="html">&lt;p&gt;&lt;sup&gt;(Thanks to Shauna Gordon-McKeon, Fred Hasselman, Daniël Lakens, Sean Mackinnon, and Sheila Miguez for their contributions and feedback to this post.)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I recently took on the task of calculating a confidence interval around an effect size stemming from a noncentral statistical distribution (the F-distribution to be precise).  This was new to me, and as I am of the view that such statistical procedures would add value to the work being done in the social and behavioral sciences, but that they are not common in practice at the present time, potentially due to lack of awareness, I wanted to pass along some of the things that I found.&lt;/em&gt;&lt;br /&gt;
In an effort to estimate the replicability of psychological science, an important first step is to determine the criteria for declaring a given replication attempt as successful.  Lacking clear consensus around this criteria, the OpenScience group determined that rather than settling on a single set of criteria by which the replicability of psychological research would be assessed, multiple methods would be employed, all which provide a measure of valuable insight regarding the reproducibility of published findings in psychology (OpenScience Collaboration, 2012).  One such method is to examine the confidence interval around the original target effect and to see if this confidence interval overlaps with the confidence interval from the replication effect.  However, estimating the confidence interval around many effects in social science research requires the use of  non-central probability distributions, and most mainstream statistical packages (e.g. SAS, SPSS) do not provide off the shelf capabilities for deriving confidence intervals from these distributions (Kelley, 2007).  &lt;/p&gt;
&lt;p&gt;Most of us probably picture common statistical distributions such as the t-distribution, the F-distribution, and the χ2 distribution as being two dimensional, with the x-axis representing the value of the test statistic and the area under the curve representing the likelihood of observing such a value in a sample population.  When first learning to conduct these statistical tests, such visual representations likely provided a helpful way to convey the concept that more extreme values of the test statistic were less likely.  In the realm of null hypothesis statistical testing (NHST), this provides a tool for visualizing how extreme the test statistic would need to be before we would be willing to reject a null hypothesis.  However, it is important to remember that these distributions vary along a third parameter as well: the noncentrality parameter.  The distribution that we use to determine the cut-off points for rejecting a null hypothesis is a special, central case of the distribution when the noncentrality parameter is zero.  This special-case distribution gives the probabilities of test statistic values when the null hypothesis is true (i.e., when the population effect is zero).  As the noncentrality parameter changes (i.e., when we assume that an effect does exist), the shape of the distribution which defines the probabilities of obtaining various values of the parameter in our statistical tests changes as well.  The following figure (copied from the Wikipedia page for the noncentral t-distribution) might help provide a sense of how the shape of the t-distribution changes as the noncentrality parameter varies.  &lt;/p&gt;
&lt;p&gt;&lt;img src="images/CIs.png" alt="non-central T distribution" align="center" style="padding-right: 20px;" width="360px" /&gt;&lt;br /&gt;
&lt;sup&gt;Figure by &lt;a href="http://en.wikipedia.org/wiki/File:Nc_student_t_pdf.svg"&gt;Skbkekas&lt;/a&gt;, licensed &lt;a href="http://creativecommons.org/licenses/by/3.0/deed.en"&gt;CC BY 3.0.&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The first two plots (orange and purple) illustrate the different shapes of the distribution under the assumption that the true population parameter (the difference in means) is zero.  The value of v indicates the degrees of freedom used to determine the probabilities under the curve.  The difference between these first two curves stems from the fact that the purple curve has more degrees of freedom (a larger sample), and thus there will be a higher probability of observing values near the mean.  These distributions are central (and symmetrical), and as such, values of x that are equally higher or lower than the mean are equally probable.  The second two plots (blue and green) illustrate the shapes of the distribution under the assumption that the true population parameter is two.  Notice that both of these curves are positively skewed, and that this skewness is particularly pronounced in the blue curve as it is based on fewer degrees of freedom (smaller sample size).  The important thing to note is that for these plots, values of x that are equally higher or lower than the mean are NOT equally probable.  Observing a value of x = 4 under the assumption that the true value of x is two is considerably more probable than observing a value of x = 0.  Because of this, a confidence interval around an effect that is anything other than zero will be asymmetrical and will require a bit of work to calculate.  &lt;/p&gt;
&lt;p&gt;Because the shape (and thus the degree of symmetry) of many statistical distributions depends on the size of the effect that is present in the population, we need a noncentrality parameter to aid in determining the shape of the distribution and the boundaries of any confidence interval of the population effect.  As mentioned previously, these complexities do not arise as often as we might expect in everyday research because when we use these distributions in the context of null-hypothesis statistical testing (NHST), we can assume a special, ‘centralized’ case of the distributions that occurs when the true population effect of interest is zero (the typical null hypothesis).  However, confidence intervals can provide different information than what can be obtained through NHST.  When testing a null hypothesis, what we glean from our statistics is the probability of obtaining the effect observed in our sample if the true population effect is zero.  The p-value represents this probability, and is derived from a probability curve with a noncentrality parameter of zero.  As mentioned above, these special cases of statistical distributions such as the t, F, and χ2 are ‘central’ distributions.  On the other hand, when we wish to construct a confidence interval of a population effect, we are no longer in the NHST world, and we no longer operate under the assumption of ‘no effect’.  In fact, when we build a confidence interval, we are not necessarily making assumptions at all about the existence or non-existence of an effect.  Instead, when we build a confidence interval, we want a range of values that is likely to contain the true population effect with some degree of confidence.  To be crystal clear, when we construct a 95% confidence interval around a test statistic, what we are saying is that if we repeatedly tested random samples of the same size from the target population under identical conditions, the true population parameter will be bounded by the 95% confidence interval derived from these samples 95% of the time.  &lt;/p&gt;
&lt;p&gt;From a practical standpoint, a confidence interval can tell us everything that NHST can, and then some.  If the 95% confidence interval of a given effect contains the value of zero, then there is a good chance that there is a negligible effect in the relationship you are testing.  In this case, as a researcher, the conclusion that you would reach is conceptually similar to declaring that you are not willing to reject a null hypothesis of zero effect on the grounds that there is greater than a 5% chance that the effect is actually zero.  However, a confidence interval allows the researcher to say a bit more about the potential size of a population effect as well as the degree of variability that exists in it’s estimate, whereas NHST only permits the researcher to state, with a specified level of confidence, the likelihood that an effect exists at all.  &lt;/p&gt;
&lt;p&gt;Why, then, is NHST the overwhelming choice of statisticians in the social sciences?  The likely answer has to do with the idea of non-centrality stated above.  When we build a confidence interval around an effect size, we generally do not build the confidence interval around an effect of zero.  Instead, we build the confidence interval around the effect that we find in our sample.  As such, we are unable to build the confidence interval using the symmetrical, special case instances of many of our statistical distributions.  We have to build it using an asymmetrical distribution that has a shape (a degree of non-centrality) that depends on the effect that we found in our sample.  This gets messy, complicated, and requires a lot of computation.  As such, the calculation of these confidence intervals was not practical until it became commonplace for researchers to have at their disposal the computational power available in modern computing systems.  However, research in the social sciences has been around much longer than your everyday, affordable, quad-core laptop, and because building confidence intervals around effects from non-central distributions was impractical for much of the history of the social sciences, these statistical techniques were not often taught, and their lack of use is likely to be an artifact of institutional history (Steiger &amp;amp; Fouladi, 1997).
All of this to say that in today’s world, researchers generally have more than enough computational power at their disposal to easily and efficiently construct a confidence interval around an effect from a non-central distribution.  The barriers to these statistical techniques have been largely removed, and as the value of the information obtained from a confidence interval exceeds the value of the information that can be obtained from NHST, it is useful to spread the word about resources that can help in the computation of confidence intervals around common effect size metrics in the social and behavioral sciences.  &lt;/p&gt;
&lt;p&gt;One resource that I found to be particularly useful is the MBESS (Methods for the Behavioral, Educational, and Social Sciences) package for the &lt;a href="http://www.r-project.org/"&gt;R statistical software platform&lt;/a&gt;.  For those unfamiliar with R, it is a free, open-source statistical software package which can be run on Unix, Mac, and Windows platforms.  The standard R software contains basic statistics functionality, but also provides the capability for contributors to develop their own functionality (typically referred to as ‘packages’) which can be made available to the larger user community for download.   MBESS is one such package which provides ninety-seven different functions for statistical procedures that are readily applicable to statistical analysis techniques in the behavioral, educational, and social sciences.  Twenty-five of these functions involve the calculation of confidence intervals or confidence limits, mostly for statistics stemming from noncentral distributions.  &lt;/p&gt;
&lt;p&gt;For example, I used the ci.pvaf (confidence interval of the proportion of variance accounted for) function from the MBESS package to obtain a 95% confidence interval around an η2 effect of 0.11 from a one-way between groups analysis of variance.  In order to do this, I only needed to supply the function with several relevant arguments:  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;F-value:&lt;/strong&gt;  This is the F-value from a fixed-effects ANOVA&lt;br /&gt;
&lt;strong&gt;df:&lt;/strong&gt;  The numerator and denominator degrees of freedom from the analysis&lt;br /&gt;
&lt;strong&gt;N:&lt;/strong&gt;  The sample size&lt;br /&gt;
&lt;strong&gt;Confidence Level:&lt;/strong&gt;  The confidence level coverage that you desire (i.e. 95%)  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No more information is required.  Based on this, the function can calculate the desired confidence interval around the effect. Here is a copy of the code that I entered and what was produced (with comments in italics to explain what is going on in each step):  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;library(MBESS);&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;once you have installed the MBESS package, this command makes it available for your current session of R&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ci.pvaf(F.value=4.97, df.1=2, df.2=81, N=84, conf.level=.95)&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;this uses the ci.pvaf function in the MBESS package to calculate the confidence interval.  I have given # the function an F-value (F.value) of 4.97, with 2 degrees of freedom between groups (df.1), and 81 # degrees of freedom within groups (df.2), a sample size (N) of 84, and have asked it to produce a 95% confidence interval (conf.level).  Executing the above command produces the following output:&lt;/em&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$Lower.Limit.Proportion.of.Variance.Accounted.for&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;[1] 0.007611619&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$Probability.Less.Lower.Limit&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;[1] 0.025&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$Upper.Limit.Proportion.of.Variance.Accounted.for&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;[1] 0.2320935&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$Probability.Greater.Upper.Limit&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;[1] 0.025&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$Actual.Coverage&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;[1] 0.95&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thus, the 95% confidence interval around my η2 effect is &lt;strong&gt;[0.01 - 0.23]&lt;/strong&gt;.&lt;/em&gt;  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similar functions are available in the MBESS package for calculating confidence intervals around a contrast in a fixed-effects ANOVA, multiple correlation coefficient, squared multiple correlation coefficient, regression coefficient, reliability coefficient, RMSEA, standardized mean difference, signal-to-noise ratio, and χ2 parameters, among others.  &lt;/p&gt;
&lt;h5&gt;Additional Resources&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Fred Hasselman has created &lt;a href="http://osc.centerforopenscience.org/static/CIs_in_r.html"&gt;a brief tutorial&lt;/a&gt; for computing effect size confidence intervals using R.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For those more familiar with conducting statistics in an SPSS environment, Dr. Karl Wuensch at East Carolina University provides links to several SPSS programs on his Web Page.  &lt;a href="http://core.ecu.edu/psyc/wuenschk/SPSS/SPSS-Programs.htm"&gt;This program&lt;/a&gt; is for calculating confidence intervals for a standardized mean difference (Cohen’s d).  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, I came across several publications that I found useful in providing background information regarding non-central distributions (a few of which are cited above).  I’m sure there are more, but I found these to be a good place to start:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Cumming, G. (2006).  &lt;em&gt;How the noncentral t distribution got its hump.&lt;/em&gt;  Paper presented at the seventh International Conference on Teaching Statistics, Salvador, Bahia, Brazil.  &lt;/p&gt;
&lt;p&gt;Cumming, G. (2014).  &lt;em&gt;The new statistics: Why and how.&lt;/em&gt;  Psychological Science, 25, 7-29.  DOI: 10.1177/0956797613504966  &lt;/p&gt;
&lt;p&gt;Kelley, K. (2007).  &lt;em&gt;Confidence intervals for standardized effect sizes: Theory, application, and implementation.&lt;/em&gt;  Journal of Statistical Software, 20, 1-24.  &lt;/p&gt;
&lt;p&gt;Smithson, M. (2001). &lt;em&gt;Correct confidence intervals for various regression effect sizes and parameters: The importance of noncentral distributions in computing intervals.&lt;/em&gt; Educational And Psychological Measurement, 61(4), 605-632. doi:10.1177/00131640121971392  &lt;/p&gt;
&lt;p&gt;Steiger, J. H., &amp;amp; Fouladi, R. T. (1997).  &lt;em&gt;Noncentrality interval estimation and the evaluation of statistical models.&lt;/em&gt;  In L. Harlow, S. &amp;gt; Mulaik, &amp;amp; J. Steiger (Eds.), What if there were no significance tests? (pp. 221-256).  Mahwah, NJ: Erlbaum.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;There is also &lt;a href="http://cran.r-project.org/web/packages/MBESS/MBESS.pdf"&gt;thorough documentation of the MBESS package itself&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully others find this information as useful as I did!&lt;/p&gt;</summary></entry><entry><title>Data trawling and bycatch – using it well</title><link href="http://osc.centerforopenscience.org/2014/02/27/data-trawling/" rel="alternate"></link><updated>2014-02-27T12:00:00-05:00</updated><author><name>Ruben Arslan</name></author><id>tag:osc.centerforopenscience.org,2014-02-27:2014/02/27/data-trawling/</id><summary type="html">&lt;!-- PELICAN_BEGIN_SUMMARY --&gt;

&lt;p&gt;Pre-registration is starting to outgrow its old home, &lt;a href="http://neuroskeptic.blogspot.co.uk/2012/04/fixing-science-systems-and-politics.html"&gt;clinical trials&lt;/a&gt;. Because it is a good way to (a) show that your theory can make viable predictions and (b) that your empirical finding is not vulnerable to hypothesising after the results are known (HARKing) and some other questionable research practices, more and more scientists endorse and actually do pre-registration. &lt;a href="http://funderstorms.wordpress.com/2014/02/25/nsf-gets-an-earful-about-replication/"&gt;Many&lt;/a&gt; remain wary though and &lt;a href="http://andrewgelman.com/2014/01/23/discussion-preregistration-research-studies/"&gt;some&lt;/a&gt; simply think pre-registration cannot work for their kind of research. A recent amendment (October 2013) to the Declaration of Helsinki mandates public registration of all research on humans before recruiting the first subject and the publication of all results, positive, negative and inconclusive. &lt;/p&gt;
&lt;p&gt;For some of  science the widespread “fishing for significance” metaphor illustrates the problem well: Like an experimental scientist the fisherman casts out the rod many times, tinkering with a variety of baits and bobbers, one at a time, trying to make a good catch, but possibly developing a superstition about the best bobber. And, like an experimental scientist, if he returns the next day to the same spot, it would be easy to check whether the success of the bobber replicates. If he prefers to tell fishing lore and enshrine his bobber in a display at his home, other fishermen can evaluate his lore by doing as he did in his stories.&lt;/p&gt;
&lt;p&gt;Some disciplines (epidemiology, economics, developmental and personality psychology come to mind) proceed, quite legitimately, more like fishing trawlers – that is to say data collection is a laborious, time-consuming, collaborative endeavour. Because these operations are so large and complex, some data bycatch will inevitably end up in the dragnet. &lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;Consider a massive project like the National Longitudinal Survey of Youth. Of course the Bureau of Labour Statistics had some clear research questions (primarily related to the labour market) set out, when the project started. Still, the public, free NLSY data has since been used to answer many other questions that were almost certainly not originally envisioned. &lt;/p&gt;
&lt;p&gt;Unfortunately, research like this can engender superstitious beliefs as well and these are not so easily subjected to replication. It can happen because researchers explore the data, do not appropriately deal with multiple comparisons and then HARK, leading to exploratory research posing as confirmatory (ERPAC). But problems can also occur if  researchers analyse the data with an &lt;em&gt;a priori&lt;/em&gt; hypothesis and good intentions, mostly because publication bias is so prevalent: If only the few among many who happen to make the magic decisions (which can itself be &lt;a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf"&gt;data-dependent&lt;/a&gt;) leading to a significant result get to publish, all the other attempts are lost and the result looks less fickle than it really is.&lt;/p&gt;
&lt;p&gt;So, if replication of such laborious efforts is hard or unlikely, we should look to other means to improve this kind of science. I think this situation can be remedied through pre-registration too. The NLSY already requires scientists to sign up for access and to choose the variables they need. Other data repositories even require researchers to formulate a research proposal before access is granted. The two could be combined, so that researchers state their hypothesis, how their analysis will look and which data they’ll need. Then, they’d only get the necessary data subset, preventing ERPAC and every attempted analysis would be on the record, mitigating publication bias.&lt;/p&gt;
&lt;p&gt;However, this would prevent exploratory research and my impression is that this is the evil plan most frequently ascribed to pre-registration supporters. Again, I think this can be remedied within the framework of pre-registration to the benefit of confirmatory &lt;em&gt;and&lt;/em&gt; exploratory science: The NLSY contains information from more than 10 000 children, enough data to do research with only half of the data and still have more power than most other studies. So my suggestion to solve the problem is to &lt;strong&gt;randomly choose half of the data and make it available for exploratory research&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Every publication using such data would then be expected to replicate its central results in the other half. This way we would be assured that confirmatory research lived up to its name, but we would also improve exploratory research. After all, exploratory research can be more than tabulating hundreds of correlations and picking the ones with the most stars, a strategy that would hopefully die out if findings were to be replicated. Research that is based on a solid theory, so that predictions can be registered before looking at the data, would get a power boost because it could use the entire sample to test its predictions.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/datatrawling.png" alt="flow chart" align="left" style="padding-right: 20px;"/&gt;&lt;/p&gt;
&lt;p&gt;Now, consider a longitudinal study of high school exchange students – once a fledgling PhD has recruited two hundred and plans to track them for years to come, she won’t limit herself to asking them about homesickness, even if this is her primary research topic. At the same time the time constraints of a typical PhD may imply that not every possible research question and applicable theory is clear when data collection begins. Especially for novice researchers, the time it takes to run such a study may be sufficient for a complete methods overhaul. We should not want to force a newly-minted imputation specialist to use the pre-registered last-observation-carried-forward approach. In addition, it’s fairly common for other researchers to piggyback on such an effort, resulting in even more unintended combinations of variables and thus potentially answerable research questions. &lt;/p&gt;
&lt;p&gt;So, there will presumably be bycatch in this endeavour too and it’s important to use it efficiently. Imagine the data collection happens to include a survey before and after a deadly earthquake – this would make the data unique, impossible to replicate. Laborious studies need not be unique to make a replication unlikely, the cost of data collection does that job on its own.&lt;/p&gt;
&lt;p&gt;And even for such designs pre-registration and the aforementioned split data procedure is possible: Data collection efforts in developmental and personality psychology could store data in a central repository (e.g. Dataverse) and give the researchers access to half of the data, until they pre-register. That way replication of the most complex and time-consuming studies would become a default rather than the rarest exception. In the meanwhile it is no technical problem to let survey software use all data and e.g. choose whom to remind to participate. I made such a survey software, formr.org,  which allows for separating the process of managing a longitudinal study (including reminders and personalised feedback) from having access to the data of interest, so I know this is technically possible. Admittedly, researchers may be too clingy parents to allow such a long separation from their data so early on.&lt;/p&gt;
&lt;p&gt;Some advocates of open data might not like that the confirmatory half of the data would be gated, not publicly available. This concern may be alleviated by (a) not making it up to primary researchers to control access, but to a simple technical mechanism with public preservation of access requests (b) considering that longitudinal data can often be easily re-identified so that gated access is the only realistic option anyway and breaking up the data into subsets may in fact relax data protection constraints.&lt;/p&gt;
&lt;p&gt;Another problem is that this method would double sample size requirements for exploratory studies. However, I’d hope researchers would see this as a tool to foster trust in their findings and willingly recruit more participants, which, as a fraction of the total time of preparing a complex longitudinal study, is a small part of the effort, especially if the management is automated. &lt;/p&gt;
&lt;p&gt;Unfortunately, splitting data into two halves and checking whether results replicate is not the most efficient cross-validation technique, just the only available one that can address the ERPAC problem. It also seems unlikely that splitting data on collection and separating it from the collectors would be possible for data that is not directly uploaded via a computer, e.g. videos, therapist diagnoses, neurological or genetic data, though such data could of course still be shared by primary investigators in the split repositories I described above. &lt;/p&gt;
&lt;p&gt;If this practice became adopted by some, we would foster a culture of properly annotated datasets (good metadata so you know which variables to pick in the pre-registration of your hypothesis), because even the primary researchers would have to use the same interface as secondary users. This seems like a hitherto unsolved problem in the advocacy of open data. &lt;/p&gt;
&lt;p&gt;The data would also be stored in central repositories, thus making it less likely that important data dies a lonesome death on someone’s USB stick. In addition, this method would enable exploratory researchers to benefit from initiatives such as &lt;a href="http://cdn.elsevier.com/promis_misc/PROMIS%20pub_idt_CORTEX%20Guidelines_RR_29_04_2013.pdf"&gt;Registered Reports&lt;/a&gt;, where papers are accepted on the basis of methods and proposed analysis, only in this case the methods would include the exploratory strategy (and possibly preliminary results) used to derive the predictions. &lt;/p&gt;
&lt;p&gt;Maybe this argument can soothe the &lt;a href="http://funderstorms.wordpress.com/2014/02/25/nsf-gets-an-earful-about-replication/"&gt;recently voiced fear&lt;/a&gt; that pre-registration would increase the pressure to deliver the promised results and thus incentivise new questionable research practices. By design, pre-registration is meant to do the opposite. It need not mean pressure to deliver, endless forms or bureaucracy.  It can instead be an informal process where the forethought you put into your research anyway is written down and timestamped. &lt;/p&gt;</summary></entry><entry><title>Open Data and IRBs</title><link href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/" rel="alternate"></link><updated>2014-02-05T11:00:00-05:00</updated><author><name>Bryan Burnham</name></author><id>tag:osc.centerforopenscience.org,2014-02-05:2014/02/05/open-data-and-IRBs/</id><summary type="html">&lt;p&gt;Among other things the open science movement encourages “open data” practices, that is, researchers making data freely available on personal/lab websites or institutional repositories for others to use. For some, open data is a necessity as the &lt;a href="http://grants.nih.gov/grants/policy/data_sharing/data_sharing_guidance.htm#goals"&gt;NIH&lt;/a&gt; and &lt;a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp"&gt;NSF&lt;/a&gt; have adopted data-sharing policies and require some grant applications to include data management and dissemination plans. According to the NIH:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“...all data should be considered for data sharing. &lt;strong&gt;Data should be made as widely and freely available as possible while safeguarding the privacy of participants, and protecting confidential and proprietary data.&lt;/strong&gt;” (emphasis theirs)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Before making human subject data open several issues must be considered. First, data should be de-identified to maintain subject confidentiality so responses cannot be linked to identities and data are seemingly anonymous. Second, researchers should consider Institutional Review Board’s (IRB) policies about data sharing. (Disclosure: I have been a member of my university's IRB for 6 years and chair of my Departmental Review Board, DRB, for 7 years.)&lt;/p&gt;
&lt;p&gt;Unfortunately, while the policies and procedure of all IRBs require researchers to obtain consent, disclose study procedures to subjects, and maintain confidentiality, it is unknown how many IRBs have policies and procedures for open data dissemination. Thus, a conflict may arise between researchers who want to adopt open data practices or &lt;em&gt;need&lt;/em&gt; to disseminate data (those with NIH or NSF grants) and judgements of IRBs.&lt;/p&gt;
&lt;p&gt;This is an especially important issue for those who want to share data that are already collected: can use data be openly disseminated without IRB review? (I address this below when I offer recommendations.) What can researchers do when they want or need to share data freely, but their IRB does not have a clear policy? And what say does an IRB have in open data practices?&lt;/p&gt;
&lt;p&gt;While IRBs should be consulted and informed about open data, as I delineate below IRBs are not now and were never intended to be data-monitoring groups (Bankert &amp;amp; Amdur, 2000). IRBs are regulated and have little say in whether a researcher can share data, based on the purview, scope, and responsibilities of IRBs.&lt;/p&gt;
&lt;p&gt;IRBs in the United States are regulated under &lt;a href="http://www.hhs.gov/ohrp/humansubjects/guidance/45cfr46.html"&gt;US Health and Human Services (HHS)&lt;/a&gt; guidelines for Protection of Human Subjects. The guidelines describe the composition of IRBs, record keeping, define levels of risk, and list specific duties of IRBs and hint at their limits.&lt;/p&gt;
&lt;p&gt;When they function appropriately IRBs review research protocols to (1) evaluate risks; (2) determine whether subject confidentiality is maintained, that is, whether responses are linked to identities (‘confidentiality’ differs from ‘privacy’, which means others will not know a person participated in a study); and (3) evaluate whether subjects are given sufficient information about risks, procedures, privacy, and confidentiality. HHS Regulations Part 46, Subpart A, Section 111 ("Criteria for IRB Approval of Research") (a)(2), is very specific on the purview of IRBs in evaluating protocols:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"In evaluating risks and benefits, &lt;strong&gt;the IRB should consider only those risks and benefits that may result from the research&lt;/strong&gt; (as distinguished from risks and benefits of therapies subjects would receive even if not participating in the research). &lt;strong&gt;The IRB should not consider possible long-range effects of applying knowledge gained in the research (for example, the possible effects of the research on public policy) as among those research risks that fall within the purview of its responsibility.&lt;/strong&gt;" [emphasis added]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And regulations §46.111 (a)(6) and (a)(7) state that IRBs are to evaluate the safety, privacy, and confidentiality of subjects in proposed research:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(a)(6)  "When appropriate, the research plan makes adequate provision for monitoring the data collected to ensure the safety of subjects.”
(a)(7) “When appropriate, there are adequate provisions to protect the privacy of subjects and to maintain the confidentiality of data." &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The regulations make it clear that IRBs should consider only risks directly related to the study, and explicitly forbid IRBs from evaluating potential long-range effects of new knowledge gained from the study, as in new knowledge resulting from data sharing. Thus, IRBs should concern themselves with evaluating a study for safety, confidentiality, and that information is disclosed; reviewing existing data for dissemination is &lt;em&gt;not&lt;/em&gt; under the purview of the IRB. The &lt;em&gt;only&lt;/em&gt; issue that should concern IRBs about open data is whether the data are de-identified to “...protect the privacy of subjects and to maintain the confidentiality of data." It is not the responsibility of the IRB to monitor data, that responsibility falls to the researcher.&lt;/p&gt;
&lt;p&gt;Nonetheless, IRBs may take the position that they are data monitors and deny a researcher’s request to openly disseminate data. In denying a request an IRB may use the argument ‘&lt;em&gt;subjects would not have participated if they knew the data would be openly shared.&lt;/em&gt;’ In this case, IRBs would be playing mind-readers; there is no way an IRB can assume subjects would not have participated if they knew data would be openly shared. However, whether a person would decline to participate if they were informed about a researcher’s intent to openly disseminate data is an empirical question.&lt;/p&gt;
&lt;p&gt;Also, with this argument the IRB is implicitly suggesting subjects would need to have been informed about open data dissemination in the consent form. But, such a requirement for consent forms neglects other federal guidelines. The &lt;a href="http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html%20"&gt;Belmont Report&lt;/a&gt; provides responsibilities for human researchers, much like the &lt;a href="http://www.apa.org/ethics/code/index.aspx"&gt;APA's ethical principles&lt;/a&gt;, and describes what information should be included in the consent process:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Most codes of research establish specific items for disclosure intended to assure that subjects are given sufficient information. These items generally include: the research procedure, their purposes, risks and anticipated benefits, alternative procedures (where therapy is involved), and a statement offering the subject the opportunity to ask questions and to withdraw at any time from the research.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Belmont Report does not even mention that subjects should be informed about the potential long-range plans or uses of the data they provide. Indeed, researchers do not have to tell subjects what analyses will be used, and for good reason. All the Belmont requires is for subjects be informed about the purpose of the study, the procedures, and be informed about their privacy and confidentiality of responses.&lt;/p&gt;
&lt;p&gt;Another argument an IRB could make is the data could be used maliciously. For example, a researcher could make a data set open that included ethnicity and test scores and someone else could use that data to show certain ethnic groups are smarter than others. (This example is based on a recent &lt;a href="https://groups.google.com/forum/#!topic/openscienceframework/JHucNxN19hc"&gt;Open Science Framework post&lt;/a&gt; that is the basis for this post.) &lt;/p&gt;
&lt;p&gt;Although it is more likely that open data would be used as intended, someone could use data as they were not intended and may find a relationship between ethnicity and test scores. So what? The data are not malicious or problematic, it is the person using (misusing?) the data, and IRBs should not be in the habit of allowing only politically correct research to proceed (Lilienfeld, 2010). Also, by considering what others &lt;em&gt;might&lt;/em&gt; do with open data, IRBs would be mind-reading and overstepping its purview by considering “...long-range effects of applying knowledge gained in the research (for example, the possible effects of the research on public policy).”&lt;/p&gt;
&lt;p&gt;The bottom line is IRBs cannot know whether subjects would not have participated in a project if they knew the data would be openly disseminated, or potential findings by others. Federal regulations inform IRBs of their specific duties, which do &lt;em&gt;not&lt;/em&gt; include data monitoring or making judgments on open data dissemination; those duties are the responsibilities of the researcher.&lt;/p&gt;
&lt;p&gt;So what should you do if you want to make your data open? First, don't fear the IRB, but don’t forget the IRB. Perhaps re-examine IRB policies any time you plan a new project to remind yourself of the IRB requirements.&lt;/p&gt;
&lt;p&gt;Second, making your data open does depend on what subjects agree to on the consent form, and this is especially important if you want to make existing data open. If subjects are told their participation will remain private (identities not disclosed) and responses will remain confidential (identities not linked to responses), openly disseminating de-identified data would not violate the agreement. However, if subjects were told the data would ‘not be disseminated’, the researcher &lt;em&gt;may&lt;/em&gt; violate the agreement if they openly share data. In this case the IRB would need to be involved, subjects may need to re-consent to allow their responses to be disseminated, and new IRB approval may be needed as the original consent agreement may change.&lt;/p&gt;
&lt;p&gt;Third, &lt;a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/"&gt;de-identify data sets you plan to make open&lt;/a&gt;. This includes removing names, student IDs, the subject numbers, timestamps, and anything else that could be used to uniquely identify a person.&lt;/p&gt;
&lt;p&gt;Fourth, inform your IRB and department of your intentions. Describe your de-identification process and that you are engaging in open data practices as you see appropriate while maintaining subject confidentiality and privacy. (If someone objects, direct them toward federal IRB regulations.)&lt;/p&gt;
&lt;p&gt;Finally, work with your IRB to develop guidelines and policies for data sharing. Given the speed and recency of the open science and open data movements, it is unlikely many IRBs have considered such policies.&lt;/p&gt;
&lt;p&gt;We want greater transparency in science, and open data is one practice the can help. The IRB should not be seen as a hurdle or barrier to disseminating data, but as a reminder that one of the best practices in science is to ensure the integrity of our data and information communications by responsibly maintaining the confidence and privacy of our research subjects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bankert, E., &amp;amp; Amdur, R. (2000). &lt;a href="http://www.jstor.org/stable/3563586"&gt;The IRB is not a data and safety monitoring board.&lt;/a&gt; IRB: Ethics and Human Research, 22(6), 9-11. &lt;/p&gt;
&lt;p&gt;De Wolfe, V. A., Sieber, J. E., Steel, P. M., &amp;amp; Zarate, A. O. (2005). &lt;a href="http://www.jstor.org/stable/3563537"&gt;Part I: What is the requirement for data sharing?&lt;/a&gt; IRB: Ethics and Human Research, 27(6), 12-16. &lt;/p&gt;
&lt;p&gt;De Wolfe, V. A., Sieber, J. E., Steel, P. M., &amp;amp; Zarate, A. O. (2006). &lt;a href="http://www.jstor.org/stable/30033191"&gt;Part III: Meeting the challenge when data sharing is required.&lt;/a&gt; IRB: Ethics and Human Research, 28(2), 10-15. &lt;/p&gt;
&lt;p&gt;Lilienfeld, S.O. (2010).  &lt;em&gt;Can psychology become a science?&lt;/em&gt;  Personality and Individual Differences, 49, 281-288.&lt;/p&gt;</summary></entry><entry><title>Privacy in the Age of Open Data</title><link href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/" rel="alternate"></link><updated>2014-01-29T11:00:00-05:00</updated><author><name>Sean Mackinnon</name></author><id>tag:osc.centerforopenscience.org,2014-01-29:2014/01/29/privacy-and-open-data/</id><summary type="html">&lt;p&gt;Nothing is really private anymore.  Corporations like Facebook and Google have been collecting our information for some time, and selling it in aggregate to the highest bidder. People have been raising concerns over these invasions of privacy, but generally only technically-savvy, highly motivated people can really be successful at remaining anonymous in this new digital world.&lt;/p&gt;
&lt;p&gt;For a variety of incredibly important reasons, we are moving towards open research data as a scientific norm – that is, micro datasets and statistical syntax openly available to anyone who wants it. However, some people are uncomfortable with open research data, because they have concerns about privacy and confidentiality violations.  Some of these violations are even making the news: &lt;a href="http://wi.mit.edu/news/archive/2013/scientists-expose-new-vulnerabilities-security-personal-genetic-information"&gt;A high profile case&lt;/a&gt; about people being identified from their publicly shared genetic information comes to mind.&lt;/p&gt;
&lt;p&gt;With open data comes increased responsibility. As researchers, we need to take particular care to balance the advantages of data-sharing with the need to protect research participants from harm.  I’m particularly primed for this issue because my own research often intersects with clinical psychology. I ask questions about things like depression, anxiety, eating disorders, substance use and conflict with romantic partners.  The data collected in many of my studies has the potential to seriously harm the reputation – and potentially the mental health – of participants if linked to their identity by a malicious person.  This said, I believe in the value of open data sharing. In this post, I’m going to discuss a few core issues as it pertains to de-identification – that is, ensuring the anonymity of participants in an openly shared dataset.  Violations of privacy will always be a risk: However, some relatively simple steps on the part of the researcher can make re-identification of individual participants much more challenging.&lt;/p&gt;
&lt;h3&gt;Who are we protecting the data from?&lt;/h3&gt;
&lt;p&gt;Throughout the process, it’s helpful to imagine yourself as a person trying to get dirt on a potential participant. Of course, this is ignoring the fact that very few people are likely to use data for malicious purposes … but for now, let’s just consider the rare cases where this might happen. It only takes one high-profile incident to be a public relations and ethics nightmare for your research! There are two possibilities for malicious users that I can think of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Identity thieves who don’t know the participant directly, but are looking for enough personal information to duplicate someone’s identity for criminal activities, such as credit card fraud. These users are unlikely to know anything about participants ahead of time, so they have a much more challenging job because they have to be able to identify people exclusively using publicly available information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;People who know the participant in real-life and want to find out private information about someone for some unpleasant purpose (e.g., stalkers, jealous romantic partners, a fired employee, etc.). In this case, the party likely knows (a) that the person of interest is in your dataset; (b) basic demographic information on the person such as sex, age, occupation, and the city they live in.  Whether or not this user is successful in identifying individuals in an open dataset depends on what exactly the researcher has shared.  For fine-grained data, it could be very easy; however, for properly de-identified data, it should be virtually impossible.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Key Identifiers to Consider when De-Identifying Data&lt;/h3&gt;
&lt;p&gt;The primary way to safeguard privacy in publicly shared data is to avoid identifiers; that is, pieces of information that can be used directly or indirectly to determine a person’s identity. A useful starting point for this is the list of 18 identifiers indicated in the &lt;a href="http://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act"&gt;Health Insurance Portability and Accountability Act&lt;/a&gt; that are to be used with Protected Health Information. &lt;a href="http://www.oshpd.ca.gov/Boards/CPHS/HIPAAIdentifiers.pdf"&gt;A full list of these identifiers can be found here.&lt;/a&gt; Many of these identifiers are obvious (e.g., no names, phone numbers, SIN numbers, etc.), but some identifiers are worth discussing more specifically in the context of psychological research paradigm which shares data openly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Demographic variables&lt;/strong&gt;. Most of the variables that psychologists are interested in are not going to be very informative for identifying individuals.  For example, reaction time data (even if unique to an individual) is very unlikely to identify participants – and in any event, most people are unlikely to care if other people know that they respond 50ms faster to certain types of visual stimuli. The type of data that are generally problematic are what I’ll call “demographic variables.” So things like sex, ethnicity, age, occupation, university major, etc.  These data are sometimes used in analyses, but most often are just used to characterize the sample in the participants section of manuscripts. Most of the time, demographic variables can’t be used in isolation to identify people; instead, combinations of variables are used (e.g., a 27-year old, Mexican woman who works as a nurse may be the only person with that combination of traits in the data, leaving her vulnerable to loss of privacy). Because the combination of several demographic characteristics can potentially produce identifiable profiles, a common rule of thumb I picked up when working with Statistics Canada is to require a minimum of 5 participants per cell. In other words, if a particular combination of demographic features yields less than 5 individuals, the group will be collapsed into a larger, more anonymous, aggregate group. The most common example of this would be using age ranges (e.g., ages 18-25) instead of exact ages; similar logic could apply to most demographic variables. This rule can get restrictive fast (but also demonstrates how little data can be required to identify individual people!) so ideally, share only the demographic information that is theoretically and empirically important to your research area.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Outliers and rare values&lt;/strong&gt;. Another major issue are outliers and other rare values. Outliers are variably defined depending on the statistical text you read, but generally refer to extreme values when variables are using continuous, interval, or ordinal measurement (e.g., someone has an IQ of 150 in your sample, and the next highest person is 120). Rare values refer to categorical data that very few people endorse (e.g., the only physics professor in a sample). There are lots of different ways you can deal with outliers, and there’s not necessarily a lot of agreement on which is the best – indeed, it’s one of those &lt;a href="http://osc.centerforopenscience.org/2013/12/18/researcher-degrees-of-freedom/"&gt;researcher degrees of freedom&lt;/a&gt; you might have heard about. Though this may depend on the sensitivity of the data in question, outliers often have the potential to be a privacy risk. From a privacy standpoint, it may be best for the researcher to deal with outliers by deleting or transforming them before sharing the data. For rare values, you can collapse response options together until there are no more unique values (e.g., perhaps classify the physics professor as a “teaching professional” if there are other teachers in the sample). In the worst case scenario, you may need to report the value as missing data (e.g., a single intersex person in your sample that doesn’t identify as male or female). Whatever you decide, you should disclose to readers what your strategy was for dealing with outliers and rare values in the accompanying documentation so it is clear for everyone using the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates&lt;/strong&gt;. Though it might not be immediately obvious, any exact dates in the dataset place participants at risk for re-identification. For example, if someone knew what day the participant took part in a study (e.g., they mention it to a friend; they’re seen in a participant waiting area) then their data would be easily identifiable by this date.  To minimize privacy risks, no exact dates should be included in the shared dataset. If dates are necessary for certain analyses, transforming the data into some less identifiable format that is still useful for analyses is preferable (e.g., have variables for “day of week” or “number of days in between measurement occasions” if these are important).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Geographic Locations&lt;/strong&gt;. The rule of having “no geographic subdivisions smaller than a state” from the &lt;a href="http://www.oshpd.ca.gov/Boards/CPHS/HIPAAIdentifiers.pdf"&gt;HIPAA guidelines&lt;/a&gt; is immediately problematic for many studies. Most researchers collect data from their surrounding community. Thus, it will be impossible to blind the geographic location in many circumstances (e.g., if I recruit psychology students for my study, it will be easy for others to infer that I did so from my place of employment at Dalhousie University). So at a minimum, people will know that participants are probably living relatively close to my place of employment. This is going to be unavoidable in many circumstances, but in most cases it should not be enough to identify participants. However, you will need to consider if this geographical information can be combined with other demographic information to potentially identify people, since it will not be possible to suppress this information in many cases. Aside from that, you’ll just have to do your best to avoid more finely grained geographical information. For example, in Canada, &lt;a href="http://www.canadapost.ca/cpotools/apps/fpc/personal/findAnAddress?execution=e3s1"&gt;a reverse lookup of postal codes&lt;/a&gt; can identify some locations with a surprising degree of accuracy, sometimes down to a particular street!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Participant ID numbers&lt;/strong&gt;. Almost every dataset will (and should) have a unique identification number for each participant. If this is just a randomly selected number, there are no major issues. However, most researchers I know generate ID numbers in non-random ways. For example, in my own research on romantic couples we assign ID numbers chronologically, with a suffix number of “1” indicating men and “2” indicating women. So ID 003-2 would be the third couple that participated, and the male within that couple. In this kind of research, the most likely person to snoop would probably be the other romantic partner. If I were to leave the ID numbers as originally entered, the romantic partner would easily be able to find their own partner’s data (assuming a heterosexual relationship and that participants remember their own ID number). There are many other algorithms researchers might use to create ID numbers, many of which do not provide helpful information to other researchers, but could be used to identify people. Before freely sharing data, you might consider scrambling the unique ID numbers so that they cannot be a privacy risk (you can, of course, keep a record of the original ID numbers in your own files if needed for administrative purposes).&lt;/p&gt;
&lt;h3&gt;Some Final Thoughts&lt;/h3&gt;
&lt;p&gt;Risk of re-identification is never zero. Especially when data are shared openly online, there will always be a risk for participants. Making sure participants are fully informed about the risks involved during the consent process is essential. Careless sharing of data could result in a breach of privacy, which could have extremely negative consequences both for the participants and for your own research program. However, with proper safeguards, the risk of re-identification is low, in part due to some naturally occurring features of research. The slow, plodding pace of scientific research inadvertently protects the privacy of participants: Databases are likely to be 1-3 years old by the time they are posted, and people can change considerably within that time, making them harder to identify. Naturally occurring noise (e.g., missing data, imputation, errors by participants) also impedes the ability to identify people, and the variables psychologists are usually most interested in are often not likely candidates to re-identify someone.&lt;/p&gt;
&lt;p&gt;As a community of scientists devoted to making science more transparent and open, we also carry the responsibility of protecting the privacy and rights of participants as much as is possible. I don’t think we have all the answers yet, and there’s a lot more to consider when moving forward.  Ethical principles are not static; there are no single “right” answers that will be appropriate for all research, and standards will change as technology and social mores change with each generation. Still, by moving forward with an open mind, and a strong ethical conscience to protect the privacy of participants, I believe that data can really be both open and private. &lt;/p&gt;</summary></entry><entry><title>Open Projects - Wikipedia Project Medicine</title><link href="http://osc.centerforopenscience.org/2014/01/22/op-wiki-project-med/" rel="alternate"></link><updated>2014-01-22T12:00:00-05:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-01-22:2014/01/22/op-wiki-project-med/</id><summary type="html">&lt;p&gt;&lt;em&gt;This article is the first in &lt;a href="http://osc.centerforopenscience.org/tag/open-projects.html"&gt;a series highlighting open science projects&lt;/a&gt; around the community.  You can read the interview this article was based on: &lt;a href="https://docs.google.com/document/d/1xSHIF3hqqqF_iyb5olwj5NvVIVwIZX794rAINq93q_s/edit?usp=sharing"&gt;edited for clarity&lt;/a&gt;, &lt;a href="https://docs.google.com/document/d/1Ci9pYkjbignjuir9Tjcx2j4TMTS_iFJ8PAcdXhtlIlA/edit?usp=sharing"&gt;unedited&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Six years ago, Doctor James Heilman was working a night shift in the ER when he came across an error-ridden article on Wikipedia.  Someone else might have used the article to dismiss the online encyclopedia, which was then less than half the size it is now.  Instead, Heilman decided to improve the article.  “I noticed an edit button and realized that I could fix it.  Sort of got hooked from there.  I’m still finding lots of articles that need a great deal of work before they reflect the best available medical evidence.”&lt;/p&gt;
&lt;p&gt;Heilman, who goes by the username &lt;a href="http://en.wikipedia.org/wiki/User:Jmh649"&gt;Jmh649&lt;/a&gt; on Wikipedia, is now the president of the board of &lt;a href="http://meta.wikimedia.org/wiki/Wiki_Project_Med"&gt;Wiki Project Med&lt;/a&gt;.  A non-profit corporation created to promote medical content on Wikipedia, WPM contains over a dozen different initiatives aimed at adding and improving articles, building relationships with schools, journals and other medical organizations, and increasing access to research.&lt;/p&gt;
&lt;p&gt;One of the initiatives closest to Heilman’s heart is the Translation Task Force, an effort to identify key medical articles and translate them into as many languages as they can.  These articles cover common and potentially deadly medical circumstances, such as gastroenteritis (diarrhea), birth control, HIV/AIDS, and burns.  With the help of Translators Without Borders, over 3 million words have been translated into about 60 languages.  One of these languages is Yoruba, a West African language.  Although Yoruba is spoken by nearly 30 million people, there are only a few editors working to translate medical articles into it.  &lt;/p&gt;
&lt;p&gt;“The first two billion people online by and large speak/understand at least one of the wealthy languages of the world.  With more and more people getting online via cellphones that is not going to be true for the next 5 billion coming online.  Many of them will find little that they can understand.”  &lt;a href="http://wikimediafoundation.org/wiki/Wikipedia_Zero"&gt;Wikipedia Zero&lt;/a&gt;, a program which provides users in some developing countries access to Wikipedia without mobile data charges, is increasing access to the site.  &lt;/p&gt;
&lt;p&gt;“People are, for better or worse, learning about life and death issues through Wikipedia.  So we need to make sure that content is accurate, up to date, well-sourced, comprehensive, and accessible.  For readers with no native medical literature, Wikipedia may well be the only option they have to learn about health and disease.” &lt;/p&gt;
&lt;p&gt;That’s Jake Orlowitz (&lt;a href="http://en.wikipedia.org/wiki/User:Ocaasi/About"&gt;Ocaasi&lt;/a&gt;), WPM’s outreach coordinator.  He and Heilman stress that there’s a lot of need for volunteer help, and not just with translating.  Of the 80+ &lt;a href="http://en.wikipedia.org/wiki/Book:Health_care"&gt;articles identified as key&lt;/a&gt;, only 31 are ready to be translated.  The rest need citations verified, jargon simplified, content updated and restructured, and more.  &lt;/p&gt;
&lt;p&gt;In an effort to find more expert contributors, WPM has launched a number of initiatives to partner with medical schools and other research organizations.  Orlowitz was recently a course ambassador to the UCSF medical school, where students edited Wikipedia articles for credit.  He also set up a partnership with the &lt;a href="http://www.cochrane.org/"&gt;Cochrane Collaboration&lt;/a&gt; a non-profit made up of over 30,000 volunteers, mostly medical professionals, who conduct reviews of medical interventions.  “We arranged a donation of 100 full access accounts to The Cochrane Library, and we are currently coordinating a Wikipedian in Residence position with them.  That person will teach dozens of Cochrane authors how to incorporate their findings into Wikipedia,” explains Orlowitz.&lt;/p&gt;
&lt;p&gt;Those who are familiar with how Wikipedia is edited might balk at the thought of contributing.  Won’t they be drawn in to “edit wars”, endless battles with people who don’t believe in evolution or who just enjoy conflict?  “There are edit wars,” admits Heilman.  “They are not that common though.  99% of articles can be easily edited without problems.”&lt;/p&gt;
&lt;p&gt;Orlowitz elaborates on some of the problems that arise.  “We have a lot of new editors who don't understand evidence quality.”  The medical experts they recruit face a different set of challenges.  “One difficulty many experts have is that they wish to reference their own primary sources.  Or write about themselves.  Both those are frowned upon.  We also have some drug and device companies that edit articles in their area of business--we discourage this strongly and it's something we keep an eye on.”&lt;/p&gt;
&lt;p&gt;And what about legitimate differences of opinion about as yet unsettled medical theories, facts and treatments?  &lt;/p&gt;
&lt;p&gt;“Wikipedia 'describes debates rather than engaging in them'.  We don't take sides, we just summarize the evidence on all sides--in proportion to the quality and quantity of that evidence,” says Orlowitz.  Heilman continues: “For example Cochrane reviews state it is unclear if the risk versus benefits of breast cancer screening are positive or negative.  The USPSTF is supportive.  We state both.”  Wikipedia provides &lt;a href="http://en.wikipedia.org/wiki/Wikipedia:Identifying_reliable_sources_(medicine)"&gt;detailed guidelines&lt;/a&gt; for evaluating sources and dealing with conflicting evidence.  &lt;/p&gt;
&lt;p&gt;Another reason academics might hesitate before contributing is the poor reputation Wikipedia has in academic circles.  Another initiative, the Wikipedia-journal collaboration, 
states: "One reason some academics express for not contributing to Wikipedia is that they are unable to get the recognition they require for their current professional position. A number of medical journals have agreed in principle to publishing high quality Wikipedia articles under authors' real names following formal peer review.”  A pilot paper, adapted from the Wikipedia article on Dengue Fever, is to be published in the &lt;a href="http://www.openmedicine.ca/"&gt;Journal of Open Medicine&lt;/a&gt;, with more publications hopefully to come.&lt;/p&gt;
&lt;p&gt;The stigma against Wikipedia itself is also decreasing.  “The usage stats for the lay public, medical students, junior physicians, and doctors, and pharmacists are just mindbogglingly high.  It's in the range of 50-90%, even for clinical professionals.  We hear a lot that doctors 'jog their memory' with Wikipedia, or use it as a starting point,” says Orlowitz.  One &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3713956/"&gt;2013 study&lt;/a&gt; found that a third or more of general practitioners, specialists and medical professors had used Wikipedia, with over half of physicians in training accessing it.  As more diverse kinds of scientific contributions begin to be recognized, Wikipedia edits may make their way onto CVs.  &lt;/p&gt;
&lt;p&gt;Open science activists may be disappointed to learn that Wikipedia doesn’t require or even prefer open access sources for its articles.  “&lt;a href="http://enwp.org/WP:PAYWALL"&gt;Our policy&lt;/a&gt; simply states that our primary concern is article content, and verifi_ability_.  That standard is irrespective of how hard or easy it is to verify,” explains Orlowitz.  Both Wikipedians personally support open access, and would welcome efforts to supplement closed access citations with open ones.  “If there are multiple sources of equal quality that come to the same conclusions we support using the open source ones,” says Heilman.  A new project, the &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Open_Access/Signalling_OA-ness"&gt;Open Access Signalling project&lt;/a&gt; aims to help readers quickly distinguish what sources they’ll be able to access.&lt;/p&gt;
&lt;p&gt;So what are the best ways for newcomers to get involved?  Heilman stresses that editing articles remains one of the most important tasks of the project.  This is especially true of people affiliated with universities.  “Ironically, since these folks have access to high quality paywalled sources, one great thing they could do would be to update articles with them. We also could explore affiliating a Wikipedia editor with a university as a Visiting Scholar, so they'd have access to the library's catalogue to improve Wikipedia, in the spirit of research affiliates,” says Orlowitz.   &lt;/p&gt;
&lt;p&gt;Adds Heilman, “If there are institution who would be willing to donate library accounts to Wikipedia's we would appreciate it.  This would require having the Wikipedian register in some manner with the university.  There are also a number of us who may be willing / able to speak to Universities that wish to learn more about the place of Wikipedia in Medicine.”  The two also speak at conferences and other events.  &lt;/p&gt;
&lt;p&gt;Wiki Project Med, like Wikipedia itself, is an open community - a “do-ocracy”, as Orlowitz calls it.  If you’re interested in learning more, or in getting involved, you can check out their &lt;a href="http://meta.wikimedia.org/wiki/Wiki_Project_Med"&gt;project page&lt;/a&gt;, which details their many initiatives, or reach out to Orlowitz or the project as a whole on Twitter (&lt;a href="https://twitter.com/JakeOrlowitz"&gt;@JakeOrlowitz&lt;/a&gt;, &lt;a href="https://twitter.com/WikiProjectMed"&gt;@WikiProjectMed&lt;/a&gt;) or via email (jorlowitz@gmail.com, wikiprojectmed@gmail.com).&lt;/p&gt;</summary><category term="open-projects"></category></entry><entry><title>The APA and Open Data: one step forward, two steps back?</title><link href="http://osc.centerforopenscience.org/2014/01/15/apa-and-open-data/" rel="alternate"></link><updated>2014-01-15T11:20:00-05:00</updated><author><name>Denny Borsboom</name></author><id>tag:osc.centerforopenscience.org,2014-01-15:2014/01/15/apa-and-open-data/</id><summary type="html">&lt;p&gt;&lt;img src="images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /&gt;&lt;/p&gt;
&lt;p&gt;I was pleasantly surprised when, last year, I was approached with the request to become Consulting Editor for a new APA journal called &lt;em&gt;Archives of Scientific Psychology&lt;/em&gt;. The journal, as advertised on its website upon launch, had a distinct Open Science signature. As its motto said, it was an “Open Methodology, Open Data, Open Access journal”.  That’s a lot of openness indeed.&lt;/p&gt;
&lt;p&gt;When the journal started, the website not only boosted the Open Access feature of the journal, but went on to say that "[t]he authors have made available for use by others the data that underlie the analyses presented in the paper". This was an incredibly daunting move by APA - or so it seemed. Of course, I happily accepted the position.&lt;/p&gt;
&lt;p&gt;After a few months, the first papers in &lt;em&gt;Archives&lt;/em&gt; were published. Open Data enthusiast Jelte Wicherts of Tilburg University immediately tried to retrieve data for reanalysis. Then it turned out that the APA holds a quite ideosyncratic definition of the word “open”: upon his request, Wicherts was referred to a website that presented a &lt;a href="http://www.apa.org/pubs/journals/arc/data-access.aspx"&gt;daunting list of requirements&lt;/a&gt; for data-requests to fulfill. That was quite a bit more intimidating than the positive tone struck in &lt;a href="http://www.apa.org/pubs/journals/features/arc-1-1-1.pdf"&gt;the editorial&lt;/a&gt; that accompanied the launch of the journal.&lt;/p&gt;
&lt;p&gt;This didn’t seem open to me at all. So: I approached the editors and said that I could not subscribe to this procedure, given the fact that the journal is supposed to have open data. The editors then informed me that their choice to implement these procedures was an entirely conscious one, and that they stood by it. Their point of view is articulated in their data sharing guidelines. For instance, "next-users of data must formally agree to offer co-authorship to the generator(s) of the data on any subsequent publications" since "[i]t is the opinion of the &lt;em&gt;Archives&lt;/em&gt; editors that designing and conducting the original data collection is a scientific contribution that cannot be exhausted after one use of the data; it resides in the data permanently."&lt;/p&gt;
&lt;p&gt;Well, that's not my opinion at all. In fact it's quite directly opposed to virtually everything I think is important about openness in scientific research. So I chose to resign my position.&lt;/p&gt;
&lt;p&gt;In October 2013, I learned that Wicherts had taken the initiative of exposing the &lt;em&gt;Archives&lt;/em&gt;’ policy in an open letter to the editorial board, in which he says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“[…] I recently learned that data from empirical articles published in the &lt;em&gt;Archives&lt;/em&gt; are not even close to being “open”.&lt;/p&gt;
&lt;p&gt;In fact, a request for data published in the &lt;em&gt;Archives&lt;/em&gt; involves not only a full-blown review committee but also the filling in and signing of an extensive form: http://www.apa.org/pubs/journals/features/arc-data-access-request-form.pdf&lt;/p&gt;
&lt;p&gt;This 15-page form asks for the sending of professional resumes, descriptions of the policies concerning academic integrity at one’s institution, explicit research plans including hypotheses and societal relevance, specification of the types of analyses, full ethics approval of the reanalysis by the IRB, descriptions of the background of the research environment, an indication of the primary source of revenue of one’s institution, dissemination plans of the work to be done with the data, a justification for the data request, manners of storage, types of computers and storage media being used, ways of transmitting data between research team members,  whether data will be encrypted, and signatures of institutional heads.&lt;/p&gt;
&lt;p&gt;The requester of the data also has to sign that (s)he provides an “Offer [of] co-authorship to the data generators on any subsequent publications” and the (s)he will offer to the review committee an “annual data use report that outlines what has been done, that the investigator remains in compliance with the original research proposal, and provide references of any resulting publications.”&lt;/p&gt;
&lt;p&gt;In case of non-compliance of any of these stipulations, the requester can face up to a $10,000 fine as well a future prohibition of data access from work published in the &lt;em&gt;Archives&lt;/em&gt;.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A fine?  Seriously? Kafkaesque!&lt;/p&gt;
&lt;p&gt;Wicherts also notes that “the guidelines with respect to data sharing in the &lt;em&gt;Archives&lt;/em&gt; considerably exceed APA’s Ethical Standard 8.14”. &lt;a href="http://www.apa.org/ethics/code/index.aspx?item=11"&gt;Ethical Standard 8.14&lt;/a&gt; is a default that applies to all APA journals, and says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“After research results are published, psychologists do not withhold the data on which their conclusions are based from other competent professionals who seek to verify the substantive claims through reanalysis and who intend to use such data only for that purpose, provided that the confidentiality of the participants can be protected and unless legal rights concerning proprietary data preclude their release.” &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since this guideline says nothing about fines and co-authorship requirements, we indeed have to conclude that it’s &lt;em&gt;harder&lt;/em&gt; to get data from APA’s open science journal, than it is to get data from its regular journals. Picture that!&lt;/p&gt;
&lt;p&gt;In response to my resignation and Wicherts' letter, the editors have taken an interesting course of action. Rather than change their policy such that their deeds match their name, they have changed their name to match their deeds. The journal is now no longer an "Open Methodology, Open Data, Open Access Journal" but an "Open Methodology, Collaborative Data Sharing, Open Access Journal".&lt;/p&gt;
&lt;p&gt;The APA and open data. One step forward, two steps back.&lt;/p&gt;</summary></entry><entry><title>When Open Science is Hard Science</title><link href="http://osc.centerforopenscience.org/2014/01/08/hard-science/" rel="alternate"></link><updated>2014-01-08T12:00:00-05:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2014-01-08:2014/01/08/hard-science/</id><summary type="html">&lt;p&gt;When it comes to opening up your work there is, ironically, a bit of a secret.  Here it is: being open - in open science, open source software, or any other open community - can be hard.  Sometimes it can be harder than being closed.&lt;/p&gt;
&lt;p&gt;In an effort to attract more people to the cause, advocates of openness tend to tout its benefits.  Said benefits are bountiful: increased collaboration and dissemination of ideas, transparency leading to more frequent error checking, improved reproducibility, easier meta-analysis, and greater diversity in participation, just to name a few.&lt;/p&gt;
&lt;p&gt;But there are downsides, too.  One of those is that it can be difficult to do your research openly.  (Note here that I mean well and openly.  Taking the full contents of your hard drive and dumping it on a server somewhere might be technically open, but it’s not much use to anyone.)&lt;/p&gt;
&lt;p&gt;How is it hard to open up your work?  And why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closed means privacy.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the privacy of my own home, I seldom brush my hair.  Sometimes I spend all day in my pajamas.  I leave my dirty dishes on the table and eat ice cream straight out of the tub.  But when I have visitors, or when I’m going out, I make sure to clean up.&lt;/p&gt;
&lt;p&gt;In the privacy of a closed access project, you might take shortcuts.  You might recruit participants from your own 101 class, or process your data without carefully documenting which steps you took.  You’d never intentionally do something unethical, but you might get sloppy.&lt;/p&gt;
&lt;p&gt;Humans are social animals.  We try to be more perfect for each other than we do for ourselves.  This makes openness better, but it also makes it harder.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Two heads need more explanation than one.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As I mentioned above, taking all your work and throwing it online without organization or documentation is not very helpful.  There’s a difference between access and accessibility.  To create a truly open project, you need to be willing to explain your research to those trying to understand it. &lt;/p&gt;
&lt;p&gt;There are numerous routes towards sharing your work, and the most open projects take more than one.  You can create stellar documentation of your project.  You can point people towards background material, finding good explanations of the way your research methodology was developed or the math behind your data analysis or how the code that runs your stimulus presentation works.  You can design tutorials or trainings for people who want to run your study.  You can encourage people to ask questions about the project, and reply publicly.  You can make sure to do all the above for people at all levels - laypeople, students, and participants as well as colleagues. &lt;/p&gt;
&lt;p&gt;Even closed science is usually collaborative, so hopefully your project is decently well documented.  But making it accessible to everyone is a project in itself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New ideas and tools need to be learned.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As long as closed is the default, we’ll need to learn new skills and tools in the process of becoming open, such as version control, format conversion and database management.&lt;/p&gt;
&lt;p&gt;These skills aren’t unique to working openly.  And if you have a good network of friends and colleagues, you can lean on them to supplement your own expertise.  But the fact remains that “going open” isn’t as easy as flipping a switch.  Unless you’re already well-connected and well-informed, you’ll have a lot to learn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;People can be exhausting.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Making your work open often means dealing with other people - and not always the people you want to deal with.  There are the people who mean well, but end up confusing, misleading, or offending you.  There are the people who don’t mean well at all.  There are the discussions that go off in unproductive directions, the conversations that turn into conflicts, the promises that get forgotten.&lt;/p&gt;
&lt;p&gt;Other people are both a joy and a frustration, in many areas of life beyond open science.  But the nature of openness assures you’ll get your fair share.  This is especially true of open science projects that are explicitly trying to build community.&lt;/p&gt;
&lt;p&gt;It can be all too easy to overlook this &lt;a href="http://en.wikipedia.org/wiki/Emotional_labor#Gender"&gt;emotional labor&lt;/a&gt;, but it’s work - hard work, at that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There are no guarantees.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For all the effort you put into opening up your research, you may find no one else is willing to engage with it.  There are plenty of open source software projects with no forks or new contributors, open science articles that are seldom downloaded or science wikis that remain mostly empty, open government tools or datasets that no one uses.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://opcit.eprints.org/oacitation-biblio.html"&gt;Open access may increase impact&lt;/a&gt; on the whole, but there are no promises for any particular project.  It’s a sobering prospect to someone considering opening up their research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we make open science easier?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can advocate for open science while acknowledging the barriers to achieving it.  And we can do our best to lower those barriers:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Forgive imperfections.&lt;/em&gt;  We need to create an environment where mistakes are routine and failures are expected - only then will researchers feel comfortable exposing their work to widespread review.  That’s a tall order in the cutthroat world of academia, but we can begin with our own roles as teachers, mentors, reviewers, and internet commentators.  Be a role model: encourage others to review your work and point out your mistakes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Share your skills as well as your research.&lt;/em&gt;  Talk about your experiences opening up your research with colleagues.  Host lab meetings, department events, and conference panels to discuss the practical difficulties.  If a training, website, or individual helped you understand some skill or concept, recommend widely.  Talking about the individual steps will help the journey seem less intimidating - and will give others a map for how to get there.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Recognize the hard work of others with words and, if you can, financial support.&lt;/em&gt;  Organization, documentation, mentorship, community management.  These are areas that often get overlooked when it comes to celebrating scientific achievement - and allocating funding.  Yet many open science projects would fail without leadership in these areas.  Contribute what you can and support others who take on these roles.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Collaborate.&lt;/em&gt;  Open source advocates have been creating tools to help share the work involved in opening research - there’s &lt;a href="http://software-carpentry.org/"&gt;Software Carpentry&lt;/a&gt;, the &lt;a href="https://osf.io/"&gt;Open Science Framework&lt;/a&gt;, &lt;a href="http://sagebase.org/platforms-and-services/"&gt;Sage Bionetworks&lt;/a&gt;, and &lt;a href="http://researchcompendia.org/"&gt;Research Compendia&lt;/a&gt;, just to name a few.  But beyond sharing tools, we can share time and resources.  Not every researcher will have the skillset, experience, or personality to quickly and easily open up their work.  Sharing efforts across labs, departments and even schools can lighten the load.  So can open science specialists, if we create a scientific culture where these specialists are trained, utilized and valued.&lt;/p&gt;
&lt;p&gt;We can and should demand open scientific practices from our colleagues and our institutions.  But we can also provide guidelines, tools, resources and sympathy.  Open science is hard.  Let’s not make it any harder.&lt;/p&gt;</summary></entry><entry><title>Timeline of Notable Open Science Events in 2013 - Psychology</title><link href="http://osc.centerforopenscience.org/2014/01/01/open-science-timeline/" rel="alternate"></link><updated>2014-01-01T01:00:00-05:00</updated><author><name>Jon Grahe, Pacific Lutheran University</name></author><id>tag:osc.centerforopenscience.org,2014-01-01:2014/01/01/open-science-timeline/</id><summary type="html">&lt;p&gt;Happy New Year! New Year’s is a great time for reflection and resolution, and when I reflect on 2013, I view it with an air of excitement and promise. As a social psychologist, I celebrated with my many of my colleagues in Washington, DC. at the 25th anniversary of the Association for Psychological Science. There were many celebrations including a ‘80s themed dance night at the Convention. However, this year was also marred by the “Crisis of Confidence” in psychological and broader sciences that has been percolating since the turn of the 21st century. Our timeline begins the year with the &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;’s special issue dedicated to addressing this Crisis. Rather than focusing on the problems, papers in this issue suggested solutions and many of those suggestions emerged as projects in 2013. This timeline focuses on these many Open Science Collaboration successes and initiatives and offers a glimpse at the activity directed at reaching the Scientific Utopia envisioned by so many in the OSC. &lt;/p&gt;
&lt;p&gt;Maybe when APS celebrates its 50th Anniversary, it will also mark the 25th Anniversary of the year that the tide turned on the bad practices that had led to the “Crisis of Confidence”. Perhaps in addition to a ‘13 themed dance band playing Lorde’s “Royals” or Imagine Dragon’s “Demons”, maybe there will be a theme reflecting on changing science practices. With the COS celebrating a 25th anniversary of its own, let us share your memory of the important events from 2013. &lt;/p&gt;
&lt;p&gt;These posts reflect a limited list of psychology-related events that one person noticed. We invite you to add other notable events that you feel are missing from this list, particularly in other scientific areas. Add a comment below with information about any research projects aimed at replication across institutions or initiatives directed at making science practices more transparent. &lt;/p&gt;
&lt;p&gt;&lt;a href="http://cdn.knightlab.com/libs/timeline/latest/embed/index.html?source=0An4eLhySzFmBdFV4Wjh0SkZkajU3dEV6b08tV1p4dmc&amp;amp;font=Bevan-PotanoSans&amp;amp;maptype=TERRAIN&amp;amp;lang=en&amp;amp;height=650"&gt;View the timeline!&lt;/a&gt;&lt;/p&gt;</summary></entry><entry><title>Researcher Degrees of Freedom in Data Analysis</title><link href="http://osc.centerforopenscience.org/2013/12/18/researcher-degrees-of-freedom/" rel="alternate"></link><updated>2013-12-18T12:00:00-05:00</updated><author><name>Sean Mackinnon</name></author><id>tag:osc.centerforopenscience.org,2013-12-18:2013/12/18/researcher-degrees-of-freedom/</id><summary type="html">&lt;p&gt;The enormous amount of options available for modern data analysis is both a blessing and a curse. On one hand, researchers have specialized tools for any number of complex questions. On the other hand, we’re also faced with a staggering number of equally-viable choices, many times without any clear-cut guidelines for deciding between them. For instance, I just popped open SPSS statistical software and counted 18 different ways to conduct post-hoc tests for a one-way ANOVA. Some choices are clearly inferior (e.g., the &lt;a href="http://www.graphpad.com/guides/prism/6/statistics/index.htm?stat_fishers_lsd.htm"&gt;LSD test&lt;/a&gt; doesn’t adjust p-values for multiple comparisons) but it’s possible to defend the use of many of the available options. These ambiguous choice points are sometimes referred to as researcher degrees of freedom.&lt;/p&gt;
&lt;p&gt;In theory, researcher degrees of freedom shouldn’t be a problem. More choice is better, right? The problem arises from two interconnected issues: (a) Ambiguity as to which statistical test is most appropriate and (b) an incentive system where scientists are rewarded with publications, grants, and career stability when their p-values fall below the revered p &amp;lt; .05 criterion. So, perhaps unsurprisingly, when faced with a host of ambiguous options for data analysis, most people settle on the one that achieves statistically significant results. Simmons, Nelson, and Simonsohn (2011) argue that this undisclosed flexibility in data analysis allows people to present almost any data as “significant,” and calls for 10 simple guidelines for reviewers and authors to disclose in every paper – which, if you haven’t read yet &lt;a href="http://pss.sagepub.com/content/22/11/1359.full.pdf+html"&gt;are worth checking out&lt;/a&gt;. In this post, I will discuss a few guidelines of my own for conducting data analysis in a way that strives to overcome our inherent tendency to be self-serving.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make as many data analytic decisions as possible before looking at your data. Review the statistical literature and decide on which statistical test(s) will be best before looking at your collected data. Continue to use those tests until enough evidence emerges to change your mind. The important thing is that you make these decisions before looking at your data. Once you start playing with the actual data, your self-serving biases will start to kick in. Do not underestimate your ability for self-deception: Self-serving biases are powerful, pervasive, and apply to virtually everyone. Consider pre-registering your data analysis plan (perhaps using the &lt;a href="https://openscienceframework.org/"&gt;Open Science Framework&lt;/a&gt; to keep yourself honest and to convince future reviewers that you aren’t exploiting researcher degrees of freedom.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When faced with a situation where there are too many equally viable choices, run a small number of the best choices, and report all of them. In this case, decide on 2-5 different tests ahead of time. Report the results of all choices, and make a tentative conclusion based if the majority of these tests agree. For instance, when determining model fit in structural equation modeling, there &lt;a href="http://davidakenny.net/cm/fit.htm"&gt;many different methods you might use&lt;/a&gt;. If you can’t figure out which method is best by reviewing the statistical literature – it’s not entirely clear, statisticians disagree about as often as any other group of scientists – then report the results of all tests, and make a conclusion if they all converge on the same solution. When they disagree, make a tentative conclusion based on the majority of tests that agree (e.g., 2 of 3 tests come to the same conclusion). For the record, I currently use CFI, TLI, RMSEA, and SRMR in my own work, and use these even if other fit indices provide more favorable results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When deciding on a data analysis plan after you’ve seen the data, keep in mind that most researcher degrees of freedom have minimal impact on strong results. For any number of reasons, you might find yourself deciding on a data analysis plan after you’ve played around with the data for a while. At the end of the day, strong data will not be influenced much by researcher degrees of freedom. For instance, results should look much the same regardless of whether you exclude outliers, transform them, or leave them in the data when you have a study with high statistical power. Simmons et al. (2011) specifically recommend that results should be presented (a) with and without covariates, and (b) with and without specific data points excluded, if any were removed. Again, the general idea is that strong results will not change much when you alter researcher degrees of freedom. Thus, I again recommend analyzing the data in a few different ways and looking for convergence across all methods when you’re developing a data analysis plan after seeing the data. This sets the bar higher to try and combat your natural tendency to report just the one analysis that “works.” When minor data analytic choices drastically change the conclusions, this should be a warning sign that your solution is unstable and the results are probably not trustworthy.  The number one reason why you have an unstable solution is probably because you have &lt;a href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/"&gt;low statistical power&lt;/a&gt;. Since you hopefully had a strict data collection end date, the only viable alternative when results are unstable is to replicate the results in a second, more highly-powered study using the same data analytic approach.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At the end of the day, there is no “quick-fix” for the problem of self-serving biases during data analysis so long as the incentive system continues to reward novel, statistically significant results. However, by using the tips in this article (and elsewhere) researchers can focus on finding strong, replicable results by minimizing the natural human tendency to be self-serving.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Simmons, J. P., Nelson, L. D., &amp;amp; Simonsohn, U. (2011). &lt;a href="http://pss.sagepub.com/content/22/11/1359.full.pdf+html"&gt;False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant.&lt;/a&gt; Psychological Science, 22, 1359-1366. doi:10.1177/0956797611417632&lt;/p&gt;</summary></entry><entry><title>Chasing Paper, Part 3</title><link href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/" rel="alternate"></link><updated>2013-12-13T12:00:00-05:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2013-12-13:2013/12/13/chasing-paper-3/</id><summary type="html">&lt;p&gt;&lt;em&gt;This is part three of a three part post brainstorming potential improvements to the journal article format.  Part one is &lt;a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/"&gt;here&lt;/a&gt;, part two is &lt;a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;The classic journal article is only readable by domain experts.&lt;/h3&gt;
&lt;p&gt;Journal articles are currently written for domain experts.  While novel concepts or terms are usually explained, there is the assumption of a vast array of background knowledge and jargon is the rule, not the exception.  While this leads to quick reading for domain experts, it can make for a difficult slog for everyone else.&lt;/p&gt;
&lt;p&gt;Why is this a problem?  For one thing, it prevents interdisciplinary collaboration.  Researchers will not make a habit of reading outside their field if it takes hours of painstaking, self-directed work to comprehend a single article.  It also discourages public engagement.  While science writers do admirable work boiling hard concepts down to their comprehensible cores, many non-scientists want to actually read the articles, and get discouraged when they can’t.  &lt;/p&gt;
&lt;p&gt;While opaque scientific writing exists in every format, technologies present new options to translate and teach.  Jargon could be linked to a glossary or &lt;a href="https://en.wikipedia.org/wiki/Mouseover"&gt;other reference material&lt;/a&gt;.  You could be given a plain english explanation of a term when your mouse hovers over it.  Perhaps each article could have multiple versions - for domain experts, other scientists, and for laypeople.  &lt;/p&gt;
&lt;p&gt;Of course, the ability to write accessibly is a skill not everyone has.  Luckily, any given paper would mostly use terminology already introduced in previous papers.  If researchers could easily credit the teaching and popularization work done by others, they could acknowledge the value of those contributions while at the same time making their own work accessible.&lt;/p&gt;
&lt;h3&gt;The classic journal article has no universally-agreed upon standards.&lt;/h3&gt;
&lt;p&gt;Academic publishing, historically, has been a distributed system.  Currently, the top three publishers still account for less than half (42%) of all published articles (&lt;a href="http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html"&gt;McGuigan and Russell, 2008&lt;/a&gt;).  While certain format and content conventions are shared among publishers, generally speaking it’s difficult to propagate new standards, and even harder to enforce them.  Not only do standards vary, they are frequently hidden, with most of the review and editing process taking place behind closed doors.&lt;/p&gt;
&lt;p&gt;There are benefits to decentralization, but the drawbacks are clear.  Widespread adoption of new standards, such as &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588"&gt;Simmons et al’s 21 Word Solution&lt;/a&gt; or &lt;a href="http://centerforopenscience.org/journals/"&gt;open science practices&lt;/a&gt;, depends on the hard work and high status of those advocating for them.  How can the article format be changed to better accommodate changing standards, while still retaining individual publishers’ autonomy?&lt;/p&gt;
&lt;p&gt;One option might be to create a new section of each journal article, a free-form field where users could record whether an article met this or that standard.  Researchers could then independently decide what standards they wanted to pay attention to.  While this sounds messy, if properly implemented this feature could be used very much like a search filter, yet would not require the creation or maintenance of a centralized database.&lt;/p&gt;
&lt;p&gt;A different approach is already being embraced: an effort to make the standards that currently exist more transparent by bringing peer review out into the open.  &lt;a href="https://en.wikipedia.org/wiki/Open_peer_review"&gt;Open peer review&lt;/a&gt; allows readers to view an article’s pre-publication history, including the authorship and content of peer reviews, while &lt;a href="http://publications.copernicus.org/services/public_peer_review.html"&gt;public peer review&lt;/a&gt; allows the public to participate in the review process.  However, these methods have yet to be generally adopted.&lt;/p&gt;
&lt;p&gt;*&lt;/p&gt;
&lt;p&gt;It’s clear that journal articles are already changing.  But they may not be changing fast enough.  It may be better to forgo the trappings of the journal article entirely, and seek a new system that more naturally encourages collaboration, curation, and the efficient use of the incredible resources at our disposal.  With journal articles commonly costing more than $30 each, some might jump at the chance to leave them behind.&lt;/p&gt;
&lt;p&gt;Of course, it’s easy to play “what if” and imagine alternatives; it’s far harder to actually implement them.  And not all innovations are improvements.  But with &lt;a href="http://www.battelle.org/media/press-releases/battelle-r-d-magazine-annual-global-funding-forecast-predicts-r-d-spending-growth-will-continue-while-globalization-accelerates"&gt;over a billion dollars&lt;/a&gt; spent on research each day in the United States, with &lt;a href="http://www.bmj.com/content/341/bmj.c6815"&gt;over 25,000 journals&lt;/a&gt; in existence, and over a million articles published each year, surely there is room to experiment.&lt;/p&gt;
&lt;h4&gt;Bibliography&lt;/h4&gt;
&lt;p&gt;Budd, J.M., Coble, Z.C. and Anderson, K.M.  (2011)  &lt;a href="http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf"&gt;Retracted Publications in Biomedicine: Cause for Concern.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wright, K. and McDaid, C.  (2011).  &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/?report=classic"&gt;Reporting of article retractions in bibliographic databases and online journals.&lt;/a&gt;  J Med Libr Assoc. 2011 April; 99(2): 164–167.&lt;/p&gt;
&lt;p&gt;McGuigan, G.S. and Russell, R.D.  (2008).  &lt;a href="http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html"&gt;The Business of Academic Publishing: A Strategic Analysis of the Academic Journal Publishing Industry and its Impact on the Future of Scholarly Publishing&lt;/a&gt;.  Electronic Journal of Academic and Special Librarianship.  Winter 2008; 9(3).&lt;/p&gt;
&lt;p&gt;Simmons, J.P., Nelson, L.D. and Simonsohn, U.A.  (2012)  &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588"&gt;A 21 Word Solution&lt;/a&gt;.  &lt;/p&gt;</summary><category term="chasing-paper"></category></entry><entry><title>Chasing Paper, Part 2</title><link href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/" rel="alternate"></link><updated>2013-12-12T12:00:00-05:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2013-12-12:2013/12/12/chasing-paper-2/</id><summary type="html">&lt;p&gt;&lt;em&gt;This is part two of a three part post brainstorming potential improvements to the journal article format.  Part one is &lt;a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/"&gt;here&lt;/a&gt;, part three is here &lt;a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;The classic journal article format is not easily updated or corrected.&lt;/h3&gt;
&lt;p&gt;Scientific understanding is constantly changing as phenomena are discovered and mistakes uncovered.  The classic journal article, however, is static.  When a serious flaw in an article is found, the best a paper-based system can do is issue a retraction, and hope that a reader going through past issues will eventually come across the change.&lt;/p&gt;
&lt;p&gt;Surprisingly, retractions and corrections continue to go mostly unnoticed in the digital era.  Studies have shown that retracted papers go on to receive, on average, more than 10 post-retraction citations, with less than 10% of those citations acknowledging the retraction (&lt;a href="http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf"&gt;Budd et al, 2011&lt;/a&gt;).  Why is this happening?  While many article databases such as PubMed provide retraction notices, the articles themselves are often not amended.  Readers accessing papers directly from publishers’ websites, or from previously saved copies, can sometimes miss it.  A case study of 18 retracted articles found several which they classified as “high risk of missing [the] notice”, with no notice given in the text of the pdf or html copies themselves (&lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/#mlab.1536-5050.99.2.010.sg002"&gt;Wright et al, 2011&lt;/a&gt;).  It seems likely that corrections have even more difficulty being seen and acknowledged by subsequent researchers.&lt;/p&gt;
&lt;p&gt;There are several technological solutions which can be tried.  One promising avenue would be the adoption of version control.  Also called revision control, this is a way of tracking all changes made to a project.  This technology has been used for decades in computer science and is becoming more and more popular - Wikipedia and Google Docs, for instance, both use version control.  Citations for a paper could reference the version of the paper then available, but subsequent readers would be notified that a more recent version could be viewed.  In addition to making it easy to see how articles have been changed, adopting such a system would acknowledge the frequency of retractions and corrections and the need to check for up to date information.&lt;/p&gt;
&lt;p&gt;Another potential tool would be an alert system.  When changes are made to an article, the authors of all articles which cite it could be notified.  However, this would require the maintenance of up-to-date contact information for authors, and the adoption of communications standards across publishers (something that has been accomplished before with initiatives like &lt;a href="http://www.crossref.org/"&gt;CrossRef&lt;/a&gt;).
A more transformative approach would be to view papers not as static documents but as ongoing projects that can be updated and contributed to over time.  Projects could be tracked through version control from their very inception, allowing for a kind of &lt;a href="http://www.theguardian.com/science/blog/2013/jun/05/trust-in-science-study-pre-registration"&gt;pre-registration&lt;/a&gt;.  Replications and new analyses could be added to the project as they’re completed.  The most insightful questions and critiques from the public could lead to changes in new versions of the article.&lt;/p&gt;
&lt;h3&gt;The classic journal article only recognizes certain kinds of contributions.&lt;/h3&gt;
&lt;p&gt;When journal articles were first developed in the 1600s, the idea of crediting an author or authors must have seemed straightforward.  After all, most research was being done by individuals or very small groups, and there were no such things as curriculum vitae or tenure committees.  Over time, academic authorship has become the single most important factor in determining career success for individual scientists.  The limitations of authorship can therefore have an incredible impact on scientific progress.&lt;/p&gt;
&lt;p&gt;There are two major problems with authorship as it currently functions, and they are sides of the same coin.  Authorship does not tell you what, precisely, each author did on a paper.  And authorship does not tell you who, precisely, is responsible for each part of a paper.
Currently, the authorship model provides only a vague idea of who is responsible for a paper.  While this is sometimes elaborated upon briefly in the footnotes, or mentioned in the article, more often readers employ simple heuristics.  In psychology, the first author is believed to have led the work, the last author to have provided physical and conceptual resources for the experiment, and any middle authors to have contributed in an unknown but significant way.  This is obviously not an ideal way to credit people, and often leads to disputes, with first authorship &lt;a href="http://www.nature.com/naturejobs/science/articles/10.1038/nj7417-591a"&gt;sometimes misattributed&lt;/a&gt;.  It has grown increasingly impractical as &lt;a href="http://archive.sciencewatch.com/newsletter/2012/201207/multiauthor_papers/"&gt;multiauthor papers&lt;/a&gt; have become more and more common.  What does authorship on a 500-author paper even mean?&lt;/p&gt;
&lt;p&gt;The situation is even worse for people whose contributions are not awarded with authorship.  While contributions may be mentioned in the acknowledgements or cited in the body of the paper, neither of these have much impact when scientists are applying for jobs or up for tenure.  This gives them little motivation to do work which will not be recognized with authorship.  And such work is greatly needed.  The development of tools, the collection and release of open data sets, the creation of popularizations and teaching materials, and the deep and thorough review of others’ work - these are all done as favors or side projects, even though they are vital to the progress of research.
How can new technologies address these problems?  There have been few changes made in this area, perhaps due to the heavy weight of authorship in scientific life, although there are some tools like &lt;a href="http://figshare.com/"&gt;Figshare&lt;/a&gt; which allow users to share non-traditional materials such as datasets and posters in citable (and therefore creditable) form.  A more transformative change might be to use the version control system mentioned above.  Instead of tracking changes to the article from publishing onwards, it could follow the article from its beginning stages.  In that way, each change could be attributed to a specific person.&lt;/p&gt;
&lt;p&gt;Another option might simply be to describe contributions in more detail.  Currently if I use your methodology wholesale, or briefly mention a finding of yours, I acknowledge you in the same way - a citation.  What if, instead, all significant contributions were listed?  Although space is not a constraint with digital articles, the human attention span remains limited, and so it might be useful to create common categories for contribution, such as reviewing the article, providing materials, doing analyses, or coming up with an explanation for discussion.&lt;/p&gt;
&lt;p&gt;There are two other problems are worth mentioning in brief.  First, the phenomenon of &lt;a href="https://en.wikipedia.org/wiki/Academic_authorship#Ghost_authorship"&gt;ghost authorship&lt;/a&gt;, where substantial contributions to the running of a study or preparation of a manuscript go unacknowledged.  This is frequently done in industry-sponsored research to hide conflicts of interest.  If journal articles used a format where every contribution was tracked, ghost authorship would be impossible.  Another issue is the assignment of contact authors, the researchers on a paper who readers are invited to direct questions to.  Contact information can become outdated fairly quickly, causing access to data and materials to be lost; if contact information can be changed, or responsibility passed on to a new person, such loss can be prevented.&lt;/p&gt;</summary><category term="chasing-paper"></category></entry><entry><title>Chasing Paper, Part 1</title><link href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/" rel="alternate"></link><updated>2013-12-11T11:30:00-05:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2013-12-11:2013/12/11/chasing-paper/</id><summary type="html">&lt;p&gt;&lt;em&gt;This is part one of a three part post.  Parts &lt;a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/"&gt;two&lt;/a&gt; and &lt;a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/"&gt;three&lt;/a&gt; have now been posted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The academic paper is old - older than the steam engine, the pocket watch, the piano, and the light bulb.  The first journal, &lt;a href="http://rstl.royalsocietypublishing.org/"&gt;Philosophical Transactions&lt;/a&gt;, was published on March 6th, 1665.  Now that doesn’t mean that the journal article format is obsolete - many inventions much older are still in wide use today.  But after a third of a millennium, it’s only natural that the format needs some serious updating.&lt;/p&gt;
&lt;p&gt;When brainstorming changes, it may be useful to think of the limitations of ink and paper.  From there, we can consider how new technologies can improve or even transform the journal article.  Some of these changes have already been widely adopted, while others have never even been debated.  Some are adaptive, using the greater storage capacity of computing to extend the functions of the classic journal article, while others are transformative, creating new functions and features only available in the 21st century.&lt;/p&gt;
&lt;p&gt;The ideas below are suggestions, not recommendations - it may be that some aspects of the journal article format are better left alone.  But we all benefit from challenging our assumptions about what an article is and ought to be.&lt;/p&gt;
&lt;h3&gt;The classic journal article format cannot convey the full range of information associated with an experiment.&lt;/h3&gt;
&lt;p&gt;Until the rise of modern computing, there was simply no way for researchers to share all the data they collected in their experiments.  Researchers were forced to summarize: to gloss over the details of their methods and the reasoning behind their decisions and, of course, to provide statistical analyses in the place of raw data.  While fields like &lt;a href="http://www.techrepublic.com/blog/european-technology/cern-where-the-big-bang-meets-big-data/"&gt;particle physics&lt;/a&gt; and &lt;a href="http://www.nytimes.com/2011/12/01/business/dna-sequencing-caught-in-deluge-of-data.html?pagewanted=all&amp;amp;_r=0"&gt;genetics&lt;/a&gt; continue to push the limits of memory, most experimenters now have the technical capacity to share all of their data.&lt;/p&gt;
&lt;p&gt;Many journals have taken to publishing supplemental materials, although this rarely encompasses the entirety of data collected, or enough methodological detail to allow for independent replication.  There are plenty of explanations for this slow adoption, including ethical considerations around human subjects data, the potential to patent methods, or the cost to journals of hosting this extra materials.  But these are obstacles to address, not reasons to give up.  The potential benefits are enormous:  What if every published paper contained enough methodological detail that it could be independently replicated?  What if every paper contained enough raw data that it could be included in meta-analysis?  How much of meta-scientific work is never undertaken, because it's dependent on getting dozens or hundreds of contact authors to return your emails, and on universities to properly store data and materials?&lt;/p&gt;
&lt;p&gt;Providing supplemental material, no matter how extensive, is still an adaptive change.  What might a transformative change look like?  Elsevier’s &lt;a href="http://www.articleofthefuture.com/"&gt;Article of the Future&lt;/a&gt; project attempts to answer that question with new, experimental formats that include videos, interactive models, and infographics.  These designs are just the beginning.  What if articles allowed readers to actually interact with the data and perform their own analyses?  &lt;a href="https://en.wikipedia.org/wiki/Virtual_environment_software"&gt;Virtual environments&lt;/a&gt; could be set up, lowering the barrier to independent verification of results.  What if authors reported when they made questionable methodological decisions, and allowed readers, where possible, to see the results when a variable was not controlled for, or a sample was not excluded?&lt;/p&gt;
&lt;h3&gt;The classic journal article format is difficult to organize, index or search.&lt;/h3&gt;
&lt;p&gt;New technology has already transformed the way we search the scientific literature.  Where before researchers were reliant on catalogues and indexes from publishers, and used abstracts to guess at relevance, databases such as PubMed and Google Scholar allow us to find all mentions of a term, tool, or phenomena across vast swathes of articles.  While searching databases is itself a skill, its one that allows us to search comprehensively and efficiently, and gives us more opportunities to explore.&lt;/p&gt;
&lt;p&gt;Yet old issues of organization and curation remain.  Indexes used to speed the slow process of skimming through physical papers.  Now they’re needed to help researchers sort through the abundance of articles constantly being published.  With &lt;a href="http://duncan.hull.name/2010/07/15/fifty-million/"&gt;tens of millions&lt;/a&gt; of journal articles out there, how can we be sure we’re really accessing all the relevant literature?  How can we compare and synthesize the thousands of results one might get on a given search?&lt;/p&gt;
&lt;p&gt;Special kinds of articles - reviews and meta-analyses - have traditionally helped us synthesize and curate information.  As discussed above, new technologies can help make meta-analyses more common by making it easier for researchers to access information about past studies.  We can further improve the search experience by creating more detailed &lt;a href="https://en.wikipedia.org/wiki/Metadata_standards"&gt;metadata&lt;/a&gt;.  Metadata, in this context, is the information attached to an article which lets us categorize it without having to read the article itself.  Currently, fields like title, author, date, and journal are quite common in databases.  More complicated fields less often adopted, but you can find metadata on study type, population, level of clinical trial (where applicable), and so forth.  What would truly comprehensive metadata look like?  Is it possible to store the details of experimental structure or analysis in machine-readable format - and is that even desirable?&lt;/p&gt;
&lt;p&gt;What happens when we reconsider not the metadata but the content itself?  Most articles are structurally complex, containing literature reviews, methodological information, data, and analysis.  Perhaps we might be better served by breaking those articles down into their constituent parts.  What if methods, data, analysis were always published separately, creating a network of papers that were linked but discrete?  Would that be easier or harder to organize?  It may be that what we need here is not a better kind of journal article, but a new way of curating research entirely.&lt;/p&gt;</summary><category term="chasing-paper"></category></entry><entry><title>New “Reviewer Statement” Initiative Aims to (Further) Improve Community Norms Toward Disclosure</title><link href="http://osc.centerforopenscience.org/2013/12/09/reviewer-statement-initiative/" rel="alternate"></link><updated>2013-12-09T11:30:00-05:00</updated><author><name>Etienne LeBel</name></author><id>tag:osc.centerforopenscience.org,2013-12-09:2013/12/09/reviewer-statement-initiative/</id><summary type="html">&lt;p&gt;&lt;img src="images/etiennelebel.jpg" alt="Photo of Etienne LeBel" align="left" style="padding-right: 20px;" /&gt;&lt;/p&gt;
&lt;p&gt;An Open Science Collaboration -- made up of Uri Simonsohn, Etienne LeBel, Don Moore, Leif D. Nelson, Brian Nosek, and Joe Simmons -- is glad to announce a new initiative aiming to improve community norms toward the disclosure of basic methodological information during the peer-review process. Endorsed by the Center for Open Science, the initiative involves a standard reviewer statement that any peer reviewer can include in their review requesting that authors add a statement to the paper confirming that they have disclosed all data exclusions, experimental conditions, assessed measures, and how they determined their samples sizes (following from the 21-word solution; Simmons, Nelson, &amp;amp; Simonsohn, 2012, 2013; see also &lt;a href="http://psychdisclosure.org/"&gt;PsychDisclosure.org&lt;/a&gt;; LeBel et al., 2013). Here is the statement, which is &lt;a href="http://osf.io/project/hadz3"&gt;available on the Open Science Framework&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;"I request that the authors add a statement to the paper confirming whether, for all experiments, they have reported all measures, conditions, data exclusions, and how they determined their sample sizes. The authors should, of course, add any additional text to ensure the statement is accurate. This is the standard reviewer disclosure request endorsed by the Center for Open Science (&lt;a href="http://osf.io/project/hadz3"&gt;see http://osf.io/project/hadz3&lt;/a&gt;). I include it in every review."&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The idea originated from the realization that as peer reviewers, we typically lack fundamental information regarding how the data was collected and analyzed which prevents us from be able to properly evaluate the claims made in a submitted manuscript. Some reviewers interested in requesting such information, however, were concerned that such requests would make them appear selective and/or compromise their anonymity. Discussions ensued and contributors developed a standard reviewer disclosure request statement that overcomes these concerns and allows the community of reviewers to improve community norms toward the disclosure of such methodological information across all journals and articles.&lt;/p&gt;
&lt;p&gt;Some of the contributors, including myself, were hoping for a reviewer statement with a bit more teeth. For instance, requesting the disclosure of such information as a requirement before accepting to review an article or requiring the re-review of a revised manuscript once the requested information has been disclosed. The team of contributors, however, ultimately decided that it would be better to start small to get acceptance, in order to maximize the probability that the initiative has an impact in shaping the community norms.&lt;/p&gt;
&lt;p&gt;Hence, next time you are invited to review a manuscript for publication at any journal, please remember to include the reviewer disclosure statement!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K. R., Ratliff, K. A., &amp;amp; Smith, C. T. (2013). &lt;a href="http://www.google.com/url?q=http%3A%2F%2Fpps.sagepub.com%2Fcontent%2F8%2F4%2F424.full&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNEcYQrJzKL33sb33l9teszIxVUNAg"&gt;PsychDisclosure.org: Grassroots support for reforming reporting standards in psychology.&lt;/a&gt; Perspectives on Psychological Science, 8(4), 424-432. doi: 10.1177/1745691613491437&lt;/p&gt;
&lt;p&gt;Simmons J., Nelson L. &amp;amp; Simonsohn U. (2011) &lt;a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1850704&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGqvW1VEcf3RzOb7zE0Y25FXdQHBQ"&gt;False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allow Presenting Anything as Significant.&lt;/a&gt; Psychological Science, 22(11), 1359-1366.&lt;/p&gt;
&lt;p&gt;Simmons J., Nelson L. &amp;amp; Simonsohn U. (2012) &lt;a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D2160588&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGt1mELWBFnDRS2Yh-E-qm_tAp_2A"&gt;A 21 Word Solution.&lt;/a&gt; Dialogue: The Official Newsletter of the Society for Personality and Social Psychology, 26(2), 4-7.&lt;/p&gt;</summary></entry><entry><title>The State of Open Access</title><link href="http://osc.centerforopenscience.org/2013/11/27/the-state-of-open-access/" rel="alternate"></link><updated>2013-11-27T02:15:00-05:00</updated><author><name>Shauna Gordon-McKeon</name></author><id>tag:osc.centerforopenscience.org,2013-11-27:2013/11/27/the-state-of-open-access/</id><summary type="html">&lt;p&gt;To celebrate &lt;a href="http://www.openaccessweek.org/"&gt;Open Access Week&lt;/a&gt; last month, we asked people four questions about the state of open access and how it's changing.  Here are some in depth answers from two people working on open access: &lt;a href="http://cyber.law.harvard.edu/~psuber/wiki/Peter_Suber"&gt;Peter Suber&lt;/a&gt;, Director of the &lt;a href="https://osc.hul.harvard.edu/"&gt;Harvard Office for Scholarly Communication&lt;/a&gt; and the &lt;a href="http://cyber.law.harvard.edu/research/hoap"&gt;Harvard Open Access Project&lt;/a&gt;, and &lt;a href="http://www.plos.org/staff/elizabeth-silva/"&gt;Elizabeth Silva&lt;/a&gt;, associate editor at the Public Library of Science (&lt;a href="http://www.plos.org/"&gt;PLOS&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How is your work relevant to the changing landscape of Open Access?  What would be a successful outcome of your work in this area?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Elizabeth&lt;/em&gt;:  PLOS is now synonymous with open access publishing, so it’s hard to believe that 10 years ago, when PLOS was founded, most researchers were not even aware that availability of research was a problem. We all published our best research in the best journals. We assumed our colleagues could access it, and we weren’t aware of (or didn’t recognize the problem with) the inability of people outside of the ivory tower to see this work. At that time it was apparent to the founders of PLOS, who were among the few researchers who recognized the problem, that the best way to convince researchers to publish open access would be for PLOS to become an open access publisher, and prove that OA could be a viable business model and an attractive publishing venue at the same time. I think that we can safely say that the founders of PLOS succeeded in this mission, and they did it decisively. &lt;/p&gt;
&lt;p&gt;We’re now at an exciting time, where open access in the natural sciences is all but inevitable. We now get to work on new challenges, trying to solve other issues in research communication. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Peter&lt;/em&gt;:  My current job has two parts. I direct the Harvard Office for Scholarly Communication (OSC), and I direct the Harvard Open Access Project (HOAP). The OSC aims to provide OA to research done at Harvard University. We implement Harvard's OA policies and maintain its OA repository. We focus on peer-reviewed articles by faculty, but are expanding to other categories of research and researchers. In my HOAP work, I consult pro bono with universities, scholarly societies, publishers, funding agencies, and governments, to help them adopt effective OA policies. HOAP also maintains a guide to good practices for university OA policies, manages the Open Access Tracking Project, writes reference pages on federal OA-related legislation, such as FASTR, and makes regular contributions to the Open Access Directory and the catalog of OA journals from society publishers. &lt;/p&gt;
&lt;p&gt;To me success would be making OA the default for new research in every field and language. However, this kind of success more like a new plateau than a finish line. We often focus on the goal of OA itself, or the goal of removing access barriers to knowledge. But that's merely a precondition for an exciting range of new possibilities for making use of that knowledge. In that sense, OA is closer to the minimum than the maximum of how to take advantage of the internet for improving research. Once OA is the default for new research, we can give less energy to attaining it and more energy to reaping the benefits, for example, integrating OA texts with open data, improving the methods of meta-analysis and reproducibility, and building better tools for knowledge extraction, text and data mining, question answering, reference linking, impact measurement, current awareness, search, summary, translation, organization, and recommendation.&lt;/p&gt;
&lt;p&gt;From the researcher's side, making OA the new default means that essentially all the new work they write, and essentially all the new work they want to read, will be OA. From the publisher's side, making OA the new default means that sustainability cannot depend on access barriers that subtract value, and must depend on creative ways to add value to research that is already and irrevocably OA. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do you think the lack of Open Access is currently impacting how science is practiced?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Peter&lt;/em&gt;:  The lack of OA slows down research. It distorts inquiry by making the retrievability of research a function of publisher prices and library budgets rather than author consent and internet connectivity. It hides results that happen to sit in journals that exceed the affordability threshold for you or your institution. It limits the correction of scientific error by limiting the number of eyeballs that can examine new results. It prevents the use of text and data mining to supplement human analysis with machine analysis. It hinders the reproducibility of research by excluding many who would want to reproduce it. At the same time, and ironically, it increases the inefficient duplication of research by scholars who don't realize that certain experiments have already been done. &lt;/p&gt;
&lt;p&gt;It prevents journalists from reading the latest developments, reporting on them, and providing direct, usable links for interested readers. It prevents unaffiliated scholars and the lay public from reading new work in which they may have an interest, especially in the humanities and medicine. It blocks research-driven industries from creating jobs, products, and innovations. It prevents taxpayers from maximizing the return on their enormous investment in publicly-funded research.&lt;/p&gt;
&lt;p&gt;I assume we're talking about research that authors publish voluntarily, as opposed to notes, emails, and unfinished manuscripts, and I assume we're talking about research that authors write without expectation of revenue. If so, then the lack of OA harms research and researchers without qualification. The lack of OA benefits no one except conventional publishers who want to own it, sell it, and limit the audience to paying customers. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Elizabeth&lt;/em&gt;:  There is a prevailing idea that those that need access to the literature already have it; that those that have the ability to understand the content are at institutions that can afford the subscriptions. First, this ignores the needs of physicians, educators, science 
communicators, and smaller institutions and companies. More fundamentally, limiting access to knowledge, so that rests in the hands of an elite 1%, is archaic, backwards, and counterproductive. There has never been a greater urgency to find solutions to problems that fundamentally threaten human existence – climate change, disease transmission, food security – and in the face of this why would we advocate limited dissemination of knowledge? Full adoption of open access has the potential to fundamentally change the pace of scientific progress, as we make this information available to everyone, worldwide.&lt;/p&gt;
&lt;p&gt;When it comes to issues of reproducibility, fraud or misreporting, all journals face similar issues regardless of the business model. Researchers design their experiments and collect their data long before they decide the publishing venue, and the quality of the reporting likely won’t change based on whether the venue is OA. I think that these issues are better tackled by requirements for open data and improved reporting. Of course these philosophies are certainly intrinsically linked – improved transparency and access can only improve matters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What do you think is the biggest reason that people resist Open Access?  Do you think there are good reasons for not making a paper open access?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Elizabeth&lt;/em&gt;:  Of course there are many publishers who resist open access, which reflects a need to protect established revenue streams. In addition to large commercial publishers, there are a lot of scholarly societies whose primary sources of income are the subscriptions for the journals they publish.&lt;/p&gt;
&lt;p&gt;Resistance from authors, in my experience, comes principally in two forms. The first is linked to the impact factor, rather than the business model. Researchers are stuck in a paradigm that requires them to publish as ‘high’ as possible to achieve career advancement. While there are plenty of high impact OA publications with which people choose to publish, it just so happens that the highest are subscription journals. We know that open access increases utility, visibility and impact of individual pieces of research, but the fallacy that a high impact journal is equivalent to high impact research persists.&lt;/p&gt;
&lt;p&gt;The second reason cited is that the cost is prohibitory. This is a problem everyone at PLOS can really appreciate, and we very much sympathize with authors who do not have the money in their budget to pay author publication charges (APCs). However, it’s a problem that should really be a lot easier to overcome. If research institutions were to pay publication fees, rather than subscription fees, they would save a fortune; a few institutions have realized this and are paying the APCs for authors who choose to go OA. It would also help if funders could recognize publishing as an intrinsic part of the research, folding the APC into the grant. We are also moving the technology forward in an effort to reduce costs, so that savings can be passed onto authors. PLOS ONE has been around for nearly 7 years, and the fees have not changed. This reflects efforts to keep costs as low as we can. Ironically, the biggest of the pay-walled journals already charge authors to publish: for example, it can be between $500 and $1000 for the first color figure, and a few hundred for each additional one; on top of this there are page charges and reprint costs. Not only is the public paying for the research and the subscription, they are paying for papers that they can’t read.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Peter&lt;/em&gt;:  There are no good reasons for not making a paper OA, or at least for not wanting to. &lt;/p&gt;
&lt;p&gt;There are sometimes reasons not to publish in an OA journal. For example, the best journals in your field may not be OA. Your promotion and tenure committee may give you artificial incentives to limit yourself to a certain list of journals. Or the best OA journals in your field may charge publication fees which your funder or employer will not pay on your behalf. However, in those cases you can publish in a non-OA journal and deposit the peer-reviewed manuscript in an OA repository. &lt;/p&gt;
&lt;p&gt;The resistance of non-OA publishers is easier to grasp. But if we're talking about publishing scholars, not publishers, then the largest cause of resistance by far is misunderstanding. Far too many researchers still accept false assumptions about OA, such as these 10:&lt;/p&gt;
&lt;p&gt;--that the only way to make an article OA is to publish it in an OA journal
--that all or most OA journals charge publication fees
--that all or most publication fees are paid by authors out of pocket
--that all or most OA journals are not peer reviewed
--that peer-reviewed OA journals cannot use the same standards and even the same people as the best non-OA journals
--that publishing in a non-OA journal closes the door on lawfully making the same article OA
--that making work OA makes it harder rather than easier to find
--that making work OA limits rather than enhances author rights over it
--that OA mandates are about submitting new work to OA journals rather than depositing it in OA repositories, or
--that everyone who needs access already has access. &lt;/p&gt;
&lt;p&gt;In a &lt;a href="http://www.theguardian.com/higher-education-network/blog/2013/oct/21/open-access-myths-peter-suber-harvard"&gt;recent article&lt;/a&gt; in &lt;em&gt;The Guardian&lt;/em&gt; I corrected six of the most widespread and harmful myths about OA. In a &lt;a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:4322571"&gt;2009 article&lt;/a&gt;, I corrected 25. And in my 2012 &lt;a href="http://bit.ly/oa-book"&gt;book&lt;/a&gt;, I tried to take on the whole legendarium. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How has the Open Access movement changed in the last five years?  How do you think it will change in the next five years?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Peter&lt;/em&gt;:  OA has been making unmistakable progress for more than 20 years. Five years ago we were not in a qualitatively different place. We were just a bit further down the slope from where we are today.&lt;/p&gt;
&lt;p&gt;Over the next five years, I expect more than just another five years' worth of progress as usual. I expect five years' worth of progress toward the kind of success I described in my answer to your first question. In fact, insofar as progress tends to add cooperating players and remove or convert resisting players, I expect five years' worth of compound interest and acceleration. &lt;/p&gt;
&lt;p&gt;In some fields, like particle physics, OA is already the default. In the next five years we'll see this new reality move at an uneven rate across the research landscape. Every year more and more researchers will be able to stop struggling for access against needless legal, financial, and technical barriers. Every year, those still struggling will have the benefit of a widening circle of precedents, allies, tools, policies, best practices, accommodating publishers, and alternatives to publishers.&lt;/p&gt;
&lt;p&gt;Green OA mandates are spreading among universities. They're also spreading among funding agencies, for example, in the US, the EU, and global south. This trend will definitely continue, especially with the support it has received from Global Research Council, Science Europe, the G8 Science Ministers, and the World Bank. &lt;/p&gt;
&lt;p&gt;With the exception of the UK and the Netherlands, countries adopting new OA policies are learning from the experience of their predecessors and starting with green. I've argued in many places that mandating gold OA is a mistake. But it's a mistake mainly for historical reasons, and historical circumstances will change. Gold OA mandates are foolish today in part because too few journals are OA, and there's no reason to limit the freedom of authors to publish in the journals of their choice. But the percentage of peer-reviewed journals that are OA is growing and will continue to grow. (Today it's about 30%.) Gold OA mandates are also foolish today because gold OA is much more expensive than green OA, and there's no reason to compromise the public interest in order to guarantee revenue for non-adaptive publishers. But the costs of OA journals will decline, as the growing number of OA journals compete for authors, and the money to pay for OA journals will grow as libraries redirect money from conventional journals to OA. &lt;/p&gt;
&lt;p&gt;We'll see a rise in policies linking deposit in repositories with research assessment, promotion, and tenure. These policies were pioneered by the University of Liege, and since adopted at institutions in nine countries, and recommended by the Budapest Open Access Initiative, the UK House of Commons Select Committee on Business, Innovation and Skills, and the Mediterranean Open Access Network. Most recently, this kind of policy has been proposed at the national level by the Higher Education Funding Council for England. If it's adopted, it will mitigate the damage of a gold-first policy in the UK. A similar possibility has been suggested for the Netherlands.&lt;/p&gt;
&lt;p&gt;I expect we'll see OA in the humanities start to catch up with OA in the sciences, and OA for books start to catch up with OA for articles. But in both cases, the pace of progress has already picked up significantly, and so has the number of people eager to see these two kinds of progress accelerate.&lt;/p&gt;
&lt;p&gt;The recent decision that Google's book scanning is fair use means that a much larger swath of print literature will be digitized, if not in every country, then at least in the US, and if not for OA, then at least for searching. This won't open the doors to vaults that have been closed, but it will open windows to help us see what is inside.&lt;/p&gt;
&lt;p&gt;Finally, I expect to see evolution in the genres or containers of research. Like most people, I'm accustomed to the genres I grew up with. I love articles and books, both as a reader and author. But they have limitations that we can overcome, and we don't have to drop them to enhance them or to create post-articles and post-books alongside them. The low barriers to digital experimentation mean that we can try out new breeds until we find some that carry more advantages than disadvantages for specific purposes. Last year I sketched out one idea along these lines, which I call an &lt;a href="http://dash.harvard.edu/bitstream/handle/1/10055732/12-02-12.html?sequence=2#rack"&gt;evidence rack&lt;/a&gt;, but it's only one in an indefinitely large space constrained only by the limits on our imagination. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Elizabeth&lt;/em&gt;: It’s starting to feel like universal open access is no longer “if” but “when”. In the next five years we will see funders and institutions recognize the importance of access and adopt policies that mandate and financially support OA; resistance will fade away, and it will simply be the way research is published. As that happens, I think the OA movement will shift towards tackling other issues in research communication: providing better measures of impact in the form of article level metrics, decreasing the time to publication, and improving reproducibility and utility of research.&lt;/p&gt;</summary></entry><entry><title>Theoretical Amnesia</title><link href="http://osc.centerforopenscience.org/2013/11/20/theoretical-amnesia/" rel="alternate"></link><updated>2013-11-20T02:00:00-05:00</updated><author><name>Denny Borsboom</name></author><id>tag:osc.centerforopenscience.org,2013-11-20:2013/11/20/theoretical-amnesia/</id><summary type="html">&lt;p&gt;&lt;img src="images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /&gt;&lt;/p&gt;
&lt;p&gt;In the past few months, the Center for Open Science and its associated enterprises have gathered enormous support in the community of psychological scientists. While these developments are happy ones, in my view, they also cast a shadow over the field of psychology: clearly, many people think that the activities of the Center for Open Science, like organizing massive replication work and promoting preregistration, are &lt;em&gt;necessary&lt;/em&gt;. That, in turn, implies that something in the current scientific order is seriously &lt;em&gt;broken&lt;/em&gt;. I think that, apart from working towards improvements, it is useful to investigate what that something is. In this post, I want to point towards a factor that I think has received too little attention in the public debate; namely, the near absence of unambiguously formalized scientific theory in psychology.&lt;/p&gt;
&lt;p&gt;Scientific theories are perhaps the most bizarre entities that the scientific imagination has produced. They have incredible properties that, if we weren’t so familiar with them, would do pretty well in a &lt;em&gt;Harry Potter&lt;/em&gt; novel. For instance, scientific theories allow you to work out, on a piece of paper, what would happen to stuff in conditions that aren’t actually realized. So you can figure out whether an imaginary bridge will stand or collapse in imaginary conditions. You can do this by simply just feeding some imaginary quantities that your imaginary bridge would have (like its mass and dimensions) to a scientific theory (say, Newton’s) and out comes a prediction on what will happen. In the more impressive cases, the predictions are so good that you can actually design the entire bridge on paper, then build it according to specifications (by systematically mapping empirical objects to theoretical terms), and then the bridge will do precisely what the theory says it should do. No surprises.&lt;/p&gt;
&lt;p&gt;That’s how they put a man on the moon and that’s how they make the computer screen you’re now looking at. It’s all done in theory before it’s done for real, and that’s what makes it possible to construct complicated but functional pieces of equipment. This is, in effect, why scientific theory makes technology possible, and therefore this is an absolutely central ingredient of the scientific enterprise which, &lt;em&gt;without&lt;/em&gt; technology, would be much less impressive than it is.&lt;/p&gt;
&lt;p&gt;It’s useful to take stock here, and marvel. A good scientific theory allows you infer what would happen to things in certain situations &lt;em&gt;without creating the situations&lt;/em&gt;. Thus, scientific theories are crystal balls that actually work. For this reason, some philosophers of science have suggested that scientific theories should be interpreted as &lt;em&gt;inference tickets&lt;/em&gt;. Once you’ve got the ticket, you get to sidestep all the tedious empirical work. Which is great, because empirical work is, well, tedious. Scientific theories are thus exquisitely suited to the needs of lazy people.&lt;/p&gt;
&lt;p&gt;My field – psychology – unfortunately does not afford much of a lazy life. We don’t have theories that can offer predictions sufficiently precise to intervene in the world with appreciable certainty. That’s why there exists no such thing as a psychological engineer. And that’s why there are fields of theoretical physics, theoretical biology, and even theoretical economics, while there is no parallel field of theoretical psychology. It is a sad but, in my view, inescapable conclusion: we don’t have much in the way of scientific theory in psychology. For this reason, we have very few inference tickets – let alone inference tickets that work.&lt;/p&gt;
&lt;p&gt;And that’s why psychology is so hyper-ultra-mega empirical. We never know how our interventions will pan out, because we have no theory that says how they will pan out (incidentally, that’s also why we need preregistration: in psychology, predictions are made by individual researchers rather than by standing theory, and you can’t trust people the way you can trust theory). The upshot is that, if we want to know what would happen if we did X, we have to actually do X. Because we don’t have inference tickets, we never get to take the shortcut. We always have to wade through the empirical morass. Always.&lt;/p&gt;
&lt;p&gt;This has important consequences. For instance, as a field has less theory, it has to leave more to the data. Since you can’t learn anything from data without the armature of statistical analysis, a field without theory tends to grow a thriving statistical community. Thus, the role of statistics grows as soon as the presence of scientific theory wanes. In extreme cases, when statistics has entirely taken over, fields of inquiry can actually develop a kind of philosophical disorder: theoretical amnesia. In fields with this disorder, researchers no longer know what a theory is, which means that they can neither recognize its presence nor its absence. In such fields, for instance, a statistical model – like a factor model – can come to occupy the vacuum created by the absence of theory. I am often afraid that this is precisely what has happened with the advent of “theories” like those of general intelligence (a single factor model) and the so-called “Big Five” of personality (a five-factor model). In fact, I am afraid that this happened in many fields in psychology, where statistical models (which, in their barest null-hypothesis testing form, are misleadingly called “effects”) rule the day.&lt;/p&gt;
&lt;p&gt;If your science thrives on experiment and statistics, but lacks the power of theory, you get peculiar problems. Most importantly, you get slow. To see why, it’s interesting to wonder how psychologists would build a bridge, if they were to use their typical methodological strategies. Probably, they would build a thousand bridges, record whether they stand or fall, and then fit a regression equation to figure out which properties are predictive of the outcome. Predictors would be chosen on the basis of statistical significance, which would introduce a multiple testing problem. In response, some of the regressors might be clustered through factor analysis, to handle the overload of predictive variables. Such analyses would probably indicate lots of structure in the data, and psychologists would likely find that the bridges’ weight, size, and elasticity loads on a single latent “strength factor”, producing the “theory” that bridges higher on the “strength factor” are less likely to fall down. Cross validation of the model would be attempted by reproducing the analysis in a new sample of a thousand bridges, to weed out chance findings. It’s likely that, after many years of empirical research, and under a great number of “context-dependent” conditions that would be poorly understood, psychologists would be able to predict a modest but significant proportion of the variance in the outcome variable. Without a doubt, it would ta  ke a thousand years to establish empirically what Newton grasped in a split second, as he wrote down his F=m*a.&lt;/p&gt;
&lt;p&gt;Because increased reliance on empirical data makes you so incredibly slow, it also makes you susceptible to fads and frauds. A good theory can be tested in an infinity of ways, many of which are directly available to the interested reader (this is what give classroom demonstrations such enormous evidential force). But if your science is entirely built on generalizations derived from specifics of tediously gathered experimental data, you can’t really test these generalizations without tediously gathering the same, or highly similar, experimental data. That’s not something that people typically like to do, and it’s certainly not what journals want to print. As a result, a field can become dominated by poorly tested generalizations. When that happens, you’re in very big trouble. The reason is that your scientific field becomes susceptible to the equivalent of what evolutionary theorists call free riders: people who capitalize on the invested honest work of others by consistently taking the moral shortcut. Free riders can come to rule a scientific field if two conditions are satisfied: (a) fame is bestowed on whoever dares to make the most &lt;em&gt;adventurous&lt;/em&gt; claims (rather than the most &lt;em&gt;defensible&lt;/em&gt; ones), and (b) it takes longer to falsify a bogus claim than it takes to become famous. If these conditions are satisfied, you can build your scientific career on a fad and get away with it. By the time they find out your work really doesn’t survive detailed scrutiny, you’re sitting warmly by the fire in the library of your National Academy of Sciences&lt;sub&gt;1&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;Much of our standard methodological teachings in psychology rest on the implicit assumption that scientific fields are similar if not identical in their methodological setup. That simply isn’t true. Without theory, the scientific ball game has to be played by different rules. I think that these new rules are now being invented: without good theory, you need fast acting replication teams, you need a reproducibility project, and you need preregistered hypotheses. Thus, the current period of crisis may lead to extremely important methodological innovations – especially those that are crucial in fields that are low on theory.&lt;/p&gt;
&lt;p&gt;Nevertheless, it would be extremely healthy if psychologists received more education in fields which do have some theories, even if they are empirically shaky ones, like you often see in economics or biology. In itself, it’s no shame that we have so little theory: psychology probably has the hardest subject matter ever studied, and to change that may very well take a scientific event of the order of Newton’s discoveries. I don’t know how to do it and I don’t think anybody else knows either. But what we can do is keep in contact with other fields, and at least try to remember what theory is and what it’s good for, so that we don’t fall into theoretical amnesia. As they say, it’s the unknown unknowns that hurt you most.&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;1  Caveat: I am not saying that people do this on purpose. I believe that free riders are typically unaware of the fact that they are free riders – people are very good at labeling their own actions positively, especially if the rest of the world says that they are brilliant. So, if you think this post isn’t about you, that could be entirely wrong. In fact, I cannot even be   sure that this post isn’t about me.&lt;/sub&gt;&lt;/p&gt;</summary></entry><entry><title>Let’s Report Our Findings More Transparently – As We Used to Do</title><link href="http://osc.centerforopenscience.org/2013/11/13/report-findings-more-transparently/" rel="alternate"></link><updated>2013-11-13T12:00:00-05:00</updated><author><name>Etienne LeBel</name></author><id>tag:osc.centerforopenscience.org,2013-11-13:2013/11/13/report-findings-more-transparently/</id><summary type="html">&lt;p&gt;&lt;img src="images/etiennelebel.jpg" alt="Photo of Etienne LeBel" align="left" style="padding-right: 20px;" /&gt;&lt;/p&gt;
&lt;p&gt;In 1959, Festinger and Carlsmith reported the results of an experiment that spawned a voluminous body of research on cognitive dissonance. In that experiment, all subjects performed a boring task. Some participants were paid $1 or $20 to tell the next subject the task was interesting and fun whereas participants in a control condition did no such thing. All participants then indicated how enjoyable they felt the task was, their desire to participate in another similar experiment, and the scientific importance of the experiment. The authors hypothesized that participants paid $1 to tell the next participant that the boring task they just completed was interesting would experience internal dissonance, which could be reduced by altering their attitudes on the three outcomes measures. A little known fact about the results of this experiment, however, is that only on &lt;em&gt;one&lt;/em&gt; of these outcome measures did a statistically significant effect emerge across conditions. The authors reported that subjects paid $1 enjoyed the task more than those paid $20 (or the control participants), but no statistically significant differences emerged on the other two measures. &lt;/p&gt;
&lt;p&gt;In another highly influential paper, Word, Zanna, and Cooper (1974) documented the self-fulfilling prophecies of racial stereotypes. The researchers had white subjects interview trained white and black applicants and coded for six non-verbal behaviors of immediacy (distance, forward lean, eye contact, shoulder orientation, interview length, and speech error rate). They found that that white interviewers treated black applicants with lower levels of non-verbal immediacy than white applicants. In a follow-up study involving white subjects only, applicants treated with less immediate non-verbal behaviors were judged to perform less well during the interview than applicants treated with more immediate non-verbal behaviors. A fascinating result, however, a little known fact about this paper is that only three of the six measures of non-verbal behaviors assessed in the first study (and subsequently used in the second study) were statistically significant. &lt;/p&gt;
&lt;p&gt;What do these two examples make salient in relation to how psychologists report their results nowadays? Regular readers of prominent journals like &lt;em&gt;Psychological Science&lt;/em&gt; and &lt;em&gt;Journal of Personality and Social Psychology&lt;/em&gt; may see what I’m getting at: It is very rare these days to see articles in these journals wherein half or most of the reported dependent variables (DVs) fail to show statistically significant effects. Rather, one typically sees squeaky-clean looking articles where all of the DVs show statistically significant effects across all of the studies, with an occasional mention of a DV achieving “marginal significance” (Giner-Sorolla, 2012).&lt;/p&gt;
&lt;p&gt;In this post, I want us to consider the possibility that psychologists’ reporting practices may have changed in the past 50 years. This then raises the question as to how this came about. One possibility is that as incentives became increasingly more perverse in psychology (Nosek,Spies, &amp;amp; Motyl, 2012), some researchers realized that they could out-compete their peers by reporting “cleaner” looking results which would appear more compelling to editors and reviewers (Giner-Sorolla, 2012). For example, decisions were made to simply not report DVs that failed to show significant differences across conditions or that only achieved “marginal significance”. Indeed, nowadays sometimes even editors or reviewers will demand that such DVs not be reported (see &lt;a href="http://PsychDisclosure.org"&gt;PsychDisclosure.org&lt;/a&gt;; LeBel et al., 2013). A similar logic may also have contributed to researchers’ deciding not to fully report independent variables that failed to yield statistically significant effects and not fully reporting the exclusion of outlying participants due to fear that this information may raise doubts among the editor/reviewers and hurt their chance of getting their foot in the door (i.e., at least getting a revise-and-resubmit).&lt;/p&gt;
&lt;p&gt;An alternative explanation is that new tools and technology have given us the ability to measure a greater number of DVs, which makes it more difficult to report on all them. For example, neuroscience (e.g., EEG, fMRI) and eye-tracking methods yield multitudes of analyzable data that were not previously available. Though this is undeniably true, the internet and online article supplements gives us the ability to fully report our methods and results and use the article to draw attention to the most interesting data.&lt;/p&gt;
&lt;p&gt;Considering the possibility that psychologists’ reporting practices have changed in the past 50 years has implications for how to construe recent calls for the need to raise reporting standards in psychology (LeBel et al., 2013; Simmons, Nelson, &amp;amp; Simonsohn, 2011; Simmons, Nelson, &amp;amp; Simonsohn, 2012). Rather than seeing these calls as rigid new requirements that might interfere with exploratory research and stifle our science, one could construe such calls as a plea to revert back to the fuller reporting of results that used to be the norm in our science. From this perspective, it should not be viewed as overly onerous or authoritarian to ask researchers to disclose all excluded observations, all tested experimental conditions, all analyzed measures, and their data collection termination rule (what I’m now calling the BASIC 4 methodological categories covered by PsychDisclosure.org and Simmons et al.’s, 2012 21-word solution). It would simply be the way our forefathers used to do it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Festinger, L. &amp;amp; Carlsmith, J. M. (1959). &lt;a href="http://psychclassics.yorku.ca/Festinger/"&gt;Cognitive consequences of forced compliance.&lt;/a&gt; &lt;em&gt;The Journal of Abnormal and Social Psychology&lt;/em&gt;, 58(2), Mar 1959, 203-210. doi: 10.1037/h0041593&lt;/p&gt;
&lt;p&gt;Giner-Sorolla, R. (2012). Science or art? &lt;a href="http://pps.sagepub.com/content/7/6/562.full"&gt;How esthetic standards grease the way through the publication bottleneck but undermine science.&lt;/a&gt; &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, 7(6), 562–571. 10.1177/1745691612457576&lt;/p&gt;
&lt;p&gt;LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K. R., Ratliff, K. A., &amp;amp; Smith, C. T. (2013). &lt;a href="http://pps.sagepub.com/content/8/4/424.full"&gt;PsychDisclosure.org: Grassroots support for reforming reporting standards in psychology.&lt;/a&gt; Perspectives on Psychological Science, 8(4), 424-432. doi: 10.1177/1745691613491437&lt;/p&gt;
&lt;p&gt;Nosek, B. A., Spies, J. R., &amp;amp; Motyl, M. (2012).  &lt;a href="http://pps.sagepub.com/content/7/6/615.full"&gt;Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability.&lt;/a&gt; Perspectives on Psychological Science, 7, 615-631. doi: 10.1177/1745691612459058&lt;/p&gt;
&lt;p&gt;Simmons J., Nelson L. &amp;amp; Simonsohn U. (2011) &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704"&gt;False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allow Presenting Anything as Significant.&lt;/a&gt; &lt;em&gt;Psychological Science&lt;/em&gt;, 22(11), 1359-1366.&lt;/p&gt;
&lt;p&gt;Simmons J., Nelson L. &amp;amp; Simonsohn U. (2012) &lt;a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588"&gt;A 21 Word Solution&lt;/a&gt; &lt;em&gt;Dialogue: The Official Newsletter of the Society for Personality and Social Psychology&lt;/em&gt;, 26(2), 4-7.&lt;/p&gt;
&lt;p&gt;Word, C. O., Zanna, M. P., &amp;amp; Cooper, J. (1974). &lt;a href="https://catalyst.uw.edu/workspace/file/download/72b19d8df8de321d0fed3803109a14aaf7c7b6f3800f40f477d902ab0a5e173b"&gt;The nonverbal mediation of self-fulfilling prophecies in interracial interaction.&lt;/a&gt; &lt;em&gt;Journal of Experimental Social Psychology&lt;/em&gt;, 10(2), 109–120. doi: 10.1016/0022-1031(74)90059-6&lt;/p&gt;</summary></entry><entry><title>Increasing statistical power in psychological research without increasing sample size</title><link href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/" rel="alternate"></link><updated>2013-11-03T00:00:00-04:00</updated><author><name>Sean Mackinnon</name></author><id>tag:osc.centerforopenscience.org,2013-11-03:2013/11/03/Increasing-statistical-power/</id><summary type="html">&lt;p&gt;&lt;strong&gt;What is statistical power and precision?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This post is going to give you some practical tips to increase statistical power in your research. Before going there though, let’s make sure everyone is on the same page by starting with some definitions. &lt;/p&gt;
&lt;p&gt;Statistical power is the probability that the test will reject the null hypothesis when the null hypothesis
is false. Many authors suggest a statistical power rate of at least .80, which corresponds to an 80% probability of &lt;em&gt;not&lt;/em&gt; committing a&lt;a href="http://www.investopedia.com/terms/t/type-ii-error.asp"&gt; Type II error&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Precision refers to the width of the&lt;a href="http://www.psychologicalscience.org/index.php/publications/observer/2010/april-10/understanding-confidence-intervals-cis-and-effect-size-estimation.html"&gt; confidence interval&lt;/a&gt; for an &lt;a href="http://www.leeds.ac.uk/educol/documents/00002182.htm"&gt;effect size&lt;/a&gt;. The smaller this width, the more precise your results are. For 80% power, the confidence interval width will be roughly plus or minus 70% of the population effect size (&lt;a href="http://www.ncbi.nlm.nih.gov/pubmed/8017747"&gt;Goodman &amp;amp; Berlin, 1994&lt;/a&gt;). Studies that have low precision have a greater probability of both&lt;a href="http://www.investopedia.com/terms/t/type_1_error.asp"&gt; Type I&lt;/a&gt; and&lt;a href="http://www.investopedia.com/terms/t/type-ii-error.asp"&gt; Type II&lt;/a&gt; errors (&lt;a href="http://dx.doi.org/10.1038/nrn3475"&gt;Button et al., 2013&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;To get an idea of how this works, here are a few examples of the sample size required to achieve .80 power for small, medium, and large (&lt;a href="http://www2.psych.ubc.ca/~schaller/349and449Readings/Cohen1992.pdf"&gt;Cohen, 1992&lt;/a&gt;) correlations as well as the expected confidence intervals&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Population Effect Size&lt;/td&gt;
    &lt;td&gt;Sample Size for 80% Power&lt;/td&gt;
    &lt;td&gt;Estimated Precision&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;r = .10&lt;/td&gt;
    &lt;td&gt;782&lt;/td&gt;
    &lt;td&gt;95% CI [.03, .17]&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;r = .30&lt;/td&gt;
    &lt;td&gt;84&lt;/td&gt;
    &lt;td&gt;95% CI [.09, .51]&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;r = .50&lt;/td&gt;
    &lt;td&gt;29&lt;/td&gt;
    &lt;td&gt;95% CI [.15, .85]&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Studies in psychology are grossly underpowered&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Okay, so now you know what power is. But why should you care? Fifty years ago,&lt;a href="http://dx.doi.org/10.1037/h0045186"&gt; Cohen (1962)&lt;/a&gt; estimated the statistical power to detect a medium effect size in abnormal psychology was about .48. That’s a false negative rate of 52%, which is no better than a coin-flip! The situation has improved slightly, but it’s still a serious problem today. For instance, one review suggested only 52% of articles in the applied psychology literature achieved .80 power for a medium effect size (&lt;a href="http://dx.doi.org/10.1111/j.1744-6570.1996.tb01793.x"&gt;Mone et al., 1996&lt;/a&gt;). This is in part because psychologists are studying small effects. One massive review of 322 meta-analyses including 8 million participants (&lt;a href="http://dx.doi.org/10.1037/1089-2680.7.4.331"&gt;Richard et al., 2003&lt;/a&gt;) suggested that the average effect size in social psychology is relatively small (&lt;em&gt;r&lt;/em&gt; = .21). To put this into perspective, you’d need 175 participants to have .80 power for a simple correlation between two variables at this effect size. This gets even worse when we’re studying interaction effects. One review suggests that the average effect size for interaction effects is even smaller (f2 = .009), which means that sample sizes of around 875 people would be needed to achieve .80 power (&lt;a href="http://dx.doi.org/10.1037/0021-9010.90.1.94"&gt;Aguinis et al., 2005&lt;/a&gt;). Odds are, if you took the time to design a research study and collect data, you want to find a relationship if one really exists. You don’t want to "miss" something that is really there. More than this, you probably want to have a reasonably precise estimate of the effect size (it’s not that impressive to just say a relationship is positive and probably non-zero). Below, I discuss concrete strategies for improving power and precision.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What can we do to increase power?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is well-known that increasing sample size increases statistical power and precision. Increasing the population effect size increases statistical power, but has no effect on precision (&lt;a href="http://dx.doi.org/10.1146/annurev.psych.59.103006.093735"&gt;Maxwell et al., 2008&lt;/a&gt;). Increasing sample size improves power and precision by reducing &lt;a href="http://www.investopedia.com/terms/s/standard-error.asp"&gt;standard error&lt;/a&gt; of the effect size. Take a look at this formula for the confidence interval of a linear regression coefficient (&lt;a href="http://dx.doi.org/10.1037/1082-989X.2.1.3"&gt;McClelland, 2000&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src="images/power-equation.png" alt="Power Equation"&gt; &lt;/p&gt;
&lt;p&gt;MSE is the mean square error, n is the sample size, Vx is the variance of X, and (1-&lt;em&gt;R&lt;/em&gt;2) is the proportion of the variance in X not shared by any other variables in the model. Okay, hopefully you didn’t nod off there. There’s a reason I’m showing you this formula. In this formula, decreasing any value in the numerator (MSE) or increasing anything in the denominator (n, Vx, 1-&lt;em&gt;R&lt;/em&gt;2) will decrease the standard error of the effect size, and will thus increase power and precision. This formula demonstrates that there are at least three other ways to increase statistical power aside from sample size: (a) Decreasing the mean square error; (b) increasing the variance of x; and (c) increasing the proportion of the variance in X not shared by any other predictors in the model. Below, I’ll give you a few ways to do just that. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation 1: Decrease the mean square error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Referring to the formula above, you can see that decreasing the mean square error will have about the same impact as increasing sample size. Okay. You’ve probably heard the term "&lt;a href="http://stats.oecd.org/glossary/detail.asp?ID=3716"&gt;mean square error&lt;/a&gt;" before, but the definition might be kind of fuzzy. Basically, your model makes a prediction for what the outcome variable (Y) should be, given certain values of the predictor (X). Naturally, it’s not a perfect prediction because you have measurement error, and because there are other important variables you probably didn’t measure. The mean square error is the difference between what your model predicts, and what the true values of the data actually are.  So, anything that improves the quality of your measurement or accounts for potential confounding variables will reduce the mean square error, and thus improve statistical power. Let’s make this concrete. Here are three specific techniques you can use:&lt;/p&gt;
&lt;p&gt;a)      &lt;em&gt;Reduce measurement error by using more reliable measures&lt;/em&gt;(i.e., better internal consistency, test-retest reliability, inter-rater reliability, etc.). You’ve probably read that .70 is the "rule-of-thumb" for acceptable reliability. Okay, sure. That’s publishable. But consider this: Let’s say you want to test a correlation coefficient. Assuming both measures have a reliability of .70, your observed correlation will be about 1.43 times &lt;em&gt;smaller&lt;/em&gt; than the true population parameter (I got this using&lt;a href="http://jeromyanglim.blogspot.ca/2009/09/adjusting-correlations-for-reliability.html"&gt; Spearman’s correlation attenuation formula&lt;/a&gt;).  Because you have a smaller observed effect size, you end up with less statistical power. Why do this to yourself? Reduce measurement error. If you’re an experimentalist, make sure you execute your experimental manipulations exactly the same way each time, preferably by automating them. Slight variations in the manipulation (e.g., different locations, slight variations in timing) might reduce the reliability of the manipulation, and thus reduce power. &lt;/p&gt;
&lt;p&gt;b)      &lt;em&gt;Control for confounding variables.&lt;/em&gt; With correlational research, this means including control variables that predict the outcome variable, but are relatively uncorrelated with other predictor variables. In experimental designs, this means taking great care to control for as many possible confounds as possible. In both cases, this reduces the mean square error and improves the overall predictive power of the model – and thus, improves statistical power. Be careful when adding control variables into your models though: There are diminishing returns for adding covariates. Adding a couple of good covariates is bound to improve your model, but you always have to balance predictive power against model complexity. Adding a large number of predictors can sometimes lead to overfitting (i.e., the model is just describing noise or random error) when there are too many predictors in the model relative to the sample size. So, controlling for a couple of good covariates is generally a good idea, but too many covariates will probably make your model worse, not better, especially if the sample is small. &lt;/p&gt;
&lt;p&gt;c)      &lt;em&gt;Use repeated-measures designs.&lt;/em&gt; Repeated measures designs are where participants are measured multiple times  (e.g., once a day surveys, multiple trials in an experiment, etc.). Repeated measures designs reduce the mean square error by partitioning out the variance due to individual participants. Depending on the kind of analysis you do, it can also increase the degrees of freedom for the analysis substantially. For example, you might only have 100 participants, but if you measured them once a day for 21 days, you’ll actually have 2100 data points to analyze. The data analysis can get tricky and the interpretation of the data may change, but many multilevel and structural equation models can take advantage of these designs by examining each &lt;em&gt;measurement occasion &lt;/em&gt;(i.e., each day, each trial, etc.) as the unit of interest, instead of each individual participant. Increasing the degrees of freedom is much like increasing the sample size in terms of increasing statistical power.  I’m a big fan of repeated measures designs, because they allow researchers to collect a lot of data from fewer participants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation 2: Increase the variance of your predictor variable&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another less-known way to increase statistical power and precision is to increase the variance of your predictor variables (X). The formula listed above shows that doubling the variance of X is has the same impact on increasing statistical precision as doubling the sample size does! So it’s worth figuring this out. &lt;/p&gt;
&lt;p&gt;a)      &lt;em&gt;In correlational research, use more comprehensive continuous measures. &lt;/em&gt;That is, there should be a large possible range of values endorsed by participants. However, the measure should also capture many different aspects of the construct of interest; artificially increasing the range of X by adding redundant items (i.e., simply re-phrasing existing items to ask the same question) will actually hurt the validity of the analysis. Also, avoid dichotomizing your measures (e.g., median splits), because this reduces the variance and typically reduces power (&lt;a href="http://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf"&gt;MacCallum et al., 2002&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;b)      &lt;em&gt;In experimental research, unequally allocating participants to each condition can improve statistical power&lt;/em&gt;. For example, if you were designing an experiment with 3 conditions (let’s say low, medium, or high self-esteem). Most of us would equally assign participants to all three groups, right? Well, as it turns out, assigning participants equally across groups usually reduces statistical power. The idea behind assigning participants unequally to conditions is to maximize the variance of X for the particular kind of relationship under study -- which, according the formula I gave earlier, will increase power and precision. For example, the optimal design for a linear relationship would be 50% low, 50% high, and omit the medium condition. The optimal design for a quadratic relationship would be 25% low, 50% medium, and 25% high. The proportions vary widely depending on the design and the kind of relationship you expect, but I recommend you check out&lt;a href="http://dx.doi.org/10.1037/1082-989X.2.1.3"&gt; McClelland (1997)&lt;/a&gt; to get more information on efficient experimental designs. You might be surprised.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation 3: Make sure predictor variables are uncorrelated with each other&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A final way to increase statistical power is to &lt;em&gt;increase the proportion of the variance in X not shared with other variables in the model&lt;/em&gt;. When predictor variables are correlated with each other, this is known as colinearity. For example, depression and anxiety are positively correlated with each other; including both as simultaneous predictors (say, in multiple regression) means that statistical power will be reduced, especially if one of the two variables actually doesn’t predict the outcome variable. Lots of textbooks suggest that we should only be worried about this when colinearity is extremely high (e.g., correlations around &amp;gt; .70). However, studies have shown that even modest intercorrlations among predictor variables will reduce statistical power (&lt;a href="http://dx.doi.org/10.2307/3172863"&gt;Mason et al., 1991&lt;/a&gt;). Bottom line: If you can design a model where your predictor variables are relatively uncorrelated with each other, you can improve statistical power.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Increasing statistical power is one of the rare times where what is good for science, and what is good for your career actually coincides. It increases the accuracy and replicability of results, so it’s good for science. It also increases your likelihood of finding a statistically significant result (assuming the effect actually exists), making it more likely to get something published. You don’t need to torture your data with obsessive re-analysis until you get &lt;em&gt;p&lt;/em&gt; &amp;lt; .05.  Instead, put more thought into research design in order to maximize statistical power. Everyone wins, and you can use that time you used to spend sweating over p-values to do something more productive. Like volunteering with the&lt;a href="http://openscienceframework.org/"&gt; Open Science Collaboration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aguinis, H., Beaty, J. C., Boik, R. J., &amp;amp; Pierce, C. A. (2005). Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review. &lt;em&gt;Journal of Applied Psychology, 90,&lt;/em&gt; 94-107. doi:10.1037/0021-9010.90.1.94&lt;/p&gt;
&lt;p&gt;Button, K. S., Ioannidis, J. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. J., &amp;amp; Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365-376. doi: 10.1038/nrn3475&lt;/p&gt;
&lt;p&gt;Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. &lt;em&gt;The Journal of Abnormal and Social Psychology, 65,&lt;/em&gt; 145-153. doi:10.1037/h0045186&lt;/p&gt;
&lt;p&gt;Cohen, J. (1992). A power primer. &lt;em&gt;Psychological Bulletin, 112,&lt;/em&gt; 155-159. doi:10.1037/0033-2909.112.1.155&lt;/p&gt;
&lt;p&gt;Goodman, S. N., &amp;amp; Berlin, J. A. (1994). The use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. &lt;em&gt;Annals of Internal Medicine, 121, &lt;/em&gt;200-206. &lt;/p&gt;
&lt;p&gt;Hansen, W. B., &amp;amp; Collins, L. M. (1994). Seven ways to increase power without increasing N. In L. M. Collins &amp;amp; L. A. Seitz (Eds.), &lt;em&gt;Advances in data analysis for prevention intervention research&lt;/em&gt; (NIDA Research Monograph 142, NIH Publication No. 94-3599, pp. 184–195). Rockville, MD: National Institutes of Health.&lt;/p&gt;
&lt;p&gt;MacCallum, R. C., Zhang, S., Preacher, K. J., &amp;amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. &lt;em&gt;Psychological Methods, 7,&lt;/em&gt; 19-40. doi:10.1037/1082-989X.7.1.19&lt;/p&gt;
&lt;p&gt;Mason, C. H., &amp;amp; Perreault, W. D. (1991). Collinearity, power, and interpretation of multiple regression analysis. &lt;em&gt;Journal of Marketing Research, 28,&lt;/em&gt; 268-280. doi:10.2307/3172863&lt;/p&gt;
&lt;p&gt;Maxwell, S. E., Kelley, K., &amp;amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. &lt;em&gt;Annual Review of Psychology, 59,&lt;/em&gt; 537-563. doi:10.1146/annurev.psych.59.103006.093735&lt;/p&gt;
&lt;p&gt;McClelland, G. H. (1997). Optimal design in psychological research. &lt;em&gt;Psychological Methods, 2,&lt;/em&gt; 3-19. doi:10.1037/1082-989X.2.1.3&lt;/p&gt;
&lt;p&gt;McClelland, G. H. (2000). Increasing statistical power without increasing sample size. &lt;em&gt;American Psychologist, 55, &lt;/em&gt;963-964. doi:10.1037/0003-066X.55.8.963&lt;/p&gt;
&lt;p&gt;Mone, M. A., Mueller, G. C., &amp;amp; Mauland, W. (1996). The perceptions and usage of statistical power in applied psychology and management research. &lt;em&gt;Personnel Psychology, 49,&lt;/em&gt; 103-120. doi:10.1111/j.1744-6570.1996.tb01793.x&lt;/p&gt;
&lt;p&gt;Open Science Collaboration. (in press). The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility. In V. Stodden, F. Leisch, &amp;amp; R. Peng (Eds.), &lt;em&gt;Implementing Reproducible Computational Research (A Volume in The R Series)&lt;/em&gt;.  New York, NY: Taylor &amp;amp; Francis. doi:10.2139/ssrn.2195999&lt;/p&gt;
&lt;p&gt;Richard, F. D., Bond, C. r., &amp;amp; Stokes-Zoota, J. J. (2003). One Hundred Years of Social Psychology Quantitatively Described. &lt;em&gt;Review of General Psychology, 7,&lt;/em&gt; 331-363. doi:10.1037/1089-2680.7.4.331&lt;/p&gt;</summary></entry><entry><title>It’s Easy Being Green (Open Access)</title><link href="http://osc.centerforopenscience.org/2013/10/25/its-easy-being-green-open-access/" rel="alternate"></link><updated>2013-10-25T12:00:00-04:00</updated><author><name>Frank Farach</name></author><id>tag:osc.centerforopenscience.org,2013-10-25:2013/10/25/its-easy-being-green-open-access/</id><summary type="html">&lt;p&gt;&lt;em&gt;This is the first installment of the Open Science Toolkit, a recurring
feature that outlines practical steps individuals and organizations can
take to make science more open and reproducible.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/FrankFarachHeadshotBWSmall.jpg" alt="Photo of Frank
Farach" align="left" style="padding-right: 20px;" width="100px"&gt;&lt;/p&gt;
&lt;p&gt;Congratulations! Your manuscript has been peer reviewed and accepted for
publication in a journal. The journal is owned by a major publisher who
wants you to know that, for $3,000, you can make your article open
access (OA) forever. Anyone in the world with access to the Internet
will have access to your article, which may be cited more often because
of its OA status. Otherwise, the journal would be happy to make your
paper available to subscribers and others willing to pay a fee.&lt;/p&gt;
&lt;p&gt;Does this sound familiar? It sure does to me. For many years, when I
heard about Open Access (OA) to scientific research, it was always about
making an article freely available in a peer-reviewed journal -- the
so-called “gold” OA option -- often at considerable expense. I liked the
idea of making my work available to the widest possible audience, but
the costs were too prohibitive.&lt;/p&gt;
&lt;p&gt;As it turns out, however, the “&lt;a href="http://legacy.earlham.edu/~peters/fos/overview.htm"&gt;best-kept
secret&lt;/a&gt;” of OA is
that you can make your work OA for free by self-archiving it in an OA
repository, even if it has already been published in a non-OA journal.
Such “green” OA is possible because &lt;a href="http://www.sherpa.ac.uk/romeo/statistics.php?la=en&amp;amp;fIDnum=%7C&amp;amp;mode=simple"&gt;many
journals&lt;/a&gt;
have already provided blanket permission for authors to deposit their
peer-reviewed manuscript in an OA repository.&lt;/p&gt;
&lt;p&gt;The flowchart below shows how easy it is to make your prior work OA. The
key is to make sure you follow the self-archiving policy of the journal
your work was published in, and to deposit the work in a suitable
repository.  &lt;/p&gt;
&lt;p&gt;&lt;a href=images/GreenOALarge.png"&gt;Click to enlarge.&lt;img src="images/GreenOASmall.png" alt="Flowchart showing how to
archive your research" align="left" width="600px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Journals typically display their self-archiving and copyright policies
on their websites, but you can also search for them in the
&lt;a href="http://www.sherpa.ac.uk/romeo/"&gt;SHERPA/RoMEO&lt;/a&gt; database, which has a
nicely curated collection of policies from 1,333 publishers.  The
database assigns a code to journals based on how far in the publication
process their permissions extend.  It distinguishes between a pre-print,
which is the version of the manuscript before it underwent peer review,
and a post-print, the peer-reviewed version before the journal
copyedited and typeset it. Few journals allow you to self-archive their
copyedited PDF version of your article, but many let you do so for the
pre-print or post-print version. Unfortunately, some journals don’t
provide blanket permission for self-archiving, or require you to wait
for an embargo period to end before doing so. If you run into this
problem, you should contact the journal and ask for permission to
deposit the non-copyedited version of your article in an OA repository.&lt;/p&gt;
&lt;p&gt;It has also become easy to find a suitable OA repository in which to
deposit your work. Your first stop should be the &lt;a href="http://www.opendoar.org"&gt;Directory of Open
Access Repositories&lt;/a&gt; (OpenDOAR), which
currently lists over 2,200 institutional, disciplinary, and universal
repositories. Although an article deposited in an OA repository will be
available to anyone with Internet access, repositories differ in feature
sets and policies. For example, some repositories, like
&lt;a href="http://figshare.com/cc_license"&gt;fig&lt;strong&gt;share&lt;/strong&gt;&lt;/a&gt;, automatically assign a
&lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;CC BY&lt;/a&gt; license to all
publicly shared papers; others, like &lt;a href="http://opendepot.org/"&gt;Open
Depot&lt;/a&gt;, allow you to &lt;a href="http://creativecommons.org/choose/"&gt;choose a
license&lt;/a&gt; before making the article
public. A good OA repository will tell you how it ensures the long-term
digital preservation of your content as well as what metadata it exposes
to search engines and web services.&lt;/p&gt;
&lt;p&gt;Once you’ve deposited your article in an OA repository, consider making
others aware of its existence. Link to it on your website, mention it on
social media, or add it to your CV.&lt;/p&gt;
&lt;p&gt;In honor of Open Access Week, I am issuing a “Green OA Challenge” to
readers of this blog who have published at least one peer-reviewed
article. The challenge is to self-archive one of your articles in an OA
repository and link to it in the comments below. Please also feel free
to share any comments you have about the self-archiving process or about
green OA. Happy archiving!&lt;/p&gt;</summary></entry><entry><title>Thriving au naturel amid science on steroids</title><link href="http://osc.centerforopenscience.org/2013/10/16/thriving-au-naturel-amid-science-on-steroids/" rel="alternate"></link><updated>2013-10-16T09:54:00-04:00</updated><author><name>None</name></author><id>tag:osc.centerforopenscience.org,2013-10-16:2013/10/16/thriving-au-naturel-amid-science-on-steroids/</id><summary type="html">&lt;p&gt;&lt;a href="http://briannosek.com"&gt;Brian A. Nosek&lt;/a&gt;&lt;br /&gt;
University of Virginia, Center for Open Science&lt;/p&gt;
&lt;p&gt;&lt;a href="http://jeffspies.com"&gt;Jeffrey R. Spies&lt;/a&gt;&lt;br /&gt;
Center for Open Science&lt;/p&gt;
&lt;p&gt;&lt;img src="images/bnosek.jpg" alt="Photo of Brian Nosek" align="left" style="padding-right: 20px;" width="100px"&gt; &lt;img src="images/jspies.jpg" alt="Photo of Jeffrey Spies" align="left" style="padding-right: 20px;" width="100px"&gt; Last fall, the present first author taught a graduate class called
“Improving (Our) Science” at the University of Virginia. The class
reviewed evidence suggesting that scientific practices are not operating
ideally and are damaging the reproducibility of publishing findings. For
example, the power of an experimental design in null hypothesis
significance testing is a function of the effect size being investigated
and the size the sample to test it—power is greater when effects are
larger and samples are bigger. In the authors’ field of psychology, for
example, estimates suggest that the power of published studies to detect
an average effect size is .50 or less (Cohen, 1962; Gigerenzer &amp;amp;
Sedlmeier, 1989). Assuming that all of the published effects are true,
approximately 50% of published studies would reveal positive results
(i.e., &lt;em&gt;p&lt;/em&gt; &amp;lt; .05 supporting the hypothesis). In reality, more than 90%
of published results are positive (Sterling, 1959; &lt;a href="http://dx.doi.org/10.1371/journal.pone.0010068"&gt;Fanelli,
2010&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;How is it possible that the average study has power to detect the
average true effect 50% of the time or less and yet does so about 90% of
the time? It isn’t. Then how does this occur? One obvious contributor is
selective reporting. Positive effects are more likely than negative
effects to be submitted and accepted for publication (Greenwald, 1975).
The consequences include [1] the published literature is more likely to
exaggerate the size of true effects because with low powered designs
researchers must still leverage chance to obtain a large enough effect
size to produce a positive result; and [2] the proportion of false
positives – there isn’t actually an effect to detect – will be inflated
beyond the nominal alpha level of 5% (Ioannidis, 2005).&lt;/p&gt;
&lt;p&gt;The class discussed this and other scientific practices that may
interfere with knowledge accumulation. Some of the relatively common
ones are described in Table 1 along with some solutions that we, and
others, identified. Problem. Solution. Easy. The class just fixed
science. Now, class members can adopt the solutions as best available
practices. Our scientific outputs will be more accurate, and significant
effects will be more reproducible. Our science will be better.&lt;/p&gt;
&lt;p&gt;Alex Schiller, a class member and graduate student, demurred. He agreed
that the new practices would make science better, but disagreed that we
should do them all. A better solution, he argued, is to take small
steps: adopt one solution, wait for that to become standard scientific
practice, and then adopt another solution.&lt;/p&gt;
&lt;p&gt;We know that some of our practices are deficient, we know how to improve
them, but Alex is arguing that we shouldn’t implement all the solutions?
Alex’s lapse of judgment can be forgiven—he’s just a graduate student.
However, his point isn’t a lapse. Faced with the reality of succeeding
as a scientist, Alex is right.&lt;/p&gt;
&lt;h2&gt;Table 1&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Scientific practices that increase irreproducibility of
published findings, possible solutions, and barriers that prevent
adoption of those solutions&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Practice&lt;/th&gt;
&lt;th&gt;Problem&lt;/th&gt;
&lt;th&gt;Possible Solution&lt;/th&gt;
&lt;th&gt;Barrier to Solution&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Run many low-powered studies rather than few high-powered studies&lt;/td&gt;
&lt;td&gt;Inflates false positive and false negative rates&lt;/td&gt;
&lt;td&gt;Run high-powered studies&lt;/td&gt;
&lt;td&gt;Non-significant effects are a threat to publishability; Risky to devote extra resources to high-powered tests that might not produce significant effects&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Report significant effects and dismiss non-significant effects as methodologically flawed&lt;/td&gt;
&lt;td&gt;Using outcome to evaluate method is a logical error and can inflate false positive rate&lt;/td&gt;
&lt;td&gt;Report all effects with rationale for why some should be ignored; let reader decide&lt;/td&gt;
&lt;td&gt;Non-significant and mixed effects are a threat to publishability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Analyze during data collection, stop when significant result is obtained or continue until significant result is obtained&lt;/td&gt;
&lt;td&gt;Inflates false positive rate&lt;/td&gt;
&lt;td&gt;Define data stopping rule in advance&lt;/td&gt;
&lt;td&gt;Non-significant effects are a threat to publishability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Include multiple conditions or outcome variables, report the subset that showed significant effects&lt;/td&gt;
&lt;td&gt;Inflates false positive rate&lt;/td&gt;
&lt;td&gt;Report all conditions and outcome variables&lt;/td&gt;
&lt;td&gt;Non-significant and mixed effects are a threat to publishability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Try multiple analysis strategies, data exclusions, data transformations, report cleanest subset&lt;/td&gt;
&lt;td&gt;Inflates false positive rate&lt;/td&gt;
&lt;td&gt;Prespecify data analysis plan, or report all analysis strategies&lt;/td&gt;
&lt;td&gt;Non-significant and mixed effects are a threat to publishability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Report discoveries as if they had resulted from confirmatory tests&lt;/td&gt;
&lt;td&gt;Inflates false positive rate&lt;/td&gt;
&lt;td&gt;Pre-specify hypotheses; Report exploratory and confirmatory analyses separately&lt;/td&gt;
&lt;td&gt;Many findings are discoveries, but stories are nicer and scientists seem smarter if they had thought it in advance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never do a direct replication&lt;/td&gt;
&lt;td&gt;Inflates false positive rate&lt;/td&gt;
&lt;td&gt;Conduct direct replications of important effects&lt;/td&gt;
&lt;td&gt;Incentives are focused on innovation, replications are boring; Original authors might feel embarrassed if their original finding is irreproducible&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For reviews of these practices and their effects see Ioannidis,
2005; Giner-Sorolla, 2012; Greenwald, 1975; &lt;a href="http://dx.doi.org/10.1177/0956797611430953"&gt;John et al.,
2012&lt;/a&gt;; &lt;a href="http://dx.doi.org/10.1177/1745691612459058"&gt;Nosek et
al., 2012&lt;/a&gt;; &lt;a href="http://dx.doi.org/10.1037/0033-2909.86.3.638"&gt;Rosenthal,
1979&lt;/a&gt;; Simmons et al., 2011; Young et al., 2008      &lt;/p&gt;
&lt;h2&gt;The Context&lt;/h2&gt;
&lt;p&gt;In an ideal world, scientists use the best available practices to
produce accurate, reproducible science. But, scientists don’t live in an
ideal world. Alex is creating a career for himself. To succeed, he must
publish. Papers are academic currency. They are Alex’s ticket to job
security, fame, and fortune. Well, okay, maybe just job security. But,
not everything is published, and some publications are valued more than
others. Alex can maximize his publishing success by producing particular
kinds of results. Positive effects, not negative effects (&lt;a href="http://dx.doi.org/10.1371/journal.pone.0010068"&gt;Fanelli,
2010&lt;/a&gt;;
Sterling, 1959). Novel effects, not verifications of prior effects (&lt;a href="http://dx.doi.org/10.1177/1745691612462588"&gt;Open
Science Collaboration, 2012&lt;/a&gt;). Aesthetically appealing, clean results,
not results with ambiguities or inconsistencies (Giner-Sorolla, 2012).
Just look in the pages of &lt;em&gt;Nature&lt;/em&gt;, or any other leading journal, they
are filled with articles producing positive, novel, beautiful results.
They are wonderful, exciting, and groundbreaking. Who wouldn’t want
that?&lt;/p&gt;
&lt;p&gt;We do want that, and science advances in leaps with groundbreaking
results. The hard reality is that few results are actually
groundbreaking. And, even for important research, the results are often
far from beautiful. There are confusing contradictions, apparent
exceptions, and things that just don’t make sense. To those in the
laboratory, this is no surprise. Being at the frontiers of knowledge is
hard. We don’t quite know what we are looking at. That’s why we are
studying it. Or, as Einstein said, “If we knew what we were doing, it
wouldn’t be called research”.
 
But, those outside the laboratory get a different impression. When the
research becomes a published article, much of the muck goes away. The
published articles are like the pictures of this commentary’s authors at
the top of this page. Those pictures are about as good as we can look.
You should see the discards. Those with insider access know, for
example, that we each own three shirts with buttons and have highly
variable shaving habits. Published articles present the best-dressed,
clean-shaven versions of the actual work.&lt;/p&gt;
&lt;p&gt;Just as with people, when you replicate effects yourself to see them in
person, they may not be as beautiful as they appeared in print. The
published version often looks much better than reality. The effect is
hard to get, dependent on a multitude of unmentioned limiting
conditions, or entirely irreproducible (Begley &amp;amp; Ellis, 2012; Prinz et
al., 2011).&lt;/p&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;It is not surprising that effects are presented in their best light.
Career advancement depends on publishing success. More beautiful looking
results are easier to publish and more likely to earn rewards
(Giner-Sorolla, 2012). Individual incentives align for maximizing
publishability, even at the expense of accuracy (&lt;a href="http://dx.doi.org/10.1177/1745691612459058"&gt;Nosek et al.,
2012&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Consider three hypothetical papers shown in Table 2. For all three, the
researchers identified an important problem and had an idea for a novel
solution. &lt;em&gt;Paper A&lt;/em&gt; is a natural beauty. Two well planned studies showed
effects supporting the idea. &lt;em&gt;Paper B&lt;/em&gt; and &lt;em&gt;Paper C&lt;/em&gt; were conducted with
identical study designs. &lt;em&gt;Paper B&lt;/em&gt; is natural, but not beautiful; &lt;em&gt;Paper
C&lt;/em&gt; is a manufactured beauty. Both &lt;em&gt;Paper B&lt;/em&gt; and &lt;em&gt;Paper C&lt;/em&gt; were based on 3
studies. One study for each showed clear support for the idea. A second
study was a mixed success for &lt;em&gt;Paper B&lt;/em&gt;, but “worked” for &lt;em&gt;Paper C&lt;/em&gt; after
increasing the sample size a bit and analyzing the data a few different
ways. A third study did not work for either. &lt;em&gt;Paper B&lt;/em&gt; reported the
failure with an explanation for why the methodology might be to blame,
rather than the idea being incorrect. The authors of &lt;em&gt;Paper C&lt;/em&gt; generated
the same methodological explanation, categorized the study as a pilot,
and did not report it at all. Also, &lt;em&gt;Paper C&lt;/em&gt; described the final sample
sizes and analysis strategies, but did not mention that extra data was
collected after initial analysis, or that alternative analysis
strategies had been tried and dismissed.&lt;/p&gt;
&lt;h2&gt;Table 2&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Summary of research practices for three hypothetical papers&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Step&lt;/th&gt;
&lt;th&gt;Paper&lt;/th&gt;
&lt;th&gt;Paper B&lt;/th&gt;
&lt;th&gt;Paper C&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data collection&lt;/td&gt;
&lt;td&gt;Conducted two studies&lt;/td&gt;
&lt;td&gt;Conducted three studies&lt;/td&gt;
&lt;td&gt;Conducted three studies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data analysis&lt;/td&gt;
&lt;td&gt;Analyzed data after completing data collection following a pre-specified analysis plan&lt;/td&gt;
&lt;td&gt;Analyzed data after completing data collection following a pre-specified analysis plan&lt;/td&gt;
&lt;td&gt;Analyzed during data collection and collected more data to get to significance in one case.  Selected from multiple analysis strategies for all studies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Result reporting&lt;/td&gt;
&lt;td&gt;Reported the results of the planned analyses for both studies&lt;/td&gt;
&lt;td&gt;Reported the results of the planned analyses for all studies&lt;/td&gt;
&lt;td&gt;Reported results of final analyses only.  Did not report one study that did not reach significance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Final paper&lt;/td&gt;
&lt;td&gt;Two studies demonstrating clear support for idea&lt;/td&gt;
&lt;td&gt;One study demonstrating clear support for idea, one mixed, one not at all&lt;/td&gt;
&lt;td&gt;Two studies demonstrating clear support for idea&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Paper A&lt;/em&gt; is clearly better than &lt;em&gt;Paper B&lt;/em&gt;. &lt;em&gt;Paper A&lt;/em&gt; should be
published in a more prestigious outlet and generate more attention and
accolade. &lt;em&gt;Paper C&lt;/em&gt; &lt;strong&gt;looks&lt;/strong&gt; like &lt;em&gt;Paper A&lt;/em&gt;, but in reality it &lt;strong&gt;is&lt;/strong&gt;
like &lt;em&gt;Paper B&lt;/em&gt;. The actual evidence is more circumspect than the
apparent evidence. Based on the report, however, no one can tell the
difference between &lt;em&gt;Paper A&lt;/em&gt; and &lt;em&gt;Paper C&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Two possibilities would minimize the negative impact of publishing
manufactured beauties like &lt;em&gt;Paper C&lt;/em&gt;. First, if replication were
standard practice, then manufactured effects would be identified
rapidly. However, direct replication is very uncommon (&lt;a href="http://dx.doi.org/10.1177/1745691612462588"&gt;Open Science
Collaboration, 2012&lt;/a&gt;). Once an effect is the literature, there is little
systematic ethic to self-correct. Rather than be weeded out, false
effects persist or just slowly fade away. Second, scientists could just
avoid doing the practices that lead to &lt;em&gt;Paper C&lt;/em&gt; making this
illustration an irrelevant hypothetical. Unfortunately, a growing body
of evidence suggests that these practices occur, and some are even
common (e.g., &lt;a href="http://dx.doi.org/10.1177/0956797611430953"&gt;John et al., 2012&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;To avoid the practices that produce &lt;em&gt;Paper C&lt;/em&gt;, the scientist must be
aware of and confront a conflict-of-interest—what is best for science
versus what is best for me. Scientists have inordinate opportunity to
pursue flexible decision-making in design and analysis, and there is
minimal accountability for those practices. Further, humans’ prodigious
motivated reasoning capacities provide a way to decide that the outcomes
that look best for us also has the most compelling rationale (&lt;a href="http://dx.doi.org/10.1037/0033-2909.108.3.480"&gt;Kunda,
1990&lt;/a&gt;). So, we may convince ourselves that the best course of action for
us was the best course of action period. It is very difficult to stop
doing suspect practices when we have thoroughly convinced ourselves that
we are not doing them.&lt;/p&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;Alex needs to publish to succeed. The practices in Table 1 are to the
scientist, what steroids are to the athlete. They amplify the likelihood
of success in a competitive marketplace. If others are using, and Alex
decides to rely on his natural performance, then he will disadvantage
his career prospects. Alex wants to do the best science he can &lt;em&gt;and&lt;/em&gt; be
successful for doing it. In short, he is the same as every other
scientist we know, ourselves included. Alex shouldn’t have to make a
choice between doing the best science and being successful—these should
be the same thing.&lt;/p&gt;
&lt;p&gt;Is Alex stuck? Must he wait for institutional regulation, audits, and
the science police to fix the system? In a regulatory world, practices
are enforced and he need not worry that he’s committing career suicide
by following them. Many scientists are wary of a strong regulatory
environment in science, particularly for the possibility of stifling
innovation. Some of the best ideas start with barely any evidence at
all, and restrictive regulations on confidence in outputs could
discourage taking risks on new ideas. Nonetheless, funders, governments,
and other stakeholders are taking notice of the problematic incentive
structures in science. If we don’t solve the problem ourselves,
regulators may solve them for us.&lt;/p&gt;
&lt;p&gt;Luckily, Alex has an alternative. The practices in Table 1 may be
widespread, but the solutions are also well known and endorsed as good
practice (&lt;a href="http://dx.doi.org/10.1177/1745691612459521"&gt;Fuchs et al., 2012&lt;/a&gt;). That is, scientists easily understand the
differences between Papers A, B, and C – if they have full access to how
the findings were produced. As a consequence, the only way to be
rewarded for natural achievements over manufactured ones is to make the
process of obtaining the results transparent. Using the best available
practices privately will improve science but hurt careers. Using the
best available practices publicly will improve science while
simultaneously improving the reputation of the scientist. With openness,
success can be influenced by the results &lt;em&gt;and&lt;/em&gt; by how they were
obtained.
 &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The present incentives for publishing are focused on the one thing that
we scientists are absolutely, positively not supposed to control - the
results of the investigation. Scientists have complete control over the
design, procedures, and execution of the study. The results are what
they are.&lt;/p&gt;
&lt;p&gt;A better science will emerge when the incentives for achievement align
with the things that scientists can (and should) control with their
wits, effort, and creativity. With results, beauty is contingent on what
is known about their origin. Obfuscation of methodology can make ugly
results appear beautiful. With methodology, if it looks beautiful, it is
beautiful. The beauty of methodology is &lt;em&gt;revealed&lt;/em&gt; by openness.&lt;/p&gt;
&lt;p&gt;Most scientific results have warts. Evidence is halting, uncertain,
incomplete, confusing, and messy. It is that way because scientists are
working on hard problems. Exposing it will accelerate finding solutions
to clean it up. Instead of trying to make results look beautiful when
they are not, the inner beauty of science can be made apparent. Whatever
the results, the inner beauty—strong design, brilliant reasoning,
careful analysis—is what counts. With openness, we won’t stop aiming for
A papers. But, when we get them, it will be clear that we earned them.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Begley, C. G., &amp;amp; Ellis, L. M. (2012). Raise standards for preclinical
cancer research. &lt;em&gt;Nature, 483&lt;/em&gt;, 531-533.&lt;/p&gt;
&lt;p&gt;Cohen , J. (1962). The statistical power of abnormal-social
psychological research: A review. &lt;em&gt;Journal of Abnormal and Social
Psychology, 65&lt;/em&gt;, 145-153.&lt;/p&gt;
&lt;p&gt;Fanelli, D. (2010). "Positive" results increase down the hierarchy of
the sciences. &lt;em&gt;PLoS ONE, 5(4)&lt;/em&gt;, e10068.
&lt;a href="http://dx.doi.org/10.1371/journal.pone.0010068"&gt;doi:10.1371/journal.pone.0010068&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fuchs H., Jenny, M., &amp;amp; Fiedler, S. (2012). Psychologists are open to
change, yet wary of rules. &lt;em&gt;Perspectives on Psychological Science, 7,&lt;/em&gt;
634-637. &lt;a href="http://dx.doi.org/10.1177/1745691612459521"&gt;doi:10.1177/1745691612459521&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sedlmeier, P., &amp;amp; Gigerenzer, G. (1989). Do studies of statistical power
have an effect on the power of studies? &lt;em&gt;Psychological Bulletin, 105&lt;/em&gt;,
309-316.&lt;/p&gt;
&lt;p&gt;Giner-Sorolla, R. (2012). Science or art? How esthetic standards grease
the way through the publication bottleneck but undermine science.
&lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Greenwald, A. G. (1975). Consequences of prejudice against the null
hypothesis. &lt;em&gt;Psychological Bulletin, 82&lt;/em&gt;, 1-20.&lt;/p&gt;
&lt;p&gt;Ioannidis, J. P. A. (2005). Why most published research findings are
false. &lt;em&gt;PLoS Medicine, 2&lt;/em&gt;, e124.&lt;/p&gt;
&lt;p&gt;John, L., Loewenstein, G., &amp;amp; Prelec, D. (2012). Measuring the prevalence
of questionable research practices with incentives for truth-telling&lt;em&gt;.
Psychological Science&lt;/em&gt;, &lt;em&gt;23&lt;/em&gt;, 524-532.
&lt;a href="http://dx.doi.org/10.1177/0956797611430953"&gt;doi:10.1177/0956797611430953&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kunda, Z. (1990). The case for motivated reasoning. &lt;em&gt;Psychological
Bulletin, 108,&lt;/em&gt; 480-498. &lt;a href="http://dx.doi.org/10.1037/0033-2909.108.3.480"&gt;doi:10.1037/0033-2909.108.3.480&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nosek, B. A., Spies, J. R., &amp;amp; Motyl, M. (2012). Scientific utopia: II.
Restructuring incentives and practices to promote truth over
publishability. &lt;em&gt;Perspectives on Psychological Science, 7,&lt;/em&gt;615-631.
&lt;a href="http://dx.doi.org/10.1177/1745691612459058"&gt;doi:10.1177/1745691612459058&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Open Science Collaboration. (2012). An open, large-scale, collaborative
effort to estimate the reproducibility of psychological science.
&lt;em&gt;Perspectives on Psychological Science, 7,&lt;/em&gt; 657-660.
 &lt;a href="http://dx.doi.org/10.1177/1745691612462588"&gt;doi:10.1177/1745691612462588&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Prinz, F., Schlange, T. &amp;amp; Asadullah, K. (2011). Believe it or not: how
much can we rely on published data on potential drug targets? &lt;em&gt;Nature
Reviews Drug Discovery, 10&lt;/em&gt;, 712-713.&lt;/p&gt;
&lt;p&gt;Rosenthal, R. (1979). The file drawer problem and tolerance for null
results. &lt;em&gt;Psychological Bulletin, 86,&lt;/em&gt; 638-641.
&lt;a href="http://dx.doi.org/10.1037/0033-2909.86.3.638"&gt;doi:10.1037/0033-2909.86.3.638&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simmons, J. P., Nelson, L. D., &amp;amp; Simonsohn, U. (2011). False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as significant. &lt;em&gt;Psychological Science, 22&lt;/em&gt;,
1359-1366.&lt;/p&gt;
&lt;p&gt;Sterling, T. D. (1959). Publication decisions and their possible effects
on inferences drawn from tests of significance - or vice versa. &lt;em&gt;Journal
of the American Statistical Association, 54&lt;/em&gt;, 30-34.&lt;/p&gt;
&lt;p&gt;Young, N. S., Ioannidis, J. P. A., &amp;amp; Al-Ubaydli, O. (2008). Why current
publication practices may distort science. &lt;em&gt;PLoS Medicine, 5&lt;/em&gt;,
1418-1422.&lt;/p&gt;</summary></entry><entry><title>Opportunities for Collaborative Research</title><link href="http://osc.centerforopenscience.org/2013/10/10/opportunities-for-collaborative-research/" rel="alternate"></link><updated>2013-10-10T00:00:00-04:00</updated><author><name>Jon Grahe, Pacific Lutheran University</name></author><id>tag:osc.centerforopenscience.org,2013-10-10:2013/10/10/opportunities-for-collaborative-research/</id><summary type="html">&lt;p&gt;&lt;img src="images/jongrahe-headshot.jpg" alt="Photo of Jon Grahe" align="left" style="padding-right: 20px;" width="150px" /&gt;&lt;/p&gt;
&lt;p&gt;As a research methods instructor, I encourage my students to conduct “authentic” research projects. For now, consider authentic undergraduate research experiences as student projects at any level where the findings might result in a conference presentation or a publishable paper. This included getting IRB approval and attempts to present our findings at regional conferences, maybe including a journal submission. Eventually, I participated in opportunities for my students to contribute to big science by joining in two crowd-sourcing projects organized by Alan Reifman. The first was published in Teaching of Psychology (School Spirit Study Group, 2004) as an example for others to follow. The data included samples from 22 research methods classes who measured indices representative of the group name. Classes evaluated the data from their own school and the researchers collapsed the data to look at generalization across institutions. The second collaborative project included surveys collected by students in 10 research methods classes. The topic was primarily focused on emerging adulthood and political attitudes, but included many other psychological constructs. Later, I note the current opportunities for Emerging Adulthood theorists to use these data for a special issue of the journal, Emerging Adulthood. These two projects notwithstanding, there have been few open calls for instructors to participate in big science. Instructors who want to include authentic research experiences do so by completing all their own legwork as characterized by Frank and Saxe (2012) or as displayed by many of the poster presentations that you see at Regional Conferences.&lt;/p&gt;
&lt;p&gt;However, that state of affairs has changed. Though instructors are likely to continue engaging in authentic research in their classrooms, they don’t have to develop the projects on their own anymore. I am very excited about the recent opportunities that allow students to contribute to “big science” by acting as crowd-sourcing experimenters. In full disclosure, I acknowledge my direct involvement in developing three of the following projects. However, I will save you a description of my own pilot test called the Collective Undergraduate Research Project (Grahe, 2010). Instead, I will briefly review recent “open invitation projects”, those where any qualified researcher can contribute. The first to emerge (Psych File Drawer, Reproducibility Project) were focused on PhD level researchers. However, since August 2012 there have been three research projects that specifically invite students to help generate data either as experimenters or as coders. More are likely to emerge soon as theorists grasp the idea that others can help them collect data.&lt;/p&gt;
&lt;p&gt;This is a great time to be a research psychologist. These projects provide real opportunities for students or other researchers at any expertise level to get involved in not only authentic, but transformative research opportunities. The Council of Undergraduate Research recently published an edited volume (Karukstis &amp;amp; Hensel, 2010) dedicated to fostering transformative research for undergraduates. According to Wikipedia, “Transformative research is a term that became increasingly common within the science policy community in the 2000s for research that shifts or breaks existing scientific paradigms.” I consider open invitation projects transformative because they change the way that we view minimally acceptable standards for research. Each one is intended to change the basic premise of collaboration by bridging not only institutions, but also the chasm of acquaintance. Any qualified researcher can participate in data collection and authorship. Now, when I introduce research opportunities to my students, the ideas are grand. Here are research projects with grand ideas that invite contributions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Many Labs Project&lt;/em&gt; - The Many Labs Team’s original invitation to contributors, sent out in &lt;a href="https://groups.google.com/forum/#!topic/openscienceframework/CwwRtaUMl4I"&gt;February 2013&lt;/a&gt; asked contributors to join their “wide-scale replication project {that was} conditionally accepted for publication in the special issue of Social Psychology.” They made a follow-up invitation in July reminding us of their Oct. 1st, 2013 deadline for data collection. This deadline limits future contributors, but it is a great example of a crowd-sourcing project. Their goal is to replicate 12 effects using the &lt;a href="https://implicit.harvard.edu"&gt;Project Implicit&lt;/a&gt; infrastructure. As is typical of these projects, contributors meeting a minimum goal (N &amp;gt; 80 cases in this instance) will be listed as coauthors in future publications. As they stated in their July post, “The project and accepted proposal has been registered on the Open Science Framework and can be found at this link: &lt;a href="http://www.openscienceframework.org/project/WX7Ck/"&gt;http://www.openscienceframework.org/project/WX7Ck/&lt;/a&gt;.” Richard Klein (project coordinator) reported that their project includes 18 different researchers at 15 distinct labs, with a plan for 38 labs contributing data before the deadline. This project is nearing the completion deadline and so new contributions are limited, but the project represents an important exemplar of potential projects.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reproducibility Project&lt;/em&gt; – As stated on their &lt;a href="https://openscienceframework.org/project/EZcUj/wiki/new-contributors:replicate"&gt;invitation to new contributors&lt;/a&gt; on the Open Science Framework page, "Our primary goal is to conduct high quality replications of studies from three 2008 psychology journals and then compare our results to the original studies." As of right now, Johanna Cohoon from the Center for Open Science states, “We have 136 replication authors who come from 59 different institutions, with an additional 19 acknowledged replication researchers (155 replication researchers total). We also have an additional 40 researchers who are not involved in a replication that have earned authorship through coding/vetting 10 or more articles.” In short, this is a large collaboration and is welcoming more committed researchers. Though this project needs advanced researchers, it is possible for faculty to work closely with students who might wish to contribute.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PsychFileDrawer Project&lt;/em&gt; – This is less a collaborative effort between researchers than a compendium of replications, &lt;a href="http://www.psychfiledrawer.org/about.php"&gt;as stated&lt;/a&gt; by the project organizers: “PsychFileDrawer.org is a tool designed to address the File Drawer Problem as it pertains to psychological research: the distortion in the scientific literature that results from the failure to publish non-replications." It is collaborative in the sense that they host a “top 20” list of studies that the viewers want to see replicated. However, any researcher with a replication is invited to contribute the data. Further, although this was not initially targeted toward students, there is a new feature that allows contributors to identify a sample as a class project and &lt;a href="http://www.psychfiledrawer.org/faq.php"&gt;the FAQ page&lt;/a&gt; asks instructors to comment on, “…the level and type of instructor supervision, and instructor’s degree of confidence in the fidelity with which the experimental protocol was implemented.” &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Emerging Adulthood, Political Decisions, and More (2004) project&lt;/em&gt; – Alan Reifman is an early proponent of collaborative undergraduate research projects. After successfully guiding the School Spirit Study Group, he called again to research methods instructors to collectively conduct a survey that included two emerging adulthood scales, some political attitudes and intentions, and other scales that contributors wanted to add to the paper and pencil survey. By the time the survey was finalized, it was over 10 pages long and included hundreds of individual items measuring dozens of distinct psychological constructs on 12 scales. In retrospect, the survey was too long and the invitation to add on measures might have caused of the attrition of committed contributors that occurred. However, the final sample included over 1300 cases from 11 distinct institutions across the US. A list of contributors and initial findings can be found at Alan Reifman’s &lt;a href="http://courses.ttu.edu/hdfs3390-reifman/fall04project.htm"&gt;Emerging Adulthood Page&lt;/a&gt;. This project suffered from the amorphous structure of the collaboration. In short, no one instructor was interested in all the constructs. To resolve this situation where wonderfully rich data sit unanalyzed, the contributors are inviting emerging adulthood theorists to analyze the data and submit their work for a special issue of Emerging Adulthood. Details for how to request the data are available on &lt;a href="https://openscienceframework.org/project/yjdaf/"&gt;the project’s OSF page&lt;/a&gt;. The deadline for a submission is July, 2014.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;International Situations Project (ISP)&lt;/em&gt; – David Funder and Esther Guillaume-Hanes from the University of California—Riverside have organized a coalition of international researchers (19 and counting) to complete an internet protocol. As they describe on their about page, “Participants will describe situations by writing a brief description of the situation they experienced the previous day at 7 pm. They will identify situational characteristics uing the Riverside Situation Q-sort (RSQ) which includes 89 items that participants place into one of nine categories ranging from not at all characteristic to very characteristics. They then identify the behaviors they displayed using the Riverside Behavioral Q-Sort (RBQ) which includes 68 items using &lt;a href="http://www.internationalsituationsproject.com/about"&gt;the same sorting procedure&lt;/a&gt;. The UC-Riverside researchers are taking primary responsibility for writing research reports.  They have papers in print and others in preparation where all contributors who provide more than 80 cases are listed as authors. In Fall 2012, Psi Chi and Psi Beta encouraged their members to replicate the ISP in the US to create a series of local samples yielding 11 samples with 5 more committed contributors (Grahe, Guillaume-Hanes, &amp;amp; Rudmann, 2014). Currently, contributors are invited to complete either this project or their subsequent Personality project which includes completing this protocol, then completing the California Adult Q-Sort two weeks later. Interested researchers should contact Esther Guillaume directly at eguil002@ucr.edu.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Collaborative Replications and Education Project (CREP)&lt;/em&gt; – &lt;a href="https://openscienceframework.org/project/WFC6u/"&gt;This project&lt;/a&gt; has recently started inviting contributions and is explicitly designed for undergraduates. Instructors who guide student research projects are encouraged to share the available studies list with their students. Hans IJzerman and Mark Brandt reviewed the top three cited empirical articles in the top journals in nine sub disciplines and rated them for feasibility to be completed by undergraduates selecting nine studies that were the most feasible. The project provides small ($200-$500) CREP research awards for completed projects (sponsored by Psi Chi and the Center for Open Science). Contributors will be encouraged/supported in writing and submitting reports for publication by the project coordinators. Anyone interested in participating should contact Jon Grahe (graheje@plu.edu).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Archival Project&lt;/em&gt; – This Center for Open Science &lt;a href="http://archivalproject.org/"&gt;project&lt;/a&gt; is also specifically designed as a crowd-sourcing opportunity for students. When it completes the beta-testing phase, the Archival Project will be publicly advertised. Unlike all the projects reviewed thus far, this project asks contributors to serve as coders rather than act as experimenters. It is a companion project to the OSC Reproducibility Project in that the target articles are from the same three Journals from the first three months of 2008. This project has a low bar for entry as training can take little time (particularly with the now available online tutorial) and coders can code as few as a single article and still make a real contribution. However, this project also has a system of honors as they state on their &lt;a href="http://archivalproject.org/pages/getting_involved"&gt;“getting involved” page&lt;/a&gt;:  “Contributors who provide five or more accurate codings will be listed as part of the collective authorship.”  This project was designed with the expectation that instructors will find the opportunity pedagogically useful and that they will employ it as a course exercise. Alternatively, students in organized clubs (such as Psi Chi) are invited to participate to increase their own methodological competence while simultaneously accumulating evidence of their contributions to an authentic research project. Finally, graduate students are invited to participate without faculty supervision. Interested parties should contact Johanna Cohoon (johannacohoon@gmail.com) for more information.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Future Opportunities&lt;/em&gt; – While this is intended to be an exhaustive list of open invitation projects, the field is not static and this list is likely to grow. What is exciting is that we now have ample opportunities to participate in “big science” with relatively small contributions. When developed with care, these projects follow Grahe et al. (2012)’s recommendation to take advantage of the magnitude of research being conducted each year by psychology students. The bar for entry varies in these projects from relatively intensive (e.g. Reproducibility Project, CREP) to relatively easy (e.g. Archival Project, ISP), providing opportunities for individuals with varying resources, from graduate students and PhD level researchers capable of completing high quality replications to students and instructors who seek opportunities in the classroom. Beyond the basic opportunity to participate in transformative research, these projects provide exemplars for how future collaborative projects should be designed and managed.&lt;/p&gt;
&lt;p&gt;This is surely an incomplete list of current or potential examples of crowd-sourcing research. Please share other examples as comments below. Further consider pointing out strengths, weaknesses, opportunities or threats that could emerge from collaborative research. Finally, any public statements about intentions to participate in this type of open science initiative are welcome. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Frank, M. C., &amp;amp; Saxe, R. (2012). Teaching replication. &lt;em&gt;Perspectives On Psychological Science&lt;/em&gt;,   7(6), 600-604. doi:10.1177/1745691612460686&lt;/p&gt;
&lt;p&gt;Grahe. J. E., Gullaume-Hanes, E., &amp;amp; Rudmann, J. (2014). Students collaborate to advance science: The International Situations Project. &lt;em&gt;Council for Undergraduate Research Quarterly&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Grahe, J. E., Reifman, A., Hermann, A. D., Walker, M., Oleson, K. C., Nario-Redmond, M., &amp;amp; Wiebe, R. P. (2012). Harnessing the undiscovered resource of student research projects. &lt;em&gt;Perspectives On Psychological Science&lt;/em&gt;, 7(6), 605-607. doi:10.1177/1745691612459057&lt;/p&gt;
&lt;p&gt;Karukstis, K. K., &amp;amp; Hensel, N. (2010) Transformative research at predominately undergraduate institutions.” Council of Undergraduate Research. Washington DC., USA.&lt;/p&gt;</summary></entry><entry><title>A publishing sting, but what was stung?</title><link href="http://osc.centerforopenscience.org/2013/10/04/a-publishing-sting-but-what-was-stung/" rel="alternate"></link><updated>2013-10-04T14:45:00-04:00</updated><author><name>Åse Innes-Ker</name></author><id>tag:osc.centerforopenscience.org,2013-10-04:2013/10/04/a-publishing-sting-but-what-was-stung/</id><summary type="html">&lt;p&gt;Before Open Science there was Open Access (OA) — a movement driven by the
desire to make published research publicly accessible (after all, the public
usually had paid for it), rather than hidden behind paywalls.&lt;/p&gt;
&lt;p&gt;Open Access is, by now, its very own strong movement — follow for example &lt;a href="http://bjoern.brembs.net/"&gt;Björn
Brembs&lt;/a&gt; if you want to know what is happening there — and 
there are now nice Open Access journals, like &lt;a href="http://www.plos.org/"&gt;PLOS&lt;/a&gt; and
&lt;a href="http://www.frontiersin.org/"&gt;Frontiers&lt;/a&gt;, which are
peer-reviewed and reputable. Some of these journals charge an article
processing fee for OA articles, but in many cases funders have introduced or
are developing provisions to cover these costs. (In fact, the big private
funder in Sweden INSISTS on it.)&lt;/p&gt;
&lt;p&gt;But, as always, situations where there is money involved and targets who are
desperate (please please please publish my baby so I won’t perish) breed mimics
and cuckoos and charlatans, ready to game the new playing field to their
advantage.  This is probably just a feature of the human condition (see
Triver’s “&lt;a href="http://www.amazon.com/The-Folly-Fools-Deceit-Self-Deception/dp/B008PGKA56"&gt;Folly of Fools&lt;/a&gt;”).&lt;/p&gt;
&lt;p&gt;There are lists of potentially predatory Open Access journals — I have linked
some in on my private blog (Åse Fixes Science)
&lt;a href="http://asefixesscience.wordpress.com/2013/04/14/open-access-watch-out-list/"&gt;here&lt;/a&gt;
and
&lt;a href="http://asefixesscience.wordpress.com/2013/05/12/more-on-predatory-open-access/"&gt;here&lt;/a&gt;.
Reputation counts.  Buyers beware!&lt;/p&gt;
&lt;p&gt;Demonstrating the challenges of this new marketplace, John Bohannon published
in Science (a decidedly not Open Access journal) a sting operation in which he
spoofed Open Access journals to test their peer-review system. The papers were
machine generated nonsense — one may recall the &lt;a href="http://en.wikipedia.org/wiki/Sokal_affair"&gt;Sokal Hoax&lt;/a&gt; from the previous
Science Wars.  One may also recall the classic Ceci paper from 1982, which made
the rounds again earlier this year (and I blogged about that one too - &lt;a href="http://asehelene.wordpress.com/2013/05/31/music-social-proof-appeal-and-peer-review/"&gt;on my
other blog&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Crucially, all of Bohannon’s papers contained fatal flaws that a decent
peer-reviewer should catch. The big news? Lots of them did not (though PLOS
did). Here’s the &lt;a href="http://www.sciencemag.org/content/342/6154/60"&gt;Science&lt;/a&gt; article with
its &lt;a href="http://comments.sciencemag.org/content/10.1126/science.342.6154.60"&gt;commentary stream&lt;/a&gt;,
and a commentary from &lt;a href="http://retractionwatch.wordpress.com/2013/10/03/science-reporter-spoofs-hundreds-of-journals-with-a-fake-paper/"&gt;Retraction Watch&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is, of course, interesting — and it is generating buzz.  But, it is also
generating some negative reaction. For one, Bohannon did not include the
regular non-OA journals in his test, so the experiment lacks a control group,
which means we can make no comparison and draw no firm inferences from the
data. The reason he states (it is quoted on the Retraction Watch site) is the
very long turnaround times for regular journals, which can be months, even a
year (or longer, as I’ve heard). I kinda buy it, but this is really what is
angering the Open Access crowd, who sees this letter as an attempt to
implicate Open Access itself as the source of the problem. And, apart from
Bohannon not including regular journals in his test, Science published what he
wrote without peer reviewing it.&lt;/p&gt;
&lt;p&gt;My initial take? I think it is important to test these things — to uncover the
flaws in the system, and also to uncover the cuckoos and the mimics and the
gamers. Of course, the problems in peer review are not solely on the shoulders
of Open Access — Diederik Stapel, and other frauds, published almost
exclusively in paywalled journals (including Science).  The status of peer
review warrants its own scrutiny.&lt;/p&gt;
&lt;p&gt;But, I think the Open Access advocates have a really important point to
make.  As noted on Retraction Watch, Bohannon's study didn't include
non-OA journals, so it's unclear whether the peer-review problems he
identified in OA journals are unique to their OA status.&lt;/p&gt;
&lt;p&gt;I’ll end by linking in some of the commentary that I have seen so far — and, of
course, we’re happy to hear your comments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Guardian: &lt;a href="http://www.theguardian.com/higher-education-network/2013/oct/04/open-access-journals-fake-paper"&gt;Hundreds of Open Access journals accept fake science paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Peter Suber: &lt;a href="https://plus.google.com/109377556796183035206/posts/CRHeCAtQqGq"&gt;New “Sting” of Weak Open-Access Journals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Curt Rice: &lt;a href="http://curt-rice.com/2013/10/04/what-science-and-the-gonzo-scientist-got-wrong-open-access-will-make-research-better/"&gt;What Science — and the Gonzo Scientist — got wrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The conversation: &lt;a href="http://theconversation.com/flawed-sting-operation-singles-out-open-access-journals-18846"&gt;Flawed sting operation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The comics Grid: &lt;a href="http://blog.comicsgrid.com/2013/10/whos-afraid-open-access/"&gt;Who's afraid of Open Access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Michael Eisen: &lt;a href="http://www.michaeleisen.org/blog/?p=1439"&gt;I confess, I wrote the Arsenic DNA paper to expose flaws in peer-review at subscription based journals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/search?had_popular=true&amp;amp;q=JournalSting"&gt;#JournalSting&lt;/a&gt; and &lt;a href="https://twitter.com/search?q=%23openaccess&amp;amp;src=hash"&gt;#openaccess&lt;/a&gt; on Twitter&lt;/li&gt;
&lt;li&gt;Sauropod Vertebra Picture of the week: &lt;a href="http://svpow.com/2013/10/03/john-bohannons-peer-review-sting-against-science/"&gt;John Bohannon’s peer-review sting against Science&lt;/a&gt;. (The last one is also collecting other links commenting on this paper - good resource).&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Smoking on an Airplane</title><link href="http://osc.centerforopenscience.org/2013/10/02/smoking-on-an-airplane/" rel="alternate"></link><updated>2013-10-02T10:30:00-04:00</updated><author><name>Denny Borsboom</name></author><id>tag:osc.centerforopenscience.org,2013-10-02:2013/10/02/smoking-on-an-airplane/</id><summary type="html">&lt;p&gt;&lt;img src="images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /&gt;&lt;/p&gt;
&lt;p&gt;People used to smoke on airplanes. It's hard to imagine, but it's true. In less
than twenty years, smoking on airplanes has grown so unacceptable that it has
become difficult to see how people ever condoned it in the first place.
Psychological scientists used to refuse to share their data. It's not so hard to
imagine, and it's still partly true. However, my guess is that a few years from
now, data-secrecy will be as unimaginable as smoking on an airplane is today.
We've already come a long way. When in 2005 Jelte Wicherts, Dylan Molenaar,
Judith Kats, and I asked 141 psychological scientists to send us their raw data
to verify their analyses, many of them told us to get lost - even though, at
the time of publishing the research, they had signed an agreement to share
their data upon request. "I don't have time for this," one famous psychologist
said bluntly, as if living up to a written agreement is a hobby rather than a
moral responsibility. Many psychologists responded in the same way. If they
responded at all, that is.&lt;/p&gt;
&lt;p&gt;Like Diederik Stapel.&lt;/p&gt;
&lt;p&gt;I remember that Judith Kats, the student in our group who prepared the emails
asking researchers to make data available, stood in my office. She explained to
me how researchers had responded to our emails. Although many researchers
had refused to share data, some of our Dutch colleagues had done so in an
extremely colloquial, if not downright condescending way. Judith asked me how
she should respond. Should she once again inform our colleagues that they had
signed an APA agreement, and that they were in violation of a moral code?&lt;/p&gt;
&lt;p&gt;I said no.&lt;/p&gt;
&lt;p&gt;It's one of the very few things in my scientific career that I regret. Had we
pushed our colleagues to the limit, perhaps we would have been able to identify
Stapel's criminal practices years earlier. As his autobiography shows, Stapel
counterfeited his data in an unbelievably clumsy way, and I am convinced that
we would have easily identified his data as fake.
I had many reasons for saying no, which seemed legitimate at the time, but in
hindsight I think my behavior was a sign of adaptation to a defective research
culture. I had simply grown accustomed to the fact that, when I entered an
elevator, conversations regarding statistical analyses would fall silent. I took
it as a fact of life that, after we methodologists had explained students how to
analyze data in a responsible way, some of our colleagues would take it upon
themselves to show students how scientific data analysis really worked (today,
these practices are known as p-hacking). We all lived in a scientific version of
The Matrix, in which the reality of research was hidden from all - except those
who had the doubtful honor of being initiated. There was the science that people
reported and there was the science that people did.&lt;/p&gt;
&lt;p&gt;In Groningen University, where Stapel used to work, he was known as The Lord
of the Data, because he never let anyone near his SPSS files. He pulled results
out of thin air, throwing them around as presents to his co-workers, and when
anybody asked him to show the underlying data files, he simply didn't respond.
Very few people saw this as problematic, because, hey, these were his data, and
why should Stapel share his data with outsiders?&lt;/p&gt;
&lt;p&gt;That was the moral order of scientific psychology. Data are private property.
Nosy colleagues asking for data? Just chase them away, like you chase coyotes
from your farm. That is why researchers had no problem whatsoever denying
access to their data, and that is why several people saw the data-sharing request
itself as unethical. "Why don't you trust us?," I recall one researcher saying in a
suspicious tone of voice.&lt;/p&gt;
&lt;p&gt;It is unbelievable how quickly things have changed.&lt;/p&gt;
&lt;p&gt;In the wake of the Stapel case, the community of psychological scientists
committed to openness, data-sharing, and methodological transparency quickly
reached a critical mass. The &lt;a href="https://openscienceframework.org"&gt;Open Science Framework&lt;/a&gt; allows researchers to
archive all of their research materials, including stimuli, analysis code, and
data, to make them public by simply pressing a button. The new &lt;a href="http://openpsychologydata.metajnl.com"&gt;Journal of Open
Psychology Data&lt;/a&gt; offers an outlet
specifically designed to publish datasets, thereby giving these the status of a
publication. &lt;a href="http://psychdisclosure.org"&gt;PsychDisclosure.org&lt;/a&gt; asks researchers to document decisions
regarding, e.g., sample size determination and variable selection, that were
left unmentioned in publications; most researchers provide this information
without hesitation - some actually do so voluntarily. The journal Psychological
Science will likely implement requirements for this type information in the
submission process. Data-archiving possibilities are growing like crazy. Major
funding institutes require data-archiving or are preparing regulations that do.
In the &lt;a href="https://openscienceframework.org/project/EZcUj/wiki/home"&gt;Reproducibility Project&lt;/a&gt;, hundreds of studies are being replicated in a
concerted effort. As a major symbol of these developments, we now have the
&lt;a href="http://centerforopenscience.org"&gt;Center for Open Science&lt;/a&gt;, which facilitates the massive grassroots effort to
open up the scientific regime in psychology.&lt;/p&gt;
&lt;p&gt;If you had told me that any of this would happen back in 2005, I would have
laughed you away, just as I would have laughed you away in 1990, had you told
me that the future would involve such bizarre elements as smoke-free airplanes.&lt;/p&gt;
&lt;p&gt;The moral order of research in psychology has changed. It has changed for the
better, and I hope it has changed for good.&lt;/p&gt;</summary></entry><entry><title>Welcome!</title><link href="http://osc.centerforopenscience.org/2013/10/02/welcome/" rel="alternate"></link><updated>2013-10-02T10:15:00-04:00</updated><author><name>OSC</name></author><id>tag:osc.centerforopenscience.org,2013-10-02:2013/10/02/welcome/</id><summary type="html">&lt;p&gt;Welcome to the blog of the Open Science Collaboration! We are a loose network of researchers, professionals, citizen scientists, and others with an interest in open science, metascience, and good scientific practices.  We’ll be writing about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open science topics like reproducibility, transparency in methods and analyses, and changing editorial and publication practices&lt;/li&gt;
&lt;li&gt;Updates on open science initiatives like the &lt;a href="https://openscienceframework.org/reproducibility/‎"&gt;Reproducibility Project&lt;/a&gt; and opportunities to get involved in new projects like the &lt;a href="http://archivalproject.org/"&gt;Archival Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Interviews and opinions from researchers, developers, publishers, and citizen scientists working to make science more transparent, rigorous, or reproducible. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We hope that the blog will stimulate open discussion and help improve the way science is done!&lt;/p&gt;
&lt;p&gt;If you'd like to suggest a topic for a post, propose a guest post or column, or get involved with moderation, promotion, or technical development, we would love to hear from you! Email us at &lt;a href="mailto:oscbloggers@gmail.com"&gt;oscbloggers@gmail.com&lt;/a&gt; or tweet @OSCbloggers.&lt;/p&gt;
&lt;p&gt;The OSC is an open collaboration - anyone is welcome to join, contribute to discussions, or develop a project. The OSC blog is supported by the &lt;a href="http://centerforopenscience.org"&gt;Center for Open Science&lt;/a&gt;, a non-profit organization dedicated to increasing openness, integrity and reproducibility of scientific research.  &lt;/p&gt;</summary></entry></feed>