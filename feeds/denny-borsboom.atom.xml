<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Open Science Collaboration Blog</title><link href="http://osc.centerforopenscience.org/" rel="alternate"></link><link href="http://osc.centerforopenscience.org/feeds/denny-borsboom.atom.xml" rel="self"></link><id>http://osc.centerforopenscience.org/</id><updated>2013-11-20T02:00:00-05:00</updated><entry><title>Theoretical Amnesia</title><link href="http://osc.centerforopenscience.org/2013/11/20/theoretical-amnesia/" rel="alternate"></link><updated>2013-11-20T02:00:00-05:00</updated><author><name>Denny Borsboom</name></author><id>tag:osc.centerforopenscience.org,2013-11-20:2013/11/20/theoretical-amnesia/</id><summary type="html">&lt;p&gt;&lt;img src="/images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /&gt;&lt;/p&gt;
&lt;p&gt;In the past few months, the Center for Open Science and its associated enterprises have gathered enormous support in the community of psychological scientists. While these developments are happy ones, in my view, they also cast a shadow over the field of psychology: clearly, many people think that the activities of the Center for Open Science, like organizing massive replication work and promoting preregistration, are &lt;em&gt;necessary&lt;/em&gt;. That, in turn, implies that something in the current scientific order is seriously &lt;em&gt;broken&lt;/em&gt;. I think that, apart from working towards improvements, it is useful to investigate what that something is. In this post, I want to point towards a factor that I think has received too little attention in the public debate; namely, the near absence of unambiguously formalized scientific theory in psychology.&lt;/p&gt;
&lt;p&gt;Scientific theories are perhaps the most bizarre entities that the scientific imagination has produced. They have incredible properties that, if we weren’t so familiar with them, would do pretty well in a &lt;em&gt;Harry Potter&lt;/em&gt; novel. For instance, scientific theories allow you to work out, on a piece of paper, what would happen to stuff in conditions that aren’t actually realized. So you can figure out whether an imaginary bridge will stand or collapse in imaginary conditions. You can do this by simply just feeding some imaginary quantities that your imaginary bridge would have (like its mass and dimensions) to a scientific theory (say, Newton’s) and out comes a prediction on what will happen. In the more impressive cases, the predictions are so good that you can actually design the entire bridge on paper, then build it according to specifications (by systematically mapping empirical objects to theoretical terms), and then the bridge will do precisely what the theory says it should do. No surprises.&lt;/p&gt;
&lt;p&gt;That’s how they put a man on the moon and that’s how they make the computer screen you’re now looking at. It’s all done in theory before it’s done for real, and that’s what makes it possible to construct complicated but functional pieces of equipment. This is, in effect, why scientific theory makes technology possible, and therefore this is an absolutely central ingredient of the scientific enterprise which, &lt;em&gt;without&lt;/em&gt; technology, would be much less impressive than it is.&lt;/p&gt;
&lt;p&gt;It’s useful to take stock here, and marvel. A good scientific theory allows you infer what would happen to things in certain situations &lt;em&gt;without creating the situations&lt;/em&gt;. Thus, scientific theories are crystal balls that actually work. For this reason, some philosophers of science have suggested that scientific theories should be interpreted as &lt;em&gt;inference tickets&lt;/em&gt;. Once you’ve got the ticket, you get to sidestep all the tedious empirical work. Which is great, because empirical work is, well, tedious. Scientific theories are thus exquisitely suited to the needs of lazy people.&lt;/p&gt;
&lt;p&gt;My field – psychology – unfortunately does not afford much of a lazy life. We don’t have theories that can offer predictions sufficiently precise to intervene in the world with appreciable certainty. That’s why there exists no such thing as a psychological engineer. And that’s why there are fields of theoretical physics, theoretical biology, and even theoretical economics, while there is no parallel field of theoretical psychology. It is a sad but, in my view, inescapable conclusion: we don’t have much in the way of scientific theory in psychology. For this reason, we have very few inference tickets – let alone inference tickets that work.&lt;/p&gt;
&lt;p&gt;And that’s why psychology is so hyper-ultra-mega empirical. We never know how our interventions will pan out, because we have no theory that says how they will pan out (incidentally, that’s also why we need preregistration: in psychology, predictions are made by individual researchers rather than by standing theory, and you can’t trust people the way you can trust theory). The upshot is that, if we want to know what would happen if we did X, we have to actually do X. Because we don’t have inference tickets, we never get to take the shortcut. We always have to wade through the empirical morass. Always.&lt;/p&gt;
&lt;p&gt;This has important consequences. For instance, as a field has less theory, it has to leave more to the data. Since you can’t learn anything from data without the armature of statistical analysis, a field without theory tends to grow a thriving statistical community. Thus, the role of statistics grows as soon as the presence of scientific theory wanes. In extreme cases, when statistics has entirely taken over, fields of inquiry can actually develop a kind of philosophical disorder: theoretical amnesia. In fields with this disorder, researchers no longer know what a theory is, which means that they can neither recognize its presence nor its absence. In such fields, for instance, a statistical model – like a factor model – can come to occupy the vacuum created by the absence of theory. I am often afraid that this is precisely what has happened with the advent of “theories” like those of general intelligence (a single factor model) and the so-called “Big Five” of personality (a five-factor model). In fact, I am afraid that this happened in many fields in psychology, where statistical models (which, in their barest null-hypothesis testing form, are misleadingly called “effects”) rule the day.&lt;/p&gt;
&lt;p&gt;If your science thrives on experiment and statistics, but lacks the power of theory, you get peculiar problems. Most importantly, you get slow. To see why, it’s interesting to wonder how psychologists would build a bridge, if they were to use their typical methodological strategies. Probably, they would build a thousand bridges, record whether they stand or fall, and then fit a regression equation to figure out which properties are predictive of the outcome. Predictors would be chosen on the basis of statistical significance, which would introduce a multiple testing problem. In response, some of the regressors might be clustered through factor analysis, to handle the overload of predictive variables. Such analyses would probably indicate lots of structure in the data, and psychologists would likely find that the bridges’ weight, size, and elasticity loads on a single latent “strength factor”, producing the “theory” that bridges higher on the “strength factor” are less likely to fall down. Cross validation of the model would be attempted by reproducing the analysis in a new sample of a thousand bridges, to weed out chance findings. It’s likely that, after many years of empirical research, and under a great number of “context-dependent” conditions that would be poorly understood, psychologists would be able to predict a modest but significant proportion of the variance in the outcome variable. Without a doubt, it would ta  ke a thousand years to establish empirically what Newton grasped in a split second, as he wrote down his F=m*a.&lt;/p&gt;
&lt;p&gt;Because increased reliance on empirical data makes you so incredibly slow, it also makes you susceptible to fads and frauds. A good theory can be tested in an infinity of ways, many of which are directly available to the interested reader (this is what give classroom demonstrations such enormous evidential force). But if your science is entirely built on generalizations derived from specifics of tediously gathered experimental data, you can’t really test these generalizations without tediously gathering the same, or highly similar, experimental data. That’s not something that people typically like to do, and it’s certainly not what journals want to print. As a result, a field can become dominated by poorly tested generalizations. When that happens, you’re in very big trouble. The reason is that your scientific field becomes susceptible to the equivalent of what evolutionary theorists call free riders: people who capitalize on the invested honest work of others by consistently taking the moral shortcut. Free riders can come to rule a scientific field if two conditions are satisfied: (a) fame is bestowed on whoever dares to make the most &lt;em&gt;adventurous&lt;/em&gt; claims (rather than the most &lt;em&gt;defensible&lt;/em&gt; ones), and (b) it takes longer to falsify a bogus claim than it takes to become famous. If these conditions are satisfied, you can build your scientific career on a fad and get away with it. By the time they find out your work really doesn’t survive detailed scrutiny, you’re sitting warmly by the fire in the library of your National Academy of Sciences&lt;sub&gt;1&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;Much of our standard methodological teachings in psychology rest on the implicit assumption that scientific fields are similar if not identical in their methodological setup. That simply isn’t true. Without theory, the scientific ball game has to be played by different rules. I think that these new rules are now being invented: without good theory, you need fast acting replication teams, you need a reproducibility project, and you need preregistered hypotheses. Thus, the current period of crisis may lead to extremely important methodological innovations – especially those that are crucial in fields that are low on theory.&lt;/p&gt;
&lt;p&gt;Nevertheless, it would be extremely healthy if psychologists received more education in fields which do have some theories, even if they are empirically shaky ones, like you often see in economics or biology. In itself, it’s no shame that we have so little theory: psychology probably has the hardest subject matter ever studied, and to change that may very well take a scientific event of the order of Newton’s discoveries. I don’t know how to do it and I don’t think anybody else knows either. But what we can do is keep in contact with other fields, and at least try to remember what theory is and what it’s good for, so that we don’t fall into theoretical amnesia. As they say, it’s the unknown unknowns that hurt you most.&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;1  Caveat: I am not saying that people do this on purpose. I believe that free riders are typically unaware of the fact that they are free riders – people are very good at labeling their own actions positively, especially if the rest of the world says that they are brilliant. So, if you think this post isn’t about you, that could be entirely wrong. In fact, I cannot even be   sure that this post isn’t about me.&lt;/sub&gt;&lt;/p&gt;</summary></entry><entry><title>Smoking on an Airplane</title><link href="http://osc.centerforopenscience.org/2013/10/02/smoking-on-an-airplane/" rel="alternate"></link><updated>2013-10-02T10:30:00-04:00</updated><author><name>Denny Borsboom</name></author><id>tag:osc.centerforopenscience.org,2013-10-02:2013/10/02/smoking-on-an-airplane/</id><summary type="html">&lt;p&gt;&lt;img src="/images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /&gt;&lt;/p&gt;
&lt;p&gt;People used to smoke on airplanes. It's hard to imagine, but it's true. In less
than twenty years, smoking on airplanes has grown so unacceptable that it has
become difficult to see how people ever condoned it in the first place.
Psychological scientists used to refuse to share their data. It's not so hard to
imagine, and it's still partly true. However, my guess is that a few years from
now, data-secrecy will be as unimaginable as smoking on an airplane is today.
We've already come a long way. When in 2005 Jelte Wicherts, Dylan Molenaar,
Judith Kats, and I asked 141 psychological scientists to send us their raw data
to verify their analyses, many of them told us to get lost - even though, at
the time of publishing the research, they had signed an agreement to share
their data upon request. "I don't have time for this," one famous psychologist
said bluntly, as if living up to a written agreement is a hobby rather than a
moral responsibility. Many psychologists responded in the same way. If they
responded at all, that is.&lt;/p&gt;
&lt;p&gt;Like Diederik Stapel.&lt;/p&gt;
&lt;p&gt;I remember that Judith Kats, the student in our group who prepared the emails
asking researchers to make data available, stood in my office. She explained to
me how researchers had responded to our emails. Although many researchers
had refused to share data, some of our Dutch colleagues had done so in an
extremely colloquial, if not downright condescending way. Judith asked me how
she should respond. Should she once again inform our colleagues that they had
signed an APA agreement, and that they were in violation of a moral code?&lt;/p&gt;
&lt;p&gt;I said no.&lt;/p&gt;
&lt;p&gt;It's one of the very few things in my scientific career that I regret. Had we
pushed our colleagues to the limit, perhaps we would have been able to identify
Stapel's criminal practices years earlier. As his autobiography shows, Stapel
counterfeited his data in an unbelievably clumsy way, and I am convinced that
we would have easily identified his data as fake.
I had many reasons for saying no, which seemed legitimate at the time, but in
hindsight I think my behavior was a sign of adaptation to a defective research
culture. I had simply grown accustomed to the fact that, when I entered an
elevator, conversations regarding statistical analyses would fall silent. I took
it as a fact of life that, after we methodologists had explained students how to
analyze data in a responsible way, some of our colleagues would take it upon
themselves to show students how scientific data analysis really worked (today,
these practices are known as p-hacking). We all lived in a scientific version of
The Matrix, in which the reality of research was hidden from all - except those
who had the doubtful honor of being initiated. There was the science that people
reported and there was the science that people did.&lt;/p&gt;
&lt;p&gt;In Groningen University, where Stapel used to work, he was known as The Lord
of the Data, because he never let anyone near his SPSS files. He pulled results
out of thin air, throwing them around as presents to his co-workers, and when
anybody asked him to show the underlying data files, he simply didn't respond.
Very few people saw this as problematic, because, hey, these were his data, and
why should Stapel share his data with outsiders?&lt;/p&gt;
&lt;p&gt;That was the moral order of scientific psychology. Data are private property.
Nosy colleagues asking for data? Just chase them away, like you chase coyotes
from your farm. That is why researchers had no problem whatsoever denying
access to their data, and that is why several people saw the data-sharing request
itself as unethical. "Why don't you trust us?," I recall one researcher saying in a
suspicious tone of voice.&lt;/p&gt;
&lt;p&gt;It is unbelievable how quickly things have changed.&lt;/p&gt;
&lt;p&gt;In the wake of the Stapel case, the community of psychological scientists
committed to openness, data-sharing, and methodological transparency quickly
reached a critical mass. The &lt;a href="https://openscienceframework.org"&gt;Open Science Framework&lt;/a&gt; allows researchers to
archive all of their research materials, including stimuli, analysis code, and
data, to make them public by simply pressing a button. The new &lt;a href="http://openpsychologydata.metajnl.com"&gt;Journal of Open
Psychology Data&lt;/a&gt; offers an outlet
specifically designed to publish datasets, thereby giving these the status of a
publication. &lt;a href="http://psychdisclosure.org"&gt;PsychDisclosure.org&lt;/a&gt; asks researchers to document decisions
regarding, e.g., sample size determination and variable selection, that were
left unmentioned in publications; most researchers provide this information
without hesitation - some actually do so voluntarily. The journal Psychological
Science will likely implement requirements for this type information in the
submission process. Data-archiving possibilities are growing like crazy. Major
funding institutes require data-archiving or are preparing regulations that do.
In the &lt;a href="https://openscienceframework.org/project/EZcUj/wiki/home"&gt;Reproducibility Project&lt;/a&gt;, hundreds of studies are being replicated in a
concerted effort. As a major symbol of these developments, we now have the
&lt;a href="http://centerforopenscience.org"&gt;Center for Open Science&lt;/a&gt;, which facilitates the massive grassroots effort to
open up the scientific regime in psychology.&lt;/p&gt;
&lt;p&gt;If you had told me that any of this would happen back in 2005, I would have
laughed you away, just as I would have laughed you away in 1990, had you told
me that the future would involve such bizarre elements as smoke-free airplanes.&lt;/p&gt;
&lt;p&gt;The moral order of research in psychology has changed. It has changed for the
better, and I hope it has changed for good.&lt;/p&gt;</summary></entry></feed>