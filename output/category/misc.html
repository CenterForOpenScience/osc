<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>A Pelican Blog &middot; articles in the "misc" category</title>
<!--        <link rel="shortcut icon" href="http://centerforopenscience.github.io/osc/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://centerforopenscience.github.io/osc/category/misc.html">misc</a></li>
                <li><a href="http://centerforopenscience.github.io/osc">Home</a></li>
<li><a href="http://centerforopenscience.github.io/osc/pages/about.html">About</a></li>
<li><a href="http://centerforopenscience.github.io/osc/pages/authors.html">Authors</a></li>
<li><a href="http://centerforopenscience.github.io/osc/pages/policy.html">Policy</a></li>
<li><a href="http://centerforopenscience.github.io/osc/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://centerforopenscience.github.io/osc"><img src="http://centerforopenscience.github.io/osc/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Jul 16,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/07/16/digging-a-little-deeper/" rel="bookmark" title="Permanent Link to &quot;Digging a little deeper - Understanding workflows of archaeologists&quot;">Digging a little deeper - Understanding workflows of archaeologists</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/sara-bowman-mallory-kidwell-and-erin-braswell.html" rel="author">Sara Bowman, Mallory Kidwell, and Erin Braswell</a>

   			   <p>Scientific domains vary by the tools and instruments used, the way data are collected and managed, and even how results are analyzed and presented. As advocates of open science practices, it’s important that we understand the common obstacles to scientific workflow across many domains.  The COS team visits scientists in their labs and out in the field to discuss and experience their research processes first-hand. We experience the day-to-day of researchers and do our own investigating.  We find where data loss occurs, where there are inefficiencies in workflow, and what interferes with reproducibility.  These field trips inspire new tools and features for the <a href="http://osf.io/">Open Science Framework</a> to support openness and reproducibility across scientific domains.</p>
<p>Last week, the team visited the <a href="http://www.monticello.org/site/research-and-collections/monticello-archaeology">Monticello Department of Archaeology</a> to dig a little deeper (bad pun) into the workflow of archaeologists, as well as learn about the <a href="http://www.daacs.org/">Digital Archaeological Archive of Comparative Slavery</a> (DAACS). Derek Wheeler, Research Archaeologist at Monticello, gave us a nice overview of how the Archaeology Department surveys land for artifacts. Shovel test pits, approximately 1 foot square, are dug every 40 feet on center as deep as anyone has dug in the past (i.e., down to undisturbed clay). If artifacts are found, the shovel test pits are dug every 20 feet on center.  At Monticello, artifacts are primarily man-made items like nails, bricks or pottery. The first 300 acres surveyed contained 12,000 shovel test pits -- and that’s just 10% of the total planned survey area. That’s a whole lot of holes, and even more data. </p>
<p><img src="/images/monticello_1.jpg" alt="Fraser Neiman addresses crowd" >
<em>Fraser Neiman, Director of Archaeology at Monticello, describes the work being done to excavate on Mulberry Row - the industrial hub of Jefferson’s agricultural industry.</em></p>
<p>At the <a href="http://www.monticello.org/site/research-and-collections/mulberry-row-reassessment">Mulberry Row</a> excavation site, Fraser Neiman, Director of Archaeology, explained the meticulous and painstaking process of excavating <a href="http://en.wikipedia.org/wiki/Quadrat">quadrats</a>, small plots of land isolated for study. Within a quadrat, there exist contexts - stratigraphic units. Any artifacts found within a context are carefully recorded on a context sheet - what the artifact is, its location within the quadrat, along with information about the fill (dirt, clay, etc.) in the context. The fill itself is screened to pull out smaller artifacts the eye may not catch. All of the excavation and data collection at the Mulberry Row Reassessment is conducted following the standards of the Digital Archaeological Archive of Comparative Slavery (DAACS). Standards developed by DAACS help archaeologists in the Chesapeake region to generate, report, and compare data from 20 different sites across the region in a systematic way. Without these standards, archiving and comparing artifacts from different sites would be extremely difficult. </p>
<p><img src="/images/monticello_2.jpg" alt="Researchers measure excavation site" >
<em>Researchers make careful measurements at the Monticello Mulberry Row excavation site, while recording data on a context sheet.</em></p>
<p>The artifacts, often <a href="http://en.wikipedia.org/wiki/Sherd">sherds</a>, are collected by context and taken to the lab for washing, labeling, analysis and storage. After washing, every sherd within a particular context is labeled with the same number and stored together. All of the data from the context sheets, as well as photos of the quadrants and sherds, are carefully input into DAACS following the standards set out in the DAACS <a href="http://www.daacs.org/about-the-database/daacs-cataloging-manual/">Cataloging Manual</a>. There is an enormous amount of manual labor associated with preparing and curating each artifact.  Jillian Galle, Project Manager of DAACS, described the extensive training users must undergo in order to deposit their data in the archive to ensure the standards outlined by the Cataloging Manual are kept. This regimented process ensures the quality and consistency of the data- and thus its utility.  The result is a publicly available dataset of the history of Monticello for researchers of all kinds to examine this important site in America’s history.</p>
<p><img src="/images/monticello_3.jpg" alt="Washed and numbered sherds" >
<em>These sherds have been washed and numbered to denote their context.</em></p>
<p>Our trip to Monticello Archaeology was eye-opening, as none of us had any practical experience with archaeological research or data. The impressive DAACS protocols and standards represent an important aspect of all scientific research - the ability to accurately capture large amounts of data in a systematic, thoughtful way - and then share it freely with others. </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/07/16/digging-a-little-deeper/#disqus_thread" data-disqus-identifier="2014/07/16/digging-a-little-deeper/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/07/16/digging-a-little-deeper/">Posted at 11:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jul 10,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/07/10/what-jason-mitchell-gets-right/" rel="bookmark" title="Permanent Link to &quot;What Jason Mitchell's 'On the emptiness of failed replications' gets right&quot;">What Jason Mitchell's 'On the emptiness of failed replications' gets right</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/tom-stafford.html" rel="author">Tom Stafford</a>

   			   <p>Jason Mitchell's essay <a href="http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm">'On the emptiness of failed replications'</a> is notable for being against the current effort to publish replication attempts. Commentary on the essay that I saw was pretty negative (e.g. <a href="http://io9.com/i-was-in-an-undergrad-symposium-on-research-where-diffe-1601474824">"awe-inspiringly clueless"</a>, <a href="https://twitter.com/JustinWolfers/status/486551815815561216">“defensive pseudo-scientific, anti-Bayesian academic ass-covering”</a>, <a href="https://twitter.com/BenLillie/status/486198755385827329">"Do you get points in social psychology for publicly declaring you have no idea how science works?"</a>).</p>
<p>Although I reject his premises, and disagree with his conclusion, I don't think Mitchell's arguments are incomprehensibly mad. This seems to put me in a minority, so I thought I'd try and explain the value in what he's saying. I'd like to walk through his essay assuming he is a thoughtful rational person. Why would a smart guy come to the views he has? What is he really trying to say, and what are his assumptions about the world of psychology that might, perhaps, illuminate our own assumptions?</p>
<p><strong>Experiments as artefacts, not samples</strong></p>
<p>First off, key to Mitchell's argument is a view that experiments are complex artefacts, in the construction of which errors are very likely. Effects, in this view, are hard won, eventually teased out via a difficult process of refinement and validation. The value of replication is self-evident to anyone who thinks statistically: sampling error and publication bias will produce lots of false positives, you improve your estimate of the true effect by independent samples (= replications). Mitchell seems to be saying that the experiments are so complex that replications by other labs aren't independent samples of the same effect. Although they are called replications there are, he claims, most likely to be botched, and so informative of nothing more than the incompetence of the replicators.</p>
<p>When teaching our students many of us will have deployed the saying "The plural of anecdote is not data". What we mean by this is that many weak observations - of ghosts, aliens or psychic powers - do not combine multiplicatively to make strong evidence in favour of these phenomena. If I've read him right, Mitchell is saying the same thing about replication experiments - many weak experiments are uninformative about real effects.</p>
<p><strong>Tacit practical knowledge</strong></p>
<p>Part of Mitchell's argument rests on the importance of tacit knowledge in running experiments (see his section "The problem with recipe-following"). We all know that tacit knowledge about experimental procedures exists in science. Mitchell puts a heavy weight on the importance of this. This is a position which presumably would have lots of sympathy from Daniel Kahneman, <a href="http://www.scribd.com/doc/225285909/Kahneman-Commentary">who suggested that all replication attempts should involve the original authors</a>.</p>
<p>There's a tension here between how science should be and how it is. Obviously our job is to make things explicit, to <a href="http://psychsciencenotes.blogspot.co.uk/2014/05/psychologys-real-replication-problem.html">explain how to successfully run experiments so that anyone can run them</a> but the truth is, full explanations aren't always possible. Sure, anyone can try and replicate based on a methods section, but - says Mitchell - you will probably be wasting your time generating noise rather than data, and shouldn't be allowed to commit this to the scientific record.</p>
<p>Most of us would be comfortable with the idea that if a non-psychologist ran our experiments they might make some serious errors (one thinks of the hash some physical scientists made of psi-experiments, failing completely to account for things like demand effects, for example). Mitchell's line of thought here seems to take this one step further, you can't run a social psychologist's experiments without special training in social psychology. Or even, maybe, you can't successfully run another lab's experiment without training from that lab.</p>
<p>I think happen to think he's wrong on this, and that he neglects to mention the harm of assuming that successful experiments have a <a href="https://twitter.com/hpashler/status/486566352077848577">"special sauce"</a> which cannot be easily communicated (it seems to be a road to elitism and mysticism to me, completely contrary to the goals science should have). Nonetheless, there's definitely some truth to the idea, and I think it is useful to consider the errors we will make if we assume the contrary, that methods sections are complete records and no special background is required to run experiments.</p>
<p><strong>Innuendo</strong></p>
<p>Mitchell makes the claim that targeting an effect for replication amounts to the innuendo that the effects under inspection are unreliable, which is a slur on the scientists who originally published them. Isn't this correct? Several people on twitter admitted, or tacitly admitted, that their prior beliefs were that many of these effects aren't real. There is something disingenuous about claiming, on the one hand, that all effects should be replicated, but, on the other, targeting particular effects for attention. If you bought Mitchell's view that experiments are delicate artefacts which render most replications uninformative, you can see how the result is a situation which isn't just uninformative but actively harmful to the hard-working psychologists whose work is impugned. Even if you don't buy that view, you might think that selection of which effects should be the focus of something like the Many Labs project is an active decision made by a small number of people, and which targets particular individuals. How this processes works out in practice deserves careful consideration, even if everyone agrees that it is a Good Thing overall.</p>
<p><strong>Caveats</strong></p>
<p>There are a number of issues in Mitchell's essay I haven't touched on - this isn't meant to be a complete treatment, just an explanation of some of the reasonable arguments I think he makes. Even if I disagree with them, I think they are reasonable; they aren't as obviously wrong as some have suggested and should be countered rather than dismissed.</p>
<p>Stepping back, my take on the 'replication crisis' in psychology is that it really isn't about replication. Instead, this is what digital disruption looks like in a culture organised around scholarly kudos rather than profit. We now have the software tools to coordinate data collection, share methods and data, analyse data, and interact with non-psychologists, both directly and via the media, in unprecedented ways and at an unprecedented rate. Established scholarly communities are threatened as "the way things are done" is challenged. Witness John Bargh's <a href="http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/">incredulous reaction to having his work challenged</a> (and note that this was 'a replicate and explain via alternate mechanism' type study that Mitchell says is a valid way of doing replication). Witness the recent <a href="http://www.bmj.com/content/348/bmj.g2228/rr/693104">complaint of medical researcher Jonathan S. Nguyen-Van-Tam</a> when a journalist included critique of his analysis technique in a report on his work. These guys obviously believe in a set of rules concerning academic publishing which many of us aren't fully aware of or believe no longer apply.</p>
<p>By looking at other disrupted industries, such as music or publishing, we can discern morals for both sides. Those who can see the value in the old way of doing things, like Mitchell, need to articulate that value and fast. There's no way of going back, but we need to salvage the good things about tight-knit, slow moving, scholarly communities. The moral for the progressives is that we shouldn't let the romance of change blind us to the way that the same old evils will reassert themselves in new forms, by hiding behind a <a href="https://www.jacobinmag.com/2014/01/sharing-and-caring/">facade of being new, improved and more equitable</a>.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/07/10/what-jason-mitchell-gets-right/#disqus_thread" data-disqus-identifier="2014/07/10/what-jason-mitchell-gets-right/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/07/10/what-jason-mitchell-gets-right/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jul  9,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/07/09/response-to-jason-mitchell/" rel="bookmark" title="Permanent Link to &quot;Response to Jason Mitchell’s  “On the Emptiness of Failed Replications”&quot;">Response to Jason Mitchell’s  “On the Emptiness of Failed Replications”</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/sean-mackinnon.html" rel="author">Sean Mackinnon</a>

   			   <p>Jason Mitchell recently wrote an article entitled <a href="http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm">“On the Emptiness of Failed Replications.”</a>. In this article, Dr. Mitchell takes an unconventional and extremely strong stance against replication, arguing that: “… studies that produce null results -- including preregistered studies -- should not be published.”  The crux of the argument seems to be that "scientists who get p &gt; .05 are just incompetent." It completely ignores the possibility that a positive result could also (maybe even equally) be due to experimenter error. Dr. Mitchell also appears to ignore the possibility of simply getting a false positive (which is expected to happen under the null in 5% of cases).  </p>
<p>More importantly, it ignores issues of effect size and treats the outcome of research as a dichotomous "success or fail.” The advantages of examining effect sizes over simple directional hypotheses using null hypothesis significance testing are beyond the scope of this short post, but you might check out <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3444174/">Sullivan and Feinn (2012)</a> as an open-access starting point. Generally speaking, the problem is that sampling variation means that some experiments will find null results even when the experimenter does everything right. As an illustration, below is 1000 simulated correlations, assuming that r = .30 in the population, and a sample size of 100 (I used a <a href="http://www.quantpsy.org/rci/rci.htm">monte carlo method</a>).</p>
<p><img src="/images/response1.png" alt="" align="center" style="padding-right: 20px;" /> </p>
<p>In this picture, the units of analysis are individual correlations obtained in 1 of 1000 hypothetical research studies. The x-axis is the value of the correlation coefficient found, and the y-axis is the number of studies reporting that value. The red line is the critical value for significant results at p &lt; .05 assuming a sample size of 100. As you can see from this picture, the majority of studies are supportive of an effect that is greater than zero. However (simply due to chance) all the studies to the left of the red line turned out non-significant. If we suppressed all the null results (i.e., all those unlucky scientists to the left of the red line) as Dr. Mitchell suggests, then our estimate of the effect size in the population would be inaccurate; specifically, it would appear to be larger than it really is, because certain aspects of random variation (i.e., null results) are being selectively suppressed. Without the minority of null findings (in addition to the majority of positive findings) the overall estimate of the effect cannot be correctly estimated. </p>
<p>The situation is even more grim if there really is no effect in the population. </p>
<p><img src="/images/response2.png" alt="" align="center" style="padding-right: 20px;" /> </p>
<p>In this case, a small proportion of studies will produce false positives, with a roughly equal chance of an effect in either direction. If we fail to report null results, false positives may be reified as substantive effects. The reversal of signs across repeated studies might be a warning sign that the effect doesn’t really exist, but without replication, a single false positive could define a field if it happens (by chance) to be in line with prior theory.</p>
<p>With this in mind, I also disagree that replications are “publicly impugning the scientific integrity of their colleagues.” Some people feel threatened or attacked by replication. The ideas we produce as scientists are close to our hearts, and we tend to get defensive when they’re challenged. If we focus on effect sizes, rather than the “success or fail” logic of null hypothesis significance testing, then I don’t believe that “failed” replications damage the integrity of the original author, but rather simply suggests that we should modulate the estimate of the effect size downwards. In this framework, replication is less about “proving someone wrong” and more about centering on the magnitude of an effect size. </p>
<p>Something that is often missed in discussion of replication is that the very nature of randomness inherent in the statistical procedures scientists use means that any individual study (even if perfectly conducted) will probably generate an effect size that is a bit larger or smaller than it is in the population. It is only through repeated experiments that we are able to center on an accurate estimate of the effect size.  This issue is independent of researcher competence, and means that even the most competent researchers will come to the wrong conclusions occasionally because of the statistical procedures and cutoffs we’ve chosen to rely on. With this in mind, people should be aware that a failed replication does not necessarily mean that one of the two researchers is incorrect or incompetent – instead, it is assumed (until further evidence is collected) that the best estimate is a weighted average of the effect size from each research study.</p>
<p>For some more commentary from other bloggers, you might check out the following links:</p>
<p><a href="http://tomsett.me.uk/are-replication-efforts-pointless/">"Are replication efforts pointless?"</a> by <a href="http://tomsett.me.uk/about/">Richard Tomsett</a></p>
<p><a href="http://scientopia.org/blogs/drugmonkey/2014/07/07/being-as-wrong-as-can-be-on-the-so-called-replication-crisis-of-science/">"Being as wrong as can be on the so-called replication crisis of science"</a> by drugmonkey at <a href="http://scientopia.org/blogs/">Scientopia</a></p>
<p><a href="http://janneinosaka.blogspot.jp/2014/07/are-replication-efforts-useless.html">"Are replication efforts useless?"</a> by <a href="http://janneinosaka.blogspot.jp/">Jan Moren</a></p>
<p><a href="http://filedrawer.wordpress.com/2014/07/07/jason-mitchells-essay/">"Jason Mitchell’s essay"</a> by <a href="http://filedrawer.wordpress.com/">Chris Said</a></p>
<p><a href="http://neuroconscience.com/2014/07/08/methodswedontreport-brief-thought-on-jason-mitchell-versus-the-replicators/">"#MethodsWeDontReport – brief thought on Jason Mitchell versus the replicators"</a> by <a href="http://neuroconscience.com/micah-allen/">Micah Allen</a></p>
<p><a href="http://blogs.discovermagazine.com/neuroskeptic/2014/07/07/emptiness-failed-replications/#.U7wKgBYyxgp">"On 'On the emptiness of failed replications'"</a> by <a href="http://discovermagazine.com/authors?name=Neuroskeptic">Neuroskeptic</a></p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/07/09/response-to-jason-mitchell/#disqus_thread" data-disqus-identifier="2014/07/09/response-to-jason-mitchell/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/07/09/response-to-jason-mitchell/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jul  2,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/07/02/phack/" rel="bookmark" title="Permanent Link to &quot;phack - An R Function for Examining the Effects of P-hacking&quot;">phack - An R Function for Examining the Effects of P-hacking</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/ryne-sherman.html" rel="author">Ryne Sherman</a>

   			   <p><em>This article was <a href="http://rynesherman.com/blog/phack-an-r-function-for-examining-the-effects-of-p-hacking/">originally posted</a> in the author's personal blog.</em></p>
<p>Imagine you have a two group between-S study with N=30 in each group. You compute a two-sample t-test and the result is p = .09, not statistically significant with an effect size r = .17. Unbeknownst to you there is really no relationship between the IV and the DV. But, because you believe there is a relationship (you decided to run the study after all!), you think maybe adding five more subjects to each condition will help clarify things. So now you have N=35 in each group and you compute your t-test again. Now p = .04 with r = .21.</p>
<p>If you are reading this blog you might recognize what happened here as an instance of p-hacking. This particular form (testing periodically as you increase N) of p-hacking was one of the many data analytic flexibility issues exposed by <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704">Simmons, Nelson, and Simonshon (2011)</a>. But what are the real consequences of p-hacking? How often will p-hacking turn a null result into a positive result? What is the impact of p-hacking on effect size?</p>
<p>These were the kinds of questions that I had. So I wrote a little R function that simulates this type of p-hacking. The function – called phack – is designed to be flexible, although right now it only works for two-group between-S designs. The user is allowed to input and manipulate the following factors (argument name in parentheses):</p>
<ul>
<li>Initial Sample Size (initialN): The initial sample size (for each group) one had in mind when beginning the study (default = 30).</li>
<li>Hack Rate (hackrate): The number of subjects to add to each group if the p-value is not statistically significant before testing again (default = 5).</li>
<li>Population Means (grp1M, grp2M): The population means (Mu) for each group (default 0 for both).</li>
<li>Population SDs (grp1SD, grp2SD): The population standard deviations (Sigmas) for each group (default = 1 for both).</li>
<li>Maximum Sample Size (maxN): You weren’t really going to run the study forever right? This is the sample size (for each group) at which you will give up the endeavor and go run another study (default = 200).</li>
<li>Type I Error Rate (alpha): The value (or lower) at which you will declare a result statistically significant (default = .05).</li>
<li>Hypothesis Direction (alternative): Did your study have a directional hypothesis? Two-group studies often do (i.e., this group will have a higher mean than that group). You can choose from “greater” (Group 1 mean is higher), “less” (Group 2 mean is higher), or “two.sided” (any difference at all will work for me, thank you very much!). The default is “greater.”</li>
<li>Display p-curve graph (graph)?: The function will output a figure displaying the p-curve for the results based on the initial study and the results for just those studies that (eventually) reached statistical significance (default = TRUE). More on this below.</li>
<li>How many simulations do you want (sims). The number of times you want to simulate your p-hacking experiment.</li>
</ul>
<p>To make this concrete, consider the following R code:</p>
<div class="highlight"><pre>res <span class="o">&lt;-</span> phack<span class="p">(</span>initialN<span class="o">=</span><span class="m">30</span><span class="p">,</span> hackrate<span class="o">=</span><span class="m">5</span><span class="p">,</span> grp1M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp2M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp1SD<span class="o">=</span><span class="m">1</span><span class="p">,</span> 
  grp2SD<span class="o">=</span><span class="m">1</span><span class="p">,</span> maxN<span class="o">=</span><span class="m">200</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">.05</span><span class="p">,</span> alternative<span class="o">=</span><span class="s">&quot;greater&quot;</span><span class="p">,</span> graph<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span> sims<span class="o">=</span><span class="m">1000</span><span class="p">)</span>
</pre></div>


<p>This says you have planned a two-group study with N=30 (initialN=30) in each group. You are going to compute your t-test on that initial sample. If that is not statistically significant you are going to add 5 more (hackrate=5) to each group and repeat that process until it is statistically significant or you reach 200 subjects in each group (maxN=200). You have set the population Ms to both be 0 (grp1M=0; grp2M=0) with SDs of 1 (grp1SD=1; grp2SD=1). You have set your nominal alpha level to .05 (alpha=.05), specified a direction hypothesis where group 1 should be higher than group 2 (alternative=“greater”), and asked for graphical output (graph=TRUE). Finally, you have requested to run this simulation 1000 times (sims=1000).</p>
<p>So what happens if we run this experiment?<sup>1</sup> So we can get the same thing, I have set the random seed in the code below.</p>
<div class="highlight"><pre>source<span class="p">(</span><span class="s">&quot;http://rynesherman.com/phack.r&quot;</span><span class="p">)</span> <span class="c1"># read in the p-hack function</span>
set.seed<span class="p">(</span><span class="m">3</span><span class="p">)</span>
res <span class="o">&lt;-</span> phack<span class="p">(</span>initialN<span class="o">=</span><span class="m">30</span><span class="p">,</span> hackrate<span class="o">=</span><span class="m">5</span><span class="p">,</span> grp1M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp2M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp1SD<span class="o">=</span><span class="m">1</span><span class="p">,</span> grp2SD<span class="o">=</span><span class="m">1</span><span class="p">,</span>
   maxN<span class="o">=</span><span class="m">200</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">.05</span><span class="p">,</span> alternative<span class="o">=</span><span class="s">&quot;greater&quot;</span><span class="p">,</span> graph<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span> sims<span class="o">=</span><span class="m">1000</span><span class="p">)</span>
</pre></div>


<p>The following output appears in R:</p>
<div class="highlight"><pre><span class="n">Proportion</span> <span class="n">of</span> <span class="n">Original</span> <span class="n">Samples</span> <span class="n">Statistically</span> <span class="n">Significant</span> <span class="o">=</span> <span class="mf">0.054</span>
<span class="n">Proportion</span> <span class="n">of</span> <span class="n">Samples</span> <span class="n">Statistically</span> <span class="n">Significant</span> <span class="n">After</span> <span class="n">Hacking</span> <span class="o">=</span> <span class="mf">0.196</span>
<span class="n">Probability</span> <span class="n">of</span> <span class="n">Stopping</span> <span class="n">Before</span> <span class="n">Reaching</span> <span class="n">Significance</span> <span class="o">=</span> <span class="mf">0.805</span>
<span class="n">Average</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">Hacks</span> <span class="n">Before</span> <span class="n">Significant</span><span class="o">/</span><span class="n">Stopping</span> <span class="o">=</span> <span class="mf">28.871</span>
<span class="n">Average</span> <span class="n">N</span> <span class="n">Added</span> <span class="n">Before</span> <span class="n">Significant</span><span class="o">/</span><span class="n">Stopping</span> <span class="o">=</span> <span class="mf">144.355</span>
<span class="n">Average</span> <span class="n">Total</span> <span class="n">N</span> <span class="mf">174.355</span>
<span class="n">Estimated</span> <span class="n">r</span> <span class="n">without</span> <span class="n">hacking</span> <span class="mi">0</span>
<span class="n">Estimated</span> <span class="n">r</span> <span class="n">with</span> <span class="n">hacking</span> <span class="mf">0.03</span>
<span class="n">Estimated</span> <span class="n">r</span> <span class="n">with</span> <span class="n">hacking</span> <span class="mf">0.19</span> <span class="p">(</span><span class="n">non</span><span class="o">-</span><span class="n">significant</span> <span class="n">results</span> <span class="n">not</span> <span class="n">included</span><span class="p">)</span>
</pre></div>


<p>The first line tells us how many (out of the 1000 simulations) of the originally planned (N=30 in each group) studies had a p-value that was .05 or less. Because there was no true effect (grp1M = grp2M) this at just about the nominal rate of .05. But what if we had used our p-hacking scheme (testing every 5 subjects per condition until significant or N=200)? That result is in the next line. It shows that just about 20% of the time we would have gotten a statistically significant result. So this type of hacking has inflated our Type I error rate from 5% to 20%. How often would we have given up (i.e., N=200) before reaching statistical significance? That is about 80% of the time. We also averaged 28.87 “hacks” before reaching significance/stopping, averaged having to add N=144 (per condition) before significance/stopping, and had an average total N of 174 (per condition) before significance/stopping.</p>
<p>What about effect sizes? Naturally the estimated effect size (r) was .00 if we just used our original N=30 in each group design. If we include the results of all 1000 completed simulations that effect size averages out to be r = .03. Most importantly, if we exclude those studies that never reached statistical significance, our average effect size r = .19.</p>
<p>This is pretty telling. But there is more. We also get this nice picture:</p>
<p><img src="/images/phack.png" alt="Phack" /></p>
<p>It shows the distribution of the p-values below .05 for the initial study (upper panel) and for those p-values below .05 for those reaching statistical significance. The p-curves (see <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2256237">Simonsohn, Nelson, &amp; Simmons, 2013</a>) are also drawn on. If there is really no effect, we should see a flat p-curve (as we do in the upper panel). And if there is no effect and p-hacking has occurred, we should see a p-curve that slopes up towards the critical value (as we do in the lower panel).</p>
<p>Finally, the function provides us with more detailed output that is summarized above. We can get a glimpse of this by running the following code:</p>
<div class="highlight"><pre><span class="n">head</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>


<p>This generates the following output:</p>
<div class="highlight"><pre><span class="n">Initial</span><span class="p">.</span><span class="n">p</span>  <span class="n">Hackcount</span>     <span class="n">Final</span><span class="p">.</span><span class="n">p</span>  <span class="n">NAdded</span>    <span class="n">Initial</span><span class="p">.</span><span class="n">r</span>       <span class="n">Final</span><span class="p">.</span><span class="n">r</span>
<span class="mf">0.86410908</span>         <span class="mi">34</span>  <span class="mf">0.45176972</span>     <span class="mi">170</span>  <span class="o">-</span><span class="mf">0.14422580</span>   <span class="mf">0.006078565</span>
<span class="mf">0.28870264</span>         <span class="mi">34</span>  <span class="mf">0.56397332</span>     <span class="mi">170</span>   <span class="mf">0.07339944</span>  <span class="o">-</span><span class="mf">0.008077691</span>
<span class="mf">0.69915219</span>         <span class="mi">27</span>  <span class="mf">0.04164525</span>     <span class="mi">135</span>  <span class="o">-</span><span class="mf">0.06878039</span>   <span class="mf">0.095492249</span>
<span class="mf">0.84974744</span>         <span class="mi">34</span>  <span class="mf">0.30702946</span>     <span class="mi">170</span>  <span class="o">-</span><span class="mf">0.13594941</span>   <span class="mf">0.025289555</span>
<span class="mf">0.28048754</span>         <span class="mi">34</span>  <span class="mf">0.87849707</span>     <span class="mi">170</span>   <span class="mf">0.07656582</span>  <span class="o">-</span><span class="mf">0.058508736</span>
<span class="mf">0.07712726</span>         <span class="mi">34</span>  <span class="mf">0.58909693</span>     <span class="mi">170</span>   <span class="mf">0.18669338</span>  <span class="o">-</span><span class="mf">0.011296131</span>
</pre></div>


<p>The object res contains the key results from each simulation including the p-value for the initial study (Initial.p), the number of times we had to hack (Hackcount), the p-value for the last study run (Final.p), the total N added to each condition (NAdded), the effect size r for the initial study (Initial.r), and the effect size r for the last study run (Final.r).</p>
<p>So what can we do with this? I see lots of possibilities and quite frankly I don’t have the time or energy to do them. Here are some quick ideas:</p>
<ul>
<li>What would happen if there were a true effect?</li>
<li>What would happen if there were a true (but small) effect?</li>
<li>What would happen if we checked for significance after each subject (hackrate=1)?</li>
<li>What would happen if the maxN were lower?</li>
<li>What would happen if the initial sample size was larger/smaller?</li>
<li>What happens if we set the alpha = .10?</li>
<li>What happens if we try various combinations of these things?</li>
</ul>
<p>I’ll admit I have tried out a few of these ideas myself, but I haven’t really done anything systematic. I just thought other people might find this function interesting and fun to play with.</p>
<p><sup>1</sup> By the way, all of these arguments are set to their default, so you can do the same thing by simply running:</p>
<div class="highlight"><pre>res <span class="o">&lt;-</span> phack<span class="p">()</span>
</pre></div>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/07/02/phack/#disqus_thread" data-disqus-identifier="2014/07/02/phack/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/07/02/phack/">Posted at  3:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jun 25,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/25/a-skeptics-review/" rel="bookmark" title="Permanent Link to &quot;Bem is Back: A Skeptic's Review of a Meta-Analysis on Psi&quot;">Bem is Back: A Skeptic's Review of a Meta-Analysis on Psi</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/ej-wagenmakers.html" rel="author">EJ Wagenmakers</a>

		           	

<p>James Randi, magician and scientific skeptic, has compared those who believe in the paranormal to “unsinkable rubber ducks”: after a particular claim has been thoroughly debunked, the ducks submerge, only to resurface again a little later to put forward similar claims.  </p>
<p>In light of this analogy, it comes as no surprise that Bem and colleagues have produced a new paper claiming that people can look into the future. The paper is titled <a href="http://ssrn.com/abstract=2423692">“Feeling the Future: A Meta-Analysis of 90 Experiments on the Anomalous Anticipation of Random Future Events”</a> and it is authored by Bem, Tressoldi, Rabeyron, and Duggan.</p>
<p>Several of my colleagues have browsed Bem's meta-analysis and have asked for my opinion. Surely, they say, the statistical evidence is overwhelming, regardless of whether you compute a p-value or a Bayes factor. Have you not changed your opinion? This is a legitimate question, one which I will try and answer below by showing you my review of an earlier version of the Bem et al. manuscript.  </p>
<p>I agree with the proponents of precognition on one crucial point: their work is important and should not be ignored. In my opinion, the work on precognition shows in dramatic fashion that our current methods for quantifying empirical knowledge are insufficiently strict. If Bem and colleagues can use a meta-analysis to demonstrate the presence of precognition, what should we conclude from a meta-analysis on other, more plausible phenomena?  </p>
<p>Disclaimer: the authors have revised their manuscript since I reviewed it, and they are likely to revise their manuscript again in the future. However, my main worries call into question the validity of the enterprise as a whole.  </p>
<p>To keep this blog post self-contained, I have added annotations in italics to provide context for those who have not read the Bem et al. manuscript in detail. </p>
<p><strong>My review of Bem, Tressoldi, Rabeyron, and Duggan</strong></p>
 <a href="http://centerforopenscience.github.io/osc/2014/06/25/a-skeptics-review/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/25/a-skeptics-review/#disqus_thread" data-disqus-identifier="2014/06/25/a-skeptics-review/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/25/a-skeptics-review/">Posted at  1:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jun 18,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/" rel="bookmark" title="Permanent Link to &quot;Open Science Initiatives promote Diversity, Social Justice, and Sustainability&quot;">Open Science Initiatives promote Diversity, Social Justice, and Sustainability</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jon-grahe.html" rel="author">Jon Grahe</a>

		           	

<p>As I follow the recent social media ruckus centered on replication science questioning motives and methods, it becomes clear that the open science discussion needs to consider the point made by the title of this blog; maybe repeatedly. For readers who weren’t following this, <a href="https://politicalsciencereplication.wordpress.com/2014/05/25/replication-bullying-who-replicates-the-replicators/">this blog</a> by a political scientist and another post from the <a href="http://www.spspblog.org/psychology-news-round-up-may-23rd/">SPSP Blog</a> might be of interest. I invite you to join me in evaluating this argument as the discussion progresses. I contend that “Open Science Initiatives promote Diversity, Social Justice, and Sustainability.” Replication science and registered reports are two Open Science Initiatives and by extension should also promote these ideals. If this is not true, I will abandon this revolution and go back to the status quo. However, I am confident that when considering all the evidence, you will agree with me that these idealistic principles benefit from openness generally and by open science specifically.  </p>
<p>Before suggesting specific mechanisms by which this occurs, I will briefly note that the definitions of <a href="http://en.wikipedia.org/wiki/Open_science">Open Science</a>, <a href="http://en.wikipedia.org/wiki/Diversity">Diversity</a>, <a href="http://en.wikipedia.org/wiki/Social_justice">Social Justice</a>, and <a href="http://en.wikipedia.org/wiki/Sustainability">Sustainability</a> that are listed on Wikipedia are sufficient for this discussion since Wikipedia itself is an Open Science initiative. Also, I would like to convey the challenge of advancing each of these simultaneously. My own institution, Pacific Lutheran University (PLU), in our recent long range plan, <a href="http://issuu.com/pacific.lutheran.university/docs/plu-2020?e=1067239/2651397">PLU2020</a>, highlighted the importance of uplifting each of these at our own institution as introduced on page 11, “As we discern our commitments for the future, we reaffirm the ongoing commitments to diversity, sustainability, and justice that already shape our contemporary identity, and we resolve to integrate these values ever more intentionally into our mission and institution.” This is easier said than done because at times the goals of these ideals sometimes conflict. For instance, the environmental costs of feeding billions of people and heating their homes are enormous. Sometimes valuing diversity (such as scholarships targeted for people of color) seems unjust because resources are being assigned unevenly. These tensions can be described with many examples across numerous goals in all three dimensions and highlight the need to make balanced decisions.  </p>
<p>PLU has not yet resolved this challenge in uplifting all three simultaneously, but I hope that we succeed as we continue the vigorous discussion. Why each is important should be considered from is a Venn diagram on the sustainability Wikipedia page showing <a href="http://en.wikipedia.org/wiki/File:Sustainable_development.svg">sustainable development</a> as intersections between three pillars of sustainable development, social (people), economic, and environmental because even sustainability itself represents competing interests. Diversity and Social Justice are both core aspects of the social dimension, where uplifting diversity highlights the importance of distinct ideas and cultures and helps us understand why people and their varied ideas, in addition to oceans and forests are important resources of our planet. The ideals of social justice aim to provide mechanisms such that all members of our diverse population receive and contribute our fair share of these resources. Because resources are limited and society complex and flawed, these ideals are often more aspirational rather than practical. However, the basic premise of uplifting all three is that we are better when valuing diversity, providing social justice, and sustainably using the planet’s resources (people, animals, plants, and rocks). Below I provide examples for how OSIs promote each of these principles while illustrating why each is important to science.   </p>
 <a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/#disqus_thread" data-disqus-identifier="2014/06/18/osi-promote-dsjs/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/18/osi-promote-dsjs/">Posted at 12:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jun 11,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/11/thoughts-on-this-debate/" rel="bookmark" title="Permanent Link to &quot;Thoughts on this debate about social scientific rigor&quot;">Thoughts on this debate about social scientific rigor</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/betsy-levy-paluck.html" rel="author">Betsy Levy Paluck</a>

   			   <p><em>This article was originally posted on Betsy Levy Paluck's <a href="http://www.betsylevypaluck.com/blog/2014/5/25/what-i-stand-for-in-this-discussion-about-scientific-rigor">website</a>.</em></p>
<p>On his terrific <a href="http://hardsci.wordpress.com/">blog</a>, Professor Sanjay Srivastava <a href="http://hardsci.wordpress.com/2014/05/25/does-the-replication-debate-have-a-diversity-problem/">points out</a> that the current (vitriolic) debate about replication in psychology has been "salted with casually sexist language, and historically illiterate" arguments, on both sides. I agree, and thank him for pointing this out.  </p>
<p>I'd like to add that I believe academics participating in this debate should be mindful of co-opting powerful terms like <em>bullying</em> and <em>police</em> (e.g., the "replication police") to describe the replication movement. Why? Bullying behavior describes repeated abuse from a person of higher power and influence. Likewise, many people in the US and throughout the world have a well-grounded terror of police abuse. The terror and power inequality that these terms connote is diminished when we use it to describe the experience of academics replicating one another's studies. Let's keep evocative language in reserve so that we can use it to name and change the experience of truly powerless and oppressed people.   </p>
<p><strong>Back to replication.</strong> Here is the thing: we all believe in the principle of replication. As scientists and as psychologists, we are all here because we wish to contribute to cumulative research that makes progress on important psychological questions. This desire unites us.</p>
<p>So what's up?  </p>
<p>It seems to me that some people oppose the current wave of replication efforts because they do not like the tenor of the recent public discussions. As I already mentioned, neither do I. I'm bewildered by the vitriol. Just a few days ago, one of the most prominent modern economists, currently an internationally bestselling author, had his <a href="http://www.ft.com/cms/s/2/e1f343ca-e281-11e3-89fd-00144feabdc0.html#axzz32mFmDac2">book called into question</a> over alleged data errors in a spreadsheet that he made public. His <a href="http://blogs.ft.com/money-supply/2014/05/23/piketty-response-to-ft-data-concerns/?Authorised=false">response</a> was cordial and curious; his colleagues followed up with <a href="http://www.nytimes.com/2014/05/25/upshot/a-new-critique-of-piketty-has-its-own-shortcomings.html?_r=0">care</a>, <a href="http://www.nytimes.com/2014/05/24/upshot/did-piketty-get-his-math-wrong.html?smid=tw-upshotnyt">equanimity</a>, and respect.   </p>
<p>Are we really being taught a lesson in manners from economists? Is that happening?  </p>
<p>As one of my favorite TV characters said recently ...  </p>
<p>If we don't like the tenor of the discussion about replication, registration, etc., let's change it.  </p>
<p>In this spirit, I offer a brief description of what we are doing in my lab to try to make our social science rigorous, transparent, and replicable. It's one model for your consideration, and we are open to suggestions.  </p>
<p>For the past few years we have <strong>registered analysis plans</strong> for every new project we start. (They can be found <a href="http://e-gap.org/design-registration/registered-designs/">here</a> on the <a href="http://e-gap.org/">EGAP</a> website; this is a group to which I belong. EGAP has had great discussions in partnership with <a href="http://cega.berkeley.edu/programs/BITSS/">BITSS</a> about transparency.) My lab's analysis registrations are accompanied by a <strong>codebook</strong> describing each variable in the dataset.  </p>
<p>I am happy to say that we are just starting to get better at producing <strong>replication code</strong> and <strong>data &amp; file organization that is sharing-ready</strong> as we do the research, rather than trying to reconstruct these things from our messy code files and Dropbox disaster areas following publication (for this, I thank my <a href="http://www.betsylevypaluck.com/collaborators/">brilliant students</a>, who surpass me with their coding skills and help me to keep things organized and in place. See also <a href="http://faculty.chicagobooth.edu/matthew.gentzkow/research/CodeAndData.pdf">this</a>). What a privilege and a learning experience to have graduate students, right? Note that <a href="http://personxsituation.wordpress.com/2014/05/25/im-disappointed-a-graduate-students-perspective/">they are listening to us</a> have this debate.  </p>
<p>Margaret Tankard, Rebecca Littman, Graeme Blair, Sherry Wu, Joan Ricart-Huguet, Andreana Kenrick (awesome grad students), and Robin Gomila and David Mackenzie (awesome lab managers) have all been writing analysis registrations, organizing files, checking data codebooks, and writing replication code for the experiments we've done in the past three years, and colleagues Hana Shepherd, Peter Aronow, Debbie Prentice, and Eldar Shafir are doing the same with me. Thank goodness for all these amazing and dedicated collaborators, because one reason I understand replication to be so difficult is that it is a huge challenge to reconstruct what you thought and did over a long period of time, without careful record keeping (note: analysis registration also serves that purpose for us!).  </p>
<p>Previously, I <strong>posted data</strong> at Yale's ISPS archive, and for other datasets made them available on request if I thought I was going to work more on them. But in future we plan to post all published data plus the dataset's codebook. Economist and political scientists friends often post to their personal websites. Another possibility is posting in digital archives (like Yale's, but there are others: I follow @<a href="https://twitter.com/annthegreen">annthegreen</a> for updates on digital archiving).  </p>
<p>I owe so much of my appreciation for these practices to my advisor <a href="https://sites.google.com/site/donaldpgreen/">Donald Green</a>. I've also learned a lot from <a href="http://www.columbia.edu/~mh2245/">Macartan Humphreys</a>.  </p>
<p>I'm interested in how we can be better. I'm listening to the constructive debates and to the suggestions out there. If anyone has questions about our current process, please leave a comment below! I'd be happy to answer questions, provide examples, and to take suggestions. </p>
<p>It costs nothing to do this--but it slows us down. Slowing down is not a bad thing for research (though I recognize that a bad heuristic of quantity = quality still dominates our discipline). During registration, we can stop to think-- <em>are we sure we want to predict this? With this kind of measurement? Should we go back to the drawing board about this particular secondary prediction?</em> I know that if I personally slow down, I can oversee everything more carefully. I'm learning how to say no to new and shiny projects. </p>
<p>I want to end on the following note. I am now tenured. If good health continues, I'll be on hiring committees for years to come. In a hiring capacity, I will appreciate applicants who, though they do not have a ton of publications, can link their projects to an online analysis registration, or have posted data and replication code. Why? I will infer that they were slowing down to do very careful work, that they are doing their best to build a cumulative science. I will also appreciate candidates who have conducted studies that "failed to replicate" and who responded to those replication results with follow up work and with thoughtful engagement and curiosity (I have read about <a href="http://www.talyarkoni.org/blog/2013/12/27/what-we-can-and-cant-learn-from-the-many-labs-replication-project/">Eugene Caruso's response</a> and thought that he is a great model of this kind of response).</p>
<p>I say this because it's true, and also because some academics report that their graduate students are very nervous about how replication of their lab's studies might <a href="http://www.spspblog.org/simone-schnall-on-her-experience-with-a-registered-replication-project/">ruin their reputations on the job market</a> (see Question 13). I think the concern is understandable, so it's important for those of us in these lucky positions to speak out about what we value and to allay fears of punishment over non-replication (see Funder: <a href="http://funderstorms.wordpress.com/2012/10/31/the-perilous-plight-of-the-non-replicator/">SERIOUSLY NOT OK</a>). </p>
<p>In sum, I am excited by efforts to improve the transparency and cumulative power of our social science. I'll try them myself and support newer academics who engage in these practices. Of course, we need to have good ideas as well as good research practices (ugh--this business is not easy. Tell that to your friends who think that you've chosen grad school as a shelter from the bad job market). </p>
<p>I encourage all of my colleagues, and especially colleagues from diverse positions in academia and from underrepresented groups in science, to comment on what they are doing in their own research and how they are affected by these ideas and practices. Feel free to post below, post on (real) blogs, write letters to the editor, have conversations in your lab and department, or <a href="https://twitter.com/betsylevyp">tweet</a>. I am listening. Thanks for reading.</p>
<p>*</p>
<p><em>A collection of comments I've been reading about the replication debate, in case you haven't been keeping up. Please do post more links below, since this isn't comprehensive.</em></p>
<p><a href="http://personxsituation.wordpress.com/2014/05/25/im-disappointed-a-graduate-students-perspective/">I'm disappointed: a graduate student's perspective</a></p>
<p><a href="http://hardsci.wordpress.com/2014/05/25/does-the-replication-debate-have-a-diversity-problem/">Does the replication debate have a diversity problem?</a></p>
<p><a href="https://osf.io/98tkv/">Replications of Important Results in Social Psychology: Special Issue of Social Psychology</a></p>
<p><a href="http://funderstorms.wordpress.com/2012/10/31/the-perilous-plight-of-the-non-replicator/">The perilous plight of the (non)-replicator</a></p>
<p><a href="http://politicalsciencereplication.wordpress.com/2014/05/25/replication-bullying-who-replicates-the-replicators/">"Replication Bullying": Who replicates the replicators?</a></p>
<p><a href="http://davidjjohnson.wordpress.com/2014/05/25/rejoinder-to-schnall-2014/">Rejoinder to Schnall (2014) in Social Psychology</a></p>
<p><a href="https://docs.google.com/document/d/1ew7X0RaClU5_Ev4Ns3Uyn0I7PmjzP_Z1wKlnza_3Fe0/edit">Context and Correspondence for Special Issue of Social Psychology</a></p>
<p><a href="http://osc.centerforopenscience.org/2014/03/26/behavioral-priming/">Behavioral Priming: Time to Nut Up or Shut Up</a></p>
<p><strong>Tweets:</strong></p>
<ul>
<li><a href="https://twitter.com/DanTGilbert/status/470436673697095680">@DanTGilbert: "Simone Schnall's expose of the replication police..."</a></li>
<li><a href="https://twitter.com/BrianNosek/status/470563826274807808">@BrianNosek: "A remarkable set of public comments..."</a></li>
<li><a href="https://twitter.com/DavidFunder/status/470316176627613696">@DavidFunder: "3 terms I learned in past 24 hrs..."</a></li>
</ul>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/11/thoughts-on-this-debate/#disqus_thread" data-disqus-identifier="2014/06/11/thoughts-on-this-debate/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/11/thoughts-on-this-debate/">Posted at  4:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Jun  5,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/06/05/op-open-humans/" rel="bookmark" title="Permanent Link to &quot;Open Projects - Open Humans&quot;">Open Projects - Open Humans</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This article is the second in <a href="http://osc.centerforopenscience.org/tag/open-projects.html">a series highlighting open science projects</a> around the community. You can read the interview this article was based on: <a href="https://docs.google.com/document/d/1tQuOFme5EQbNkcGBCc1rr7FO9SWEEufPwhhJ59JXBUM/edit">edited for clarity</a>, <a href="https://docs.google.com/document/d/1c2xMBMEr5m8a3wR7Om5mPAUgsz_JBPudlYsnSEbH0lY/edit">unedited</a>.</em>  </p>
<p>While many researchers encounter no privacy-based barriers to releasing data, those working with human participants, such as doctors, psychologists, and geneticists, have a difficult problem to surmount. How do they reconcile their desire to share data, allowing their analyses and conclusions to be verified, with the need to protect participant privacy? It's a dilemma we've talked about before on the blog (see: <a href="http://osc.centerforopenscience.org/2014/02/05/open-data-and-IRBs/">Open Data and IRBs</a>, <a href="http://osc.centerforopenscience.org/2014/01/29/privacy-and-open-data/">Privacy and Open Data</a>). A new project, Open Humans, seeks to resolve the issue by finding patients who are willing - even eager - to share their personal data.  </p>
<p>Open Humans, which recently won a $500,000 grant <a href="http://blog.personalgenomes.org/2014/01/14/open-humans-network-wins-knight-news-challenge-health-award/">from the Knight Foundation</a>, grew out of the <a href="http://www.personalgenomes.org/">Personal Genome Project</a>. Founded in 2005 by Harvard genetics professor <a href="http://arep.med.harvard.edu/gmc/">George Church</a>, the Personal Genome Project sought to solve a problem that many genetics researchers had yet to recognize. "At the time people didn't really see genomes as inherently identifiable," Madeleine Price Ball explains. Ball is co-founder of OpenHumans, Senior Research Scientist at PersonalGenomes.org, and Director of Research at the Harvard Personal Genome Project.  She quotes from <a href="http://www.1000genomes.org/">1000 Genomes</a>' <a href="http://www.1000genomes.org/sites/1000genomes.org/files/docs/Informed%20Consent%20Form%20Template.pdf">informed consent form</a>: "'Because of these measures, it will be very hard for anyone who looks at any of the scientific databases to know which information came from you, or even that any information in the scientific databases came from you.'"  </p>
<p>"So that's sort of the attitude scientists had towards genomes at the time. Also, the Genetic Information Nondiscrimination Act didn't exist yet. And there was GATTACA. Privacy was still this thing everyone thought they could have, and genomes were this thing people thought would be crazy to share in an identifiable manner. I think the scientific community had a bit of unconscious blindness, because they couldn't imagine an alternative."  </p>
<p>Church found an initial <a href="https://en.wikipedia.org/wiki/Personal_Genome_Project">ten participants</a> - the list includes university professors, health care professionals, and Church himself. The IRB interviewed each of the participants to make sure they truly understood the project and, satisfied, allowed it to move forward. The Personal Genome Project now boasts <a href="https://my.pgp-hms.org/users/">over 3,400 participants</a>, each of whom have passed an entrance exam showing that they understand what will happen to their data, and the risks involved. Most participants are enthusiastic about sharing. One participant described it as "donating my body to science, but I don't have to die first".   </p>
<p>The Personal Genome Project's expansion hasn't been without growing pains. "We've started to try to collect data beyond genomes." Personal health information, including medical history, procedures, test results, prescriptions, has been provided by a subset of participants. "Every time one of these new studies was brought before the IRB they'd be like ‘what? that too?? I don't understand what are you doing???' It wasn't scaling, it was confusing, the PGP was trying to collect samples and sequence genomes <em>and</em> it was trying to let other groups collect samples and do other things."  </p>
<p>Thus, Open Humans was born. "Open Humans is an abstraction that takes part of what the PGP was doing (the second part) and make it scalable," Ball explains. "It's a cohort of participants that demonstrate an interest in public data sharing, and it's researchers that promise to return data to participants."  </p>
<p>Open Humans will start out with a number of participants and an array of public data sets, thanks to collaborating projects <a href="http://humanfoodproject.com/americangut/">American Gut</a>, <a href="https://flunearyou.org/">Flu Near You</a>, and of course, the Harvard Personal Genome Project. Participants share data and, in return, researchers promise to share results. What precisely "sharing results" means has yet to be determined. "We're just starting out and know that figuring out how this will work is a learning process," Ball explains. But she's already seen what can happen when participants are brought into the research process - and brought together:  </p>
<p>"One of the participants made an online forum, another a Facebook group, and another maintains a LinkedIn group… before this happened it hadn't occurred to me that abandoning the privacy-assurance model of research could empower participants in this manner. Think about the typical study - each participant is isolated, they never see each other. Meeting each other could breach confidentiality! Here they can talk to each other and <em>gasp</em> complain about you. That's pretty empowering." Ball and her colleague Jason Bobe, Open Humans co-founder and Executive Director of PersonalGenomes.org, hope to see all sorts of collaborations between participants and researchers. Participants could help researchers refine and test protocols, catch errors, and even provide their own analyses.  </p>
<p>Despite these dreams, Ball is keeping the project grounded. When asked whether Open Humans will require articles published using their datasets to be made open access, she replies that, "stacking up a bunch of ethical mandates can sometimes do more harm than good if it limits adoption". Asked about the effect of participant withdrawals on datasets and reproducibility, she responds, "I don't want to overthink it and implement things to protect researchers at the expense of participant autonomy based on just speculation." (It <em>is</em> mostly speculation. Less than 1% of Personal Genome Project users have withdrawn from the study, and none of the participants who've provided whole genome or exome data have done so.)  </p>
<p>It's clear that Open Humans is focused on the road directly ahead. And what does that road look like?  "Immediately, my biggest concern is building our staff. Now that we won funding, we need to hire a good programmer... so if you are or know someone that seems like a perfect fit for us, please pass along <a href="http://openhumans.org/#now_hiring">our hiring opportunities</a>". She adds that anyone can <a href="http://openhumans.org/">join the project's mailing list</a> to get updates and find out when Open Humans is open to new participants - and new researchers. "And just talk about us. Referring to us is an intangible but important aspect for helping promote awareness of participant-mediated data sharing as a participatory research method and as a method for creating open data."  </p>
<p>In other words: start spreading the news.  Participant mediated data isn't the only solution to privacy issues, but it's an enticing one - and the more people who embrace it, the better a solution it will be.  </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/06/05/op-open-humans/#disqus_thread" data-disqus-identifier="2014/06/05/op-open-humans/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/06/05/op-open-humans/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/open-projects.html">open-projects</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 29,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/" rel="bookmark" title="Permanent Link to &quot;Questions and Answers about the Förster case&quot;">Questions and Answers about the Förster case</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/denny-borsboom-han-van-der-maas-eric-jan-wagenmakers-department-of-psychological-methods-uva.html" rel="author">Denny Borsboom, Han van der Maas, Eric-Jan Wagenmakers, Department of Psychological Methods, UvA</a>

		           	

<p>By now, everyone is probably familiar with the recent investigation of the work of Dr. Förster, in which the Landelijk Orgaan Wetenschappelijke Integriteit (LOWI) concluded that data reported in a paper by Dr. Förster had been manipulated. In his reaction to the newspaper article NRC Dr. Förster suggested that our department would be involved in a witch-hunt. This is incorrect.  </p>
<p>However, we have noticed that there are many questions about both the nature of the case and the procedure followed. We have compiled the following list of questions and answers to explain what happened. If any other questions arise, feel free to email them to us so we can add them to this document.  </p>
<p><strong>Q: What was the basis of the allegations against Dr. Förster?</strong><br />
A: In every single one of 40 experiments, reported across three papers, the means of two experimental conditions (“local focus” and “global focus”) showed almost exactly opposite behavior with respect to the control condition. So whenever the local focus condition led to a one-point increase of the mean level of the dependent variable compared to the control condition, the global condition led almost exactly to a one-point decrease. Thus, the samples exhibit an unrealistic level of linearity.  </p>
<p><strong>Q: Couldn’t the effects actually be linear in reality?</strong><br />
A: Yes, that is unlikely but possible. However, in addition to the perfect linearity of the effects themselves, there is far too little variance in the means of the conditions, given the variance that is present within the conditions. In other words: the means across the conditions follow the linear pattern (much) too perfectly. To show this, the whistleblower’s complaint computed the probability of finding this level of linearity (or even more perfect linearity) in the samples researched, under the assumption that, in reality, the effect is linear in the population. That probability equals 1/508,000,000,000,000,000,000.   </p>
 <a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/">Read more...</a>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/#disqus_thread" data-disqus-identifier="2014/05/29/forster-case/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/29/forster-case/">Posted at  3:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">May 28,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/05/28/train-wreck-prevention/" rel="bookmark" title="Permanent Link to &quot;The etiquette of train wreck prevention&quot;">The etiquette of train wreck prevention</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jp-de-ruiter-bielefeld-university.html" rel="author">JP de Ruiter, Bielefeld University</a>

   			   <p>In a famous <a href="http://www.nature.com/polopoly_fs/7.6716.1349271308!/suppinfoFile/Kahneman%20Letter.pdf">open letter</a> to scientists , Daniel Kahneman, seeing “a train wreck looming”, argued that social psychologists (and presumably, especially those who are publishing social priming effects) should engage in systematic and extensive replication studies to avoid a loss of credibility in the field. The fact that a Nobel Prize winning psychologist made such a clear statement gave a strong boost of support to systematic replication efforts in social psychology (see Pashler &amp; Wagenmakers 2012, and their special issue in <em>Psychological Science</em>).  </p>
<p>But in a more recent <a href="http://www.scribd.com/doc/225285909/Kahneman-Commentary">commentary</a>, Kahneman appears to have changed his mind, and argues that “current norms allow replicators too much freedom to define their study as a direct replication of previous research”, and that the “seemingly reasonable demand” of requiring method sections to be so precise that they enable direct replications is “rarely satisfied in psychology, because behavior is easily affected by seemingly irrelevant factors”. A similar argument was put forth by Simone Schnall, who recently <a href="http://www.psychol.cam.ac.uk/cece/blog">wrote</a> that “human nature is complex, and identical materials will not necessarily have the identical effect on all people in all contexts”.  </p>
<p>While I wholeheartedly agree with Kahneman’s original letter on this topic, I strongly disagree with his commentary, for reasons that I will outline here.  </p>
<p>First, he argues (as Schnall did too) that there always are potentially influential differences between the original study and the replication attempt. But this would imply that any replication study, no matter how meticulously performed, would be meaningless. (Note that this also holds for <em>successful</em> replication studies.) This is a clear case of a <em>reductio ad absurdum</em>.  </p>
<p>The main reason why this argument is flawed is that there is a fundamental relationship between the theoretical claim based on a finding and its proper replication, which is the topic of an interesting discussion about the degree to which a replication should be similar to the study it addresses (see Stroebe &amp; Strack, 2014; Simons, 2014; Pashler &amp; Harris, 2012). My position in this debate is the following. The more general the claim that the finding is claimed to support, the more “conceptual” the replication of the supporting findings can (and should) be. Suppose we have a finding F that we report in order to claim evidence for scientific claim C. In the case that C is <em>identical</em> to F, such that C is a claim of the type “The participants in our experiment did X at time T in location L”, it is indeed impossible to do any type of replication study, because the exact circumstances of F were unique and therefore by definition irreproducible. But in this case (that F = C), C has obviously no generality at all, and is therefore scientifically not very interesting. In such a case, there would also be no point in doing inferential statistics. If, on the other hand, C is more general than F, the level of methodological detail that is provided should be sufficient to enable readers to attempt to replicate the finding, allowing for variation that the authors do not consider important. If the authors remark that this result arises under condition A but acknowledge that it might not arise under condition A' (let's say, with participants who are aged 21-24 rather than 18-21), then clearly a follow-up experiment under condition A' isn't a valid replication. But if their claim (explicitly or implicitly) is that it doesn't matter whether condition A or A' is in effect, then a follow-up study involving condition A' might well be considered a replication. The failure to specify any particular detail might reasonably be considered an implicit claim that this detail is not important.  </p>
<p>Second, Kahnemann is worried that even the rumor of a failed replication could damage the reputation of the original authors. But if researchers attempt to do a replication study, this does not imply that they believe or suggest that the original author was cheating. Cheating does occasionally happen, sadly, and replication studies are a good way to catch these cases. But, assuming that cheating is not completely rampant, it is much more likely that a finding cannot be replicated successfully because variables or interactions have been overlooked or not controlled for, that there were unintentional errors in the data collection or analysis, or because the results were simply a fluke, caused by our standard statistical practices severely overestimating evidence against the null hypothesis (Sellke, Bayarri &amp; Berger, 2001; Johnson, 2013).  </p>
<p>Furthermore, replication studies are not hostile or friendly. People are. I think it is safe to say that we all dislike uncollegial behavior and rudeness, and we all agree that it should be avoided. If Kahneman wants to give us a stern reminder that it is important for replicators to contact the original authors, then I support that, even though I personally suspect that the vast majority of replicators already do that. There already is etiquette in place in experimental psychology, and as far as I can tell, it’s mostly being observed. And for those cases where it is not, my impression is that the occasional unpleasant behavior originates not only from replicators, but also from original authors.  </p>
<p>Another point I would like to address is the asymmetry of the relationship between author and replicator. Kahneman writes: “The relationship is also radically asymmetric: the replicator is in the offense, the author plays defense.” This may be true in some sense, but it is counteracted by other asymmetries that work in the opposite direction: The author has already successfully published the finding in question and is reaping the benefits of it. The replicator, however, is up against the strong reluctance of journals to publish replication studies, is required to have a much higher statistical power (hence invest far more resources), and is often arguing against a moving target, as more and more newly emerging and potentially relevant details of the original study can be brought forward by the original authors.  </p>
<p>A final point: the problem that started the present replication discussion was that a number of findings that were deemed both important and implausible by many researchers failed to replicate. The defensiveness of the original authors of these findings is understandable, but so is the desire of skeptics to investigate if these effects are in fact reliable. I, both as a scientist and as a human being, <em>really want to know</em> if I can boost my creativity by putting an open box on my desk (Leung et al., 2012) or if the fact that I frequently take hot showers could be caused by loneliness (Bargh &amp; Shalev, 2012). As Kahneman himself rightly put it in his original open letter: “The unusually high openness to scrutiny may be annoying and even offensive, but it is a small price to pay for the big prize of restored credibility.”  </p>
<p><strong>References</strong></p>
<p>Bargh, J. A., &amp; Shalev, I. (2012). The substitutability of physical and social warmth in daily life. <em>Emotion</em>, 12(1), 154. doi:10.1037/a0023527</p>
<p>Johnson, V. E. (2013). Revised standards for statistical evidence. <em>Proceedings of the National Academy of Sciences</em>, 110(48), 19313-19317. doi: doi/10.1073/pnas.1313476110</p>
<p>Leung, A. K.-y., Kim, S., Polman, E., Ong, L. S., Qiu, L., Goncalo, J. A., et al. (2012). Embodied metaphors and creative "acts". <em>Psychological Science</em>, 23(5), 502-509. doi:10.1177/0956797611429801</p>
<p>Pashler, H., &amp; Harris, C. R. (2012). Is the replicability crisis overblown? Three arguments examined. <em>Perspectives on Psychological Science</em>, 7(6), 531-536. doi:10.1177/1745691612463401</p>
<p>Pashler, H., &amp; Wagenmakers, E.-J. (2012). Editors' Introduction to the Special Section on Replicability in Psychological Science A Crisis of Confidence? <em>Perspectives on Psychological 
Science</em>, 7(6), 528-530. doi:10.1177/1745691612465253</p>
<p>Sellke, T., Bayarri, M., &amp; Berger, J. O. (2001). Calibration of p values for testing precise null hypotheses. <em>The American Statistician</em>, 55(1), 62-71. doi:10.1198/000313001300339950</p>
<p>Simons, D. J. (2014). The Value of Direct Replication. <em>Perspectives on Psychological Science</em>, 9(1), 76-80. doi:10.1177/1745691613514755</p>
<p>Stroebe, W., &amp; Strack, F. (2014). The alleged crisis and the illusion of exact replication. <em>Perspectives on Psychological Science</em>, 9(1), 59-71. doi:10.1177/1745691613514450 </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/05/28/train-wreck-prevention/#disqus_thread" data-disqus-identifier="2014/05/28/train-wreck-prevention/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/05/28/train-wreck-prevention/">Posted at  1:30 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/misc.html" rel="tag">misc</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://centerforopenscience.github.io/osc/category/misc2.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 1 of 5</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>