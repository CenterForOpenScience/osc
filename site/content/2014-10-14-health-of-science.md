Title: A Psi Test for the Health of Science
Author: Alex Holcombe
Date: 2014-10-14 11:00
Slug: health-of-science
Status: draft

**Science is sick. How will we know when it's been cured?**

Meta-analysis quantitatively combines the evidence from multiple experiments, usually from different papers and laboratories. Meta-analyses of published laboratory studies of psi (telepathy, psychokinesis, and other parapsychological phenomena) suggest that the evidence overwhelmingly supports the existence of psi. In a heroic effort, Bosch, Steinkamp, & Boller (2006, *Psychological Bulletin*) meta-analyzed 380 laboratory studies of the ability of participants to affect the output of random number generators. The experiments involved stemmed from an older tradition in which participants attempted to influence a throw of dice to yield a particular target number. As for the older dice experiments, many of the studies found that the number spat out by the random number generator was more often the target number that the participant was gunning for than one would expect by chance. Combining all 380 experiments, Bosch et al. calculated that if in fact psychokinesis does not exist, the probability of finding the evidence published was less than one in a thousand (*z* = 3.67 in one of their calculations). In other words, it is extremely unlikely that so much evidence in favor of psychokinesis would have resulted if psychokinesis actually does not exist.

Like many others, I suspect that this evidence stems not from the existence of psi, but rather from various biases in the way science today is typically conducted. Publication bias refers to the tendency for a study to be published if it is interesting, while boring results are relegated to the file drawer. "P-hacking" - equally insidious - is the tendency of scientists to try many different statistical analyses until they find a statistically significant result- but if you try enough analyses or tests, you're practically guaranteed to eventually find a statistically significant although spurious result.

These problems have received plenty of attention over the last several years, and many measures have been proposed to address them. One big step in the right direction is for researchers to publicly announce (simply by posting on a website) a single, specific statistical analysis plan prior to collecting the data. This can eliminate the p-hacking issue. Other positive steps, like sharing of code and data, helps other scientists to evaluate the evidence more appropriately than is possible for most published scientific papers. In the case of Tressoldi et al., independent researcher Sam Schwartzkopf has been able to [wade into the arcane details of the study](http://neuro.plos.org/2014/10/01/nothing-spooky-about-decoding-telepathy-a-lesson-in-the-value-of-open-science/), revealing possible problems.

This brings me to what I call the Psi Test of the Health of Science. Despite scientists' suspicion that the seemingly-overwhelming evidence for psi is a result of publication bias and undisclosed data analysis flexibility, there is no way to prove this, or to establish it beyond a reasonable doubt (we shouldn't expect *proof*, as that may be a higher standard than is feasible for empirical studies of a probabilistic phenomenon). Sure, analytical tricks like [funnel plots](http://en.wikipedia.org/wiki/Funnel_plot) can support an accusation of publication bias. But given that most areas of science are rife with publication bias, if we use publication bias to overturn the evidence for psi, to be consistent we'd end up disbelieving countless more-legitimate phenomena. And my reading of medicine’s standard [meta-analysis guide](http://handbook.cochrane.org/), by the Cochrane Collaboration, is that in Cochrane reviews publication bias raises concerns but is not used to overturn the verdict indicated by the evidence (let me know if I’m wrong).

Until it can be concluded that Psi does not exist, using the same meta-analysis standards as are applied to any other phenomenon in the biomedical or psychological literature, science seems to be falling short. Others have made [similar points](http://osc.centerforopenscience.org/2014/06/25/a-skeptics-review/).

OK, so instead of concluding that science is sick, we might instead conclude that psi actually exists. If psi does exist, we can of course forget about having any Psi Test of the Health of Science. But like most researchers, I believe that psi as described in studies of psychokinesis doesn’t exist - mainly because of what I hear from physicists. And I think if it did exist, there’d likely be even *more* overwhelming evidence for it by now than we have. But I want us to be able to dismiss psi using the same meta-analysis techniques we use for the run-of-the-mill. 

So when preregistration, open science, or something else has truly cured science, will we know it? The Psi Test won't tell us right away. But in retrospect we’ll know. After the year science is cured, when taking psi studies published that year and after, applying the standard meta-analysis technique will result in the conclusion that psi does not exist.

Below, I consider two basic objections to this Psi test for the Health of Science.

**Objection 1**: some say that we already can conclude that psi does not exist, based on Bayesian evaluation of the proposition. To evaluate the evidence from a psi study in a Bayesian way, one assigns a probability that psi exists, prior to knowing the data from the psi studies. Most physicists and neuroscientists would say that our knowledge of how the brain works and of physical law very strongly suggests that psychokinesis is impossible. To overturn this Bayesian prior, one would need much stronger evidence than even the one-in-a-thousand chance derived from psi studies that I mentioned above. I agree with this reasoning; it's one reason I don't believe in psi. However, it's pretty hard to pin down in quantitative fashion; this is one reason why Bayesian analysis hasn’t taken over the scientific literature generally. Also, there may be expert physicists out there that think some sort of quantum interaction could underlie psi.  

Rather than relying on a Bayesian argument (or using Bayesian analysis, but with a neutral prior), I'd prefer that our future scientific practice, involving preregistration, unbiased publishing, replication protocols, and so on reach the point where if hundreds of experiments on a topic are available, they should be fairly definitive. Do you think we will get there?

**Objection 2**: Some will say that science can never eliminate publication bias. While it is reduced by the advent of journals like PLoS ONE that accept null results, and by the growing number of journals that[ accept papers prior to the data being collected](https://osf.io/8mpji/wiki/home/), it may forever remain a significant problem. But there are further steps one could take: in [open notebook science](http://en.wikipedia.org/wiki/Open_notebook_science), all data is posted on the net as soon as it is collected, eliminating all opportunity for publication bias. But perhaps open notebook science will never become standard practice, and publication bias will remain strong enough that substantial doubt will persist for many scientific issues. In that case, the only solution may be a pre-registered, confirmatory large-scale study of an issue, similar to what [we are doing ](http://www.psychologicalscience.org/index.php/replication)at *Perspectives on Psychological Science*. Will science always need that to pass the psi test?

