Title: Chasing Paper, Part 2
Author: Shauna Gordon-McKeon
Date: 2013-12-11 12:00
Status: draft
Slug: chasing-paper-2

_This is part two of a three part post brainstorming potential improvements to the journal article format.  Part one is [here](http://osc.centerforopenscience.org/2013/12/11/chasing-paper/), part three will appear tomorrow._

### The classic journal article format is not easily updated or corrected.

Scientific understanding is constantly changing as phenomena are discovered and mistakes uncovered.  The classic journal article, however, is static.  When a serious flaw in an article is found, the best a paper-based system can do is issue a retraction, and hope that a reader going through past issues will eventually come across the change.

Surprisingly, retractions and corrections continue to go mostly unnoticed in the digital era.  Studies have shown that retracted papers go on to receive, on average, more than 10 post-retraction citations, with less than 10% of those citations acknowledging the retraction ([Budd et al, 2011](http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf)).  Why is this happening?  While many article databases such as PubMed provide retraction notices, the articles themselves are often not amended.  Readers accessing papers directly from publishers’ websites, or from previously saved copies, can sometimes miss it.  A case study of 18 retracted articles found several which they classified as “high risk of missing [the] notice”, with no notice given in the text of the pdf or html copies themselves ([Wright et al, 2011](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/#mlab.1536-5050.99.2.010.sg002)).  It seems likely that corrections have even more difficulty being seen and acknowledged by subsequent researchers.

There are several technological solutions which can be tried.  One promising avenue would be the adoption of version control.  Also called revision control, this is a way of tracking all changes made to a project.  This technology has been used for decades in computer science and is becoming more and more popular - Wikipedia and Google Docs, for instance, both use version control.  Citations for a paper could reference the version of the paper then available, but subsequent readers would be notified that a more recent version could be viewed.  In addition to making it easy to see how articles have been changed, adopting such a system would acknowledge the frequency of retractions and corrections and the need to check for up to date information.

Another potential tool would be an alert system.  When changes are made to an article, the authors of all articles which cite it could be notified.  However, this would require the maintenance of up-to-date contact information for authors, and the adoption of communications standards across publishers (something that has been accomplished before with initiatives like [CrossRef](http://www.crossref.org/)).
A more transformative approach would be to view papers not as static documents but as ongoing projects that can be updated and contributed to over time.  Projects could be tracked through version control from their very inception, allowing for a kind of [pre-registration](http://www.theguardian.com/science/blog/2013/jun/05/trust-in-science-study-pre-registration).  Replications and new analyses could be added to the project as they’re completed.  The most insightful questions and critiques from the public could lead to changes in new versions of the article.

### The classic journal article only recognizes certain kinds of contributions.

When journal articles were first developed in the 1600s, the idea of crediting an author or authors must have seemed straightforward.  After all, most research was being done by individuals or very small groups, and there were no such things as curriculum vitae or tenure committees.  Over time, academic authorship has become the single most important factor in determining career success for individual scientists.  The limitations of authorship can therefore have an incredible impact on scientific progress.

There are two major problems with authorship as it currently functions, and they are sides of the same coin.  Authorship does not tell you what, precisely, each author did on a paper.  And authorship does not tell you who, precisely, is responsible for each part of a paper.
Currently, the authorship model provides only a vague idea of who is responsible for a paper.  While this is sometimes elaborated upon briefly in the footnotes, or mentioned in the article, more often readers employ simple heuristics.  In psychology, the first author is believed to have led the work, the last author to have provided physical and conceptual resources for the experiment, and any middle authors to have contributed in an unknown but significant way.  This is obviously not an ideal way to credit people, and often leads to disputes, with first authorship [sometimes misattributed](http://www.nature.com/naturejobs/science/articles/10.1038/nj7417-591a).  It has grown increasingly impractical as [multiauthor papers](http://archive.sciencewatch.com/newsletter/2012/201207/multiauthor_papers/) have become more and more common.  What does authorship on a 500-author paper even mean?

The situation is even worse for people whose contributions are not awarded with authorship.  While contributions may be mentioned in the acknowledgements or cited in the body of the paper, neither of these have much impact when scientists are applying for jobs or up for tenure.  This gives them little motivation to do work which will not be recognized with authorship.  And such work is greatly needed.  The development of tools, the collection and release of open data sets, the creation of popularizations and teaching materials, and the deep and thorough review of others’ work - these are all done as favors or side projects, even though they are vital to the progress of research.
How can new technologies address these problems?  There have been few changes made in this area, perhaps due to the heavy weight of authorship in scientific life, although there are some tools like [Figshare](http://figshare.com/) which allow users to share non-traditional materials such as datasets and posters in citable (and therefore creditable) form.  A more transformative change might be to use the version control system mentioned above.  Instead of tracking changes to the article from publishing onwards, it could follow the article from its beginning stages.  In that way, each change could be attributed to a specific person.

Another option might simply be to describe contributions in more detail.  Currently if I use your methodology wholesale, or briefly mention a finding of yours, I acknowledge you in the same way - a citation.  What if, instead, all significant contributions were listed?  Although space is not a constraint with digital articles, the human attention span remains limited, and so it might be useful to create common categories for contribution, such as reviewing the article, providing materials, doing analyses, or coming up with an explanation for discussion.

There are two other problems are worth mentioning in brief.  First, the phenomenon of [ghost authorship](https://en.wikipedia.org/wiki/Academic_authorship#Ghost_authorship), where substantial contributions to the running of a study or preparation of a manuscript go unacknowledged.  This is frequently done in industry-sponsored research to hide conflicts of interest.  If journal articles used a format where every contribution was tracked, ghost authorship would be impossible.  Another issue is the assignment of contact authors, the researchers on a paper who readers are invited to direct questions to.  Contact information can become outdated fairly quickly, causing access to data and materials to be lost; if contact information can be changed, or responsibility passed on to a new person, such loss can be prevented.

