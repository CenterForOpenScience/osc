<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>Open Science Collaboration Blog</title>
<!--        <link rel="shortcut icon" href="http://centerforopenscience.github.io/osc/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Open Science Collaboration Blog Atom Feed" />

        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="selected"><a href="http://centerforopenscience.github.io/osc">Home</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/about.html">About</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/authors.html">Authors</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/policy.html">Policy</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://centerforopenscience.github.io/osc"><img src="http://centerforopenscience.github.io/osc/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Jan  1,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/01/01/open-science-timeline/" rel="bookmark" title="Permanent Link to &quot;Timeline of Notable Open Science Events in 2013 - Psychology&quot;">Timeline of Notable Open Science Events in 2013 - Psychology</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/jon-grahe-pacific-lutheran-university.html" rel="author">Jon Grahe, Pacific Lutheran University</a>

   			   <p>Happy New Year! New Year’s is a great time for reflection and resolution, and when I reflect on 2013, I view it with an air of excitement and promise. As a social psychologist, I celebrated with my many of my colleagues in Washington, DC. at the 25th anniversary of the Association for Psychological Science. There were many celebrations including a ‘80s themed dance night at the Convention. However, this year was also marred by the “Crisis of Confidence” in psychological and broader sciences that has been percolating since the turn of the 21st century. Our timeline begins the year with the <em>Perspectives on Psychological Science</em>’s special issue dedicated to addressing this Crisis. Rather than focusing on the problems, papers in this issue suggested solutions and many of those suggestions emerged as projects in 2013. This timeline focuses on these many Open Science Collaboration successes and initiatives and offers a glimpse at the activity directed at reaching the Scientific Utopia envisioned by so many in the OSC. </p>
<p>Maybe when APS celebrates its 50th Anniversary, it will also mark the 25th Anniversary of the year that the tide turned on the bad practices that had led to the “Crisis of Confidence”. Perhaps in addition to a ‘13 themed dance band playing Lorde’s “Royals” or Imagine Dragon’s “Demons”, maybe there will be a theme reflecting on changing science practices. With the COS celebrating a 25th anniversary of its own, let us share your memory of the important events from 2013. </p>
<p>These posts reflect a limited list of psychology-related events that one person noticed. We invite you to add other notable events that you feel are missing from this list, particularly in other scientific areas. Add a comment below with information about any research projects aimed at replication across institutions or initiatives directed at making science practices more transparent. </p>
<p><a href="http://cdn.knightlab.com/libs/timeline/latest/embed/index.html?source=0An4eLhySzFmBdFV4Wjh0SkZkajU3dEV6b08tV1p4dmc&amp;font=Bevan-PotanoSans&amp;maptype=TERRAIN&amp;lang=en&amp;height=650">View the timeline!</a></p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/01/01/open-science-timeline/#disqus_thread" data-disqus-identifier="2014/01/01/open-science-timeline/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/01/01/open-science-timeline/">Posted at  1:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 18,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/12/18/researcher-degrees-of-freedom/" rel="bookmark" title="Permanent Link to &quot;Researcher Degrees of Freedom in Data Analysis&quot;">Researcher Degrees of Freedom in Data Analysis</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/sean-mackinnon.html" rel="author">Sean Mackinnon</a>

   			   <p>The enormous amount of options available for modern data analysis is both a blessing and a curse. On one hand, researchers have specialized tools for any number of complex questions. On the other hand, we’re also faced with a staggering number of equally-viable choices, many times without any clear-cut guidelines for deciding between them. For instance, I just popped open SPSS statistical software and counted 18 different ways to conduct post-hoc tests for a one-way ANOVA. Some choices are clearly inferior (e.g., the <a href="http://www.graphpad.com/guides/prism/6/statistics/index.htm?stat_fishers_lsd.htm">LSD test</a> doesn’t adjust p-values for multiple comparisons) but it’s possible to defend the use of many of the available options. These ambiguous choice points are sometimes referred to as researcher degrees of freedom.</p>
<p>In theory, researcher degrees of freedom shouldn’t be a problem. More choice is better, right? The problem arises from two interconnected issues: (a) Ambiguity as to which statistical test is most appropriate and (b) an incentive system where scientists are rewarded with publications, grants, and career stability when their p-values fall below the revered p &lt; .05 criterion. So, perhaps unsurprisingly, when faced with a host of ambiguous options for data analysis, most people settle on the one that achieves statistically significant results. Simmons, Nelson, and Simonsohn (2011) argue that this undisclosed flexibility in data analysis allows people to present almost any data as “significant,” and calls for 10 simple guidelines for reviewers and authors to disclose in every paper – which, if you haven’t read yet <a href="http://pss.sagepub.com/content/22/11/1359.full.pdf+html">are worth checking out</a>. In this post, I will discuss a few guidelines of my own for conducting data analysis in a way that strives to overcome our inherent tendency to be self-serving.</p>
<ol>
<li>
<p>Make as many data analytic decisions as possible before looking at your data. Review the statistical literature and decide on which statistical test(s) will be best before looking at your collected data. Continue to use those tests until enough evidence emerges to change your mind. The important thing is that you make these decisions before looking at your data. Once you start playing with the actual data, your self-serving biases will start to kick in. Do not underestimate your ability for self-deception: Self-serving biases are powerful, pervasive, and apply to virtually everyone. Consider pre-registering your data analysis plan (perhaps using the <a href="https://openscienceframework.org/">Open Science Framework</a> to keep yourself honest and to convince future reviewers that you aren’t exploiting researcher degrees of freedom.</p>
</li>
<li>
<p>When faced with a situation where there are too many equally viable choices, run a small number of the best choices, and report all of them. In this case, decide on 2-5 different tests ahead of time. Report the results of all choices, and make a tentative conclusion based if the majority of these tests agree. For instance, when determining model fit in structural equation modeling, there <a href="http://davidakenny.net/cm/fit.htm">many different methods you might use</a>. If you can’t figure out which method is best by reviewing the statistical literature – it’s not entirely clear, statisticians disagree about as often as any other group of scientists – then report the results of all tests, and make a conclusion if they all converge on the same solution. When they disagree, make a tentative conclusion based on the majority of tests that agree (e.g., 2 of 3 tests come to the same conclusion). For the record, I currently use CFI, TLI, RMSEA, and SRMR in my own work, and use these even if other fit indices provide more favorable results.</p>
</li>
<li>
<p>When deciding on a data analysis plan after you’ve seen the data, keep in mind that most researcher degrees of freedom have minimal impact on strong results. For any number of reasons, you might find yourself deciding on a data analysis plan after you’ve played around with the data for a while. At the end of the day, strong data will not be influenced much by researcher degrees of freedom. For instance, results should look much the same regardless of whether you exclude outliers, transform them, or leave them in the data when you have a study with high statistical power. Simmons et al. (2011) specifically recommend that results should be presented (a) with and without covariates, and (b) with and without specific data points excluded, if any were removed. Again, the general idea is that strong results will not change much when you alter researcher degrees of freedom. Thus, I again recommend analyzing the data in a few different ways and looking for convergence across all methods when you’re developing a data analysis plan after seeing the data. This sets the bar higher to try and combat your natural tendency to report just the one analysis that “works.” When minor data analytic choices drastically change the conclusions, this should be a warning sign that your solution is unstable and the results are probably not trustworthy.  The number one reason why you have an unstable solution is probably because you have <a href="http://osc.centerforopenscience.org/2013/11/03/Increasing-statistical-power/">low statistical power</a>. Since you hopefully had a strict data collection end date, the only viable alternative when results are unstable is to replicate the results in a second, more highly-powered study using the same data analytic approach.</p>
</li>
</ol>
<p>At the end of the day, there is no “quick-fix” for the problem of self-serving biases during data analysis so long as the incentive system continues to reward novel, statistically significant results. However, by using the tips in this article (and elsewhere) researchers can focus on finding strong, replicable results by minimizing the natural human tendency to be self-serving.</p>
<p><strong>References</strong></p>
<p>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). <a href="http://pss.sagepub.com/content/22/11/1359.full.pdf+html">False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant.</a> Psychological Science, 22, 1359-1366. doi:10.1177/0956797611417632</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/12/18/researcher-degrees-of-freedom/#disqus_thread" data-disqus-identifier="2013/12/18/researcher-degrees-of-freedom/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/12/18/researcher-degrees-of-freedom/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 13,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/12/13/chasing-paper-3/" rel="bookmark" title="Permanent Link to &quot;Chasing Paper, Part 3&quot;">Chasing Paper, Part 3</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This is part three of a three part post brainstorming potential improvements to the journal article format.  Part one is <a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/">here</a>, part two is <a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/">here</a>.</em></p>
<h3>The classic journal article is only readable by domain experts.</h3>
<p>Journal articles are currently written for domain experts.  While novel concepts or terms are usually explained, there is the assumption of a vast array of background knowledge and jargon is the rule, not the exception.  While this leads to quick reading for domain experts, it can make for a difficult slog for everyone else.</p>
<p>Why is this a problem?  For one thing, it prevents interdisciplinary collaboration.  Researchers will not make a habit of reading outside their field if it takes hours of painstaking, self-directed work to comprehend a single article.  It also discourages public engagement.  While science writers do admirable work boiling hard concepts down to their comprehensible cores, many non-scientists want to actually read the articles, and get discouraged when they can’t.  </p>
<p>While opaque scientific writing exists in every format, technologies present new options to translate and teach.  Jargon could be linked to a glossary or <a href="https://en.wikipedia.org/wiki/Mouseover">other reference material</a>.  You could be given a plain english explanation of a term when your mouse hovers over it.  Perhaps each article could have multiple versions - for domain experts, other scientists, and for laypeople.  </p>
<p>Of course, the ability to write accessibly is a skill not everyone has.  Luckily, any given paper would mostly use terminology already introduced in previous papers.  If researchers could easily credit the teaching and popularization work done by others, they could acknowledge the value of those contributions while at the same time making their own work accessible.</p>
<h3>The classic journal article has no universally-agreed upon standards.</h3>
<p>Academic publishing, historically, has been a distributed system.  Currently, the top three publishers still account for less than half (42%) of all published articles (<a href="http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html">McGuigan and Russell, 2008</a>).  While certain format and content conventions are shared among publishers, generally speaking it’s difficult to propagate new standards, and even harder to enforce them.  Not only do standards vary, they are frequently hidden, with most of the review and editing process taking place behind closed doors.</p>
<p>There are benefits to decentralization, but the drawbacks are clear.  Widespread adoption of new standards, such as <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588">Simmons et al’s 21 Word Solution</a> or <a href="http://centerforopenscience.org/journals/">open science practices</a>, depends on the hard work and high status of those advocating for them.  How can the article format be changed to better accommodate changing standards, while still retaining individual publishers’ autonomy?</p>
<p>One option might be to create a new section of each journal article, a free-form field where users could record whether an article met this or that standard.  Researchers could then independently decide what standards they wanted to pay attention to.  While this sounds messy, if properly implemented this feature could be used very much like a search filter, yet would not require the creation or maintenance of a centralized database.</p>
<p>A different approach is already being embraced: an effort to make the standards that currently exist more transparent by bringing peer review out into the open.  <a href="https://en.wikipedia.org/wiki/Open_peer_review">Open peer review</a> allows readers to view an article’s pre-publication history, including the authorship and content of peer reviews, while <a href="http://publications.copernicus.org/services/public_peer_review.html">public peer review</a> allows the public to participate in the review process.  However, these methods have yet to be generally adopted.</p>
<p>*</p>
<p>It’s clear that journal articles are already changing.  But they may not be changing fast enough.  It may be better to forgo the trappings of the journal article entirely, and seek a new system that more naturally encourages collaboration, curation, and the efficient use of the incredible resources at our disposal.  With journal articles commonly costing more than $30 each, some might jump at the chance to leave them behind.</p>
<p>Of course, it’s easy to play “what if” and imagine alternatives; it’s far harder to actually implement them.  And not all innovations are improvements.  But with <a href="http://www.battelle.org/media/press-releases/battelle-r-d-magazine-annual-global-funding-forecast-predicts-r-d-spending-growth-will-continue-while-globalization-accelerates">over a billion dollars</a> spent on research each day in the United States, with <a href="http://www.bmj.com/content/341/bmj.c6815">over 25,000 journals</a> in existence, and over a million articles published each year, surely there is room to experiment.</p>
<h4>Bibliography</h4>
<p>Budd, J.M., Coble, Z.C. and Anderson, K.M.  (2011)  <a href="http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf">Retracted Publications in Biomedicine: Cause for Concern.</a></p>
<p>Wright, K. and McDaid, C.  (2011).  <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/?report=classic">Reporting of article retractions in bibliographic databases and online journals.</a>  J Med Libr Assoc. 2011 April; 99(2): 164–167.</p>
<p>McGuigan, G.S. and Russell, R.D.  (2008).  <a href="http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html">The Business of Academic Publishing: A Strategic Analysis of the Academic Journal Publishing Industry and its Impact on the Future of Scholarly Publishing</a>.  Electronic Journal of Academic and Special Librarianship.  Winter 2008; 9(3).</p>
<p>Simmons, J.P., Nelson, L.D. and Simonsohn, U.A.  (2012)  <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588">A 21 Word Solution</a>.  </p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/12/13/chasing-paper-3/#disqus_thread" data-disqus-identifier="2013/12/13/chasing-paper-3/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/12/13/chasing-paper-3/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/chasing-paper.html">chasing-paper</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 12,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/12/12/chasing-paper-2/" rel="bookmark" title="Permanent Link to &quot;Chasing Paper, Part 2&quot;">Chasing Paper, Part 2</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This is part two of a three part post brainstorming potential improvements to the journal article format.  Part one is <a href="http://osc.centerforopenscience.org/2013/12/11/chasing-paper/">here</a>, part three is here <a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/">here</a>.</em></p>
<h3>The classic journal article format is not easily updated or corrected.</h3>
<p>Scientific understanding is constantly changing as phenomena are discovered and mistakes uncovered.  The classic journal article, however, is static.  When a serious flaw in an article is found, the best a paper-based system can do is issue a retraction, and hope that a reader going through past issues will eventually come across the change.</p>
<p>Surprisingly, retractions and corrections continue to go mostly unnoticed in the digital era.  Studies have shown that retracted papers go on to receive, on average, more than 10 post-retraction citations, with less than 10% of those citations acknowledging the retraction (<a href="http://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/national/2011/papers/retracted_publicatio.pdf">Budd et al, 2011</a>).  Why is this happening?  While many article databases such as PubMed provide retraction notices, the articles themselves are often not amended.  Readers accessing papers directly from publishers’ websites, or from previously saved copies, can sometimes miss it.  A case study of 18 retracted articles found several which they classified as “high risk of missing [the] notice”, with no notice given in the text of the pdf or html copies themselves (<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3066576/#mlab.1536-5050.99.2.010.sg002">Wright et al, 2011</a>).  It seems likely that corrections have even more difficulty being seen and acknowledged by subsequent researchers.</p>
<p>There are several technological solutions which can be tried.  One promising avenue would be the adoption of version control.  Also called revision control, this is a way of tracking all changes made to a project.  This technology has been used for decades in computer science and is becoming more and more popular - Wikipedia and Google Docs, for instance, both use version control.  Citations for a paper could reference the version of the paper then available, but subsequent readers would be notified that a more recent version could be viewed.  In addition to making it easy to see how articles have been changed, adopting such a system would acknowledge the frequency of retractions and corrections and the need to check for up to date information.</p>
<p>Another potential tool would be an alert system.  When changes are made to an article, the authors of all articles which cite it could be notified.  However, this would require the maintenance of up-to-date contact information for authors, and the adoption of communications standards across publishers (something that has been accomplished before with initiatives like <a href="http://www.crossref.org/">CrossRef</a>).
A more transformative approach would be to view papers not as static documents but as ongoing projects that can be updated and contributed to over time.  Projects could be tracked through version control from their very inception, allowing for a kind of <a href="http://www.theguardian.com/science/blog/2013/jun/05/trust-in-science-study-pre-registration">pre-registration</a>.  Replications and new analyses could be added to the project as they’re completed.  The most insightful questions and critiques from the public could lead to changes in new versions of the article.</p>
<h3>The classic journal article only recognizes certain kinds of contributions.</h3>
<p>When journal articles were first developed in the 1600s, the idea of crediting an author or authors must have seemed straightforward.  After all, most research was being done by individuals or very small groups, and there were no such things as curriculum vitae or tenure committees.  Over time, academic authorship has become the single most important factor in determining career success for individual scientists.  The limitations of authorship can therefore have an incredible impact on scientific progress.</p>
<p>There are two major problems with authorship as it currently functions, and they are sides of the same coin.  Authorship does not tell you what, precisely, each author did on a paper.  And authorship does not tell you who, precisely, is responsible for each part of a paper.
Currently, the authorship model provides only a vague idea of who is responsible for a paper.  While this is sometimes elaborated upon briefly in the footnotes, or mentioned in the article, more often readers employ simple heuristics.  In psychology, the first author is believed to have led the work, the last author to have provided physical and conceptual resources for the experiment, and any middle authors to have contributed in an unknown but significant way.  This is obviously not an ideal way to credit people, and often leads to disputes, with first authorship <a href="http://www.nature.com/naturejobs/science/articles/10.1038/nj7417-591a">sometimes misattributed</a>.  It has grown increasingly impractical as <a href="http://archive.sciencewatch.com/newsletter/2012/201207/multiauthor_papers/">multiauthor papers</a> have become more and more common.  What does authorship on a 500-author paper even mean?</p>
<p>The situation is even worse for people whose contributions are not awarded with authorship.  While contributions may be mentioned in the acknowledgements or cited in the body of the paper, neither of these have much impact when scientists are applying for jobs or up for tenure.  This gives them little motivation to do work which will not be recognized with authorship.  And such work is greatly needed.  The development of tools, the collection and release of open data sets, the creation of popularizations and teaching materials, and the deep and thorough review of others’ work - these are all done as favors or side projects, even though they are vital to the progress of research.
How can new technologies address these problems?  There have been few changes made in this area, perhaps due to the heavy weight of authorship in scientific life, although there are some tools like <a href="http://figshare.com/">Figshare</a> which allow users to share non-traditional materials such as datasets and posters in citable (and therefore creditable) form.  A more transformative change might be to use the version control system mentioned above.  Instead of tracking changes to the article from publishing onwards, it could follow the article from its beginning stages.  In that way, each change could be attributed to a specific person.</p>
<p>Another option might simply be to describe contributions in more detail.  Currently if I use your methodology wholesale, or briefly mention a finding of yours, I acknowledge you in the same way - a citation.  What if, instead, all significant contributions were listed?  Although space is not a constraint with digital articles, the human attention span remains limited, and so it might be useful to create common categories for contribution, such as reviewing the article, providing materials, doing analyses, or coming up with an explanation for discussion.</p>
<p>There are two other problems are worth mentioning in brief.  First, the phenomenon of <a href="https://en.wikipedia.org/wiki/Academic_authorship#Ghost_authorship">ghost authorship</a>, where substantial contributions to the running of a study or preparation of a manuscript go unacknowledged.  This is frequently done in industry-sponsored research to hide conflicts of interest.  If journal articles used a format where every contribution was tracked, ghost authorship would be impossible.  Another issue is the assignment of contact authors, the researchers on a paper who readers are invited to direct questions to.  Contact information can become outdated fairly quickly, causing access to data and materials to be lost; if contact information can be changed, or responsibility passed on to a new person, such loss can be prevented.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/12/12/chasing-paper-2/#disqus_thread" data-disqus-identifier="2013/12/12/chasing-paper-2/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/12/12/chasing-paper-2/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/chasing-paper.html">chasing-paper</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec 11,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/12/11/chasing-paper/" rel="bookmark" title="Permanent Link to &quot;Chasing Paper, Part 1&quot;">Chasing Paper, Part 1</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p><em>This is part one of a three part post.  Parts <a href="http://osc.centerforopenscience.org/2013/12/12/chasing-paper-2/">two</a> and <a href="http://osc.centerforopenscience.org/2013/12/13/chasing-paper-3/">three</a> have now been posted.</em></p>
<p>The academic paper is old - older than the steam engine, the pocket watch, the piano, and the light bulb.  The first journal, <a href="http://rstl.royalsocietypublishing.org/">Philosophical Transactions</a>, was published on March 6th, 1665.  Now that doesn’t mean that the journal article format is obsolete - many inventions much older are still in wide use today.  But after a third of a millennium, it’s only natural that the format needs some serious updating.</p>
<p>When brainstorming changes, it may be useful to think of the limitations of ink and paper.  From there, we can consider how new technologies can improve or even transform the journal article.  Some of these changes have already been widely adopted, while others have never even been debated.  Some are adaptive, using the greater storage capacity of computing to extend the functions of the classic journal article, while others are transformative, creating new functions and features only available in the 21st century.</p>
<p>The ideas below are suggestions, not recommendations - it may be that some aspects of the journal article format are better left alone.  But we all benefit from challenging our assumptions about what an article is and ought to be.</p>
<h3>The classic journal article format cannot convey the full range of information associated with an experiment.</h3>
<p>Until the rise of modern computing, there was simply no way for researchers to share all the data they collected in their experiments.  Researchers were forced to summarize: to gloss over the details of their methods and the reasoning behind their decisions and, of course, to provide statistical analyses in the place of raw data.  While fields like <a href="http://www.techrepublic.com/blog/european-technology/cern-where-the-big-bang-meets-big-data/">particle physics</a> and <a href="http://www.nytimes.com/2011/12/01/business/dna-sequencing-caught-in-deluge-of-data.html?pagewanted=all&amp;_r=0">genetics</a> continue to push the limits of memory, most experimenters now have the technical capacity to share all of their data.</p>
<p>Many journals have taken to publishing supplemental materials, although this rarely encompasses the entirety of data collected, or enough methodological detail to allow for independent replication.  There are plenty of explanations for this slow adoption, including ethical considerations around human subjects data, the potential to patent methods, or the cost to journals of hosting this extra materials.  But these are obstacles to address, not reasons to give up.  The potential benefits are enormous:  What if every published paper contained enough methodological detail that it could be independently replicated?  What if every paper contained enough raw data that it could be included in meta-analysis?  How much of meta-scientific work is never undertaken, because it's dependent on getting dozens or hundreds of contact authors to return your emails, and on universities to properly store data and materials?</p>
<p>Providing supplemental material, no matter how extensive, is still an adaptive change.  What might a transformative change look like?  Elsevier’s <a href="http://www.articleofthefuture.com/">Article of the Future</a> project attempts to answer that question with new, experimental formats that include videos, interactive models, and infographics.  These designs are just the beginning.  What if articles allowed readers to actually interact with the data and perform their own analyses?  <a href="https://en.wikipedia.org/wiki/Virtual_environment_software">Virtual environments</a> could be set up, lowering the barrier to independent verification of results.  What if authors reported when they made questionable methodological decisions, and allowed readers, where possible, to see the results when a variable was not controlled for, or a sample was not excluded?</p>
<h3>The classic journal article format is difficult to organize, index or search.</h3>
<p>New technology has already transformed the way we search the scientific literature.  Where before researchers were reliant on catalogues and indexes from publishers, and used abstracts to guess at relevance, databases such as PubMed and Google Scholar allow us to find all mentions of a term, tool, or phenomena across vast swathes of articles.  While searching databases is itself a skill, its one that allows us to search comprehensively and efficiently, and gives us more opportunities to explore.</p>
<p>Yet old issues of organization and curation remain.  Indexes used to speed the slow process of skimming through physical papers.  Now they’re needed to help researchers sort through the abundance of articles constantly being published.  With <a href="http://duncan.hull.name/2010/07/15/fifty-million/">tens of millions</a> of journal articles out there, how can we be sure we’re really accessing all the relevant literature?  How can we compare and synthesize the thousands of results one might get on a given search?</p>
<p>Special kinds of articles - reviews and meta-analyses - have traditionally helped us synthesize and curate information.  As discussed above, new technologies can help make meta-analyses more common by making it easier for researchers to access information about past studies.  We can further improve the search experience by creating more detailed <a href="https://en.wikipedia.org/wiki/Metadata_standards">metadata</a>.  Metadata, in this context, is the information attached to an article which lets us categorize it without having to read the article itself.  Currently, fields like title, author, date, and journal are quite common in databases.  More complicated fields less often adopted, but you can find metadata on study type, population, level of clinical trial (where applicable), and so forth.  What would truly comprehensive metadata look like?  Is it possible to store the details of experimental structure or analysis in machine-readable format - and is that even desirable?</p>
<p>What happens when we reconsider not the metadata but the content itself?  Most articles are structurally complex, containing literature reviews, methodological information, data, and analysis.  Perhaps we might be better served by breaking those articles down into their constituent parts.  What if methods, data, analysis were always published separately, creating a network of papers that were linked but discrete?  Would that be easier or harder to organize?  It may be that what we need here is not a better kind of journal article, but a new way of curating research entirely.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/12/11/chasing-paper/#disqus_thread" data-disqus-identifier="2013/12/11/chasing-paper/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/12/11/chasing-paper/">Posted at 11:30 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                        <div class="tags">
                            <a href="http://centerforopenscience.github.io/osc/tag/chasing-paper.html">chasing-paper</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Dec  9,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/12/09/reviewer-statement-initiative/" rel="bookmark" title="Permanent Link to &quot;New “Reviewer Statement” Initiative Aims to (Further) Improve Community Norms Toward Disclosure&quot;">New “Reviewer Statement” Initiative Aims to (Further) Improve Community Norms Toward Disclosure</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/etienne-lebel.html" rel="author">Etienne LeBel</a>

   			   <p><img src="images/etiennelebel.jpg" alt="Photo of Etienne LeBel" align="left" style="padding-right: 20px;" /></p>
<p>An Open Science Collaboration -- made up of Uri Simonsohn, Etienne LeBel, Don Moore, Leif D. Nelson, Brian Nosek, and Joe Simmons -- is glad to announce a new initiative aiming to improve community norms toward the disclosure of basic methodological information during the peer-review process. Endorsed by the Center for Open Science, the initiative involves a standard reviewer statement that any peer reviewer can include in their review requesting that authors add a statement to the paper confirming that they have disclosed all data exclusions, experimental conditions, assessed measures, and how they determined their samples sizes (following from the 21-word solution; Simmons, Nelson, &amp; Simonsohn, 2012, 2013; see also <a href="http://psychdisclosure.org/">PsychDisclosure.org</a>; LeBel et al., 2013). Here is the statement, which is <a href="http://osf.io/project/hadz3">available on the Open Science Framework</a>:</p>
<p><em>"I request that the authors add a statement to the paper confirming whether, for all experiments, they have reported all measures, conditions, data exclusions, and how they determined their sample sizes. The authors should, of course, add any additional text to ensure the statement is accurate. This is the standard reviewer disclosure request endorsed by the Center for Open Science (<a href="http://osf.io/project/hadz3">see http://osf.io/project/hadz3</a>). I include it in every review."</em></p>
<p>The idea originated from the realization that as peer reviewers, we typically lack fundamental information regarding how the data was collected and analyzed which prevents us from be able to properly evaluate the claims made in a submitted manuscript. Some reviewers interested in requesting such information, however, were concerned that such requests would make them appear selective and/or compromise their anonymity. Discussions ensued and contributors developed a standard reviewer disclosure request statement that overcomes these concerns and allows the community of reviewers to improve community norms toward the disclosure of such methodological information across all journals and articles.</p>
<p>Some of the contributors, including myself, were hoping for a reviewer statement with a bit more teeth. For instance, requesting the disclosure of such information as a requirement before accepting to review an article or requiring the re-review of a revised manuscript once the requested information has been disclosed. The team of contributors, however, ultimately decided that it would be better to start small to get acceptance, in order to maximize the probability that the initiative has an impact in shaping the community norms.</p>
<p>Hence, next time you are invited to review a manuscript for publication at any journal, please remember to include the reviewer disclosure statement!</p>
<p><strong>References</strong></p>
<p>LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K. R., Ratliff, K. A., &amp; Smith, C. T. (2013). <a href="http://www.google.com/url?q=http%3A%2F%2Fpps.sagepub.com%2Fcontent%2F8%2F4%2F424.full&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEcYQrJzKL33sb33l9teszIxVUNAg">PsychDisclosure.org: Grassroots support for reforming reporting standards in psychology.</a> Perspectives on Psychological Science, 8(4), 424-432. doi: 10.1177/1745691613491437</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2011) <a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1850704&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGqvW1VEcf3RzOb7zE0Y25FXdQHBQ">False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allow Presenting Anything as Significant.</a> Psychological Science, 22(11), 1359-1366.</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2012) <a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D2160588&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGt1mELWBFnDRS2Yh-E-qm_tAp_2A">A 21 Word Solution.</a> Dialogue: The Official Newsletter of the Society for Personality and Social Psychology, 26(2), 4-7.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/12/09/reviewer-statement-initiative/#disqus_thread" data-disqus-identifier="2013/12/09/reviewer-statement-initiative/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/12/09/reviewer-statement-initiative/">Posted at 11:30 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov 27,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/11/27/the-state-of-open-access/" rel="bookmark" title="Permanent Link to &quot;The State of Open Access&quot;">The State of Open Access</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/shauna-gordon-mckeon.html" rel="author">Shauna Gordon-McKeon</a>

   			   <p>To celebrate <a href="http://www.openaccessweek.org/">Open Access Week</a> last month, we asked people four questions about the state of open access and how it's changing.  Here are some in depth answers from two people working on open access: <a href="http://cyber.law.harvard.edu/~psuber/wiki/Peter_Suber">Peter Suber</a>, Director of the <a href="https://osc.hul.harvard.edu/">Harvard Office for Scholarly Communication</a> and the <a href="http://cyber.law.harvard.edu/research/hoap">Harvard Open Access Project</a>, and <a href="http://www.plos.org/staff/elizabeth-silva/">Elizabeth Silva</a>, associate editor at the Public Library of Science (<a href="http://www.plos.org/">PLOS</a>).</p>
<p><strong>How is your work relevant to the changing landscape of Open Access?  What would be a successful outcome of your work in this area?</strong></p>
<p><em>Elizabeth</em>:  PLOS is now synonymous with open access publishing, so it’s hard to believe that 10 years ago, when PLOS was founded, most researchers were not even aware that availability of research was a problem. We all published our best research in the best journals. We assumed our colleagues could access it, and we weren’t aware of (or didn’t recognize the problem with) the inability of people outside of the ivory tower to see this work. At that time it was apparent to the founders of PLOS, who were among the few researchers who recognized the problem, that the best way to convince researchers to publish open access would be for PLOS to become an open access publisher, and prove that OA could be a viable business model and an attractive publishing venue at the same time. I think that we can safely say that the founders of PLOS succeeded in this mission, and they did it decisively. </p>
<p>We’re now at an exciting time, where open access in the natural sciences is all but inevitable. We now get to work on new challenges, trying to solve other issues in research communication. </p>
<p><em>Peter</em>:  My current job has two parts. I direct the Harvard Office for Scholarly Communication (OSC), and I direct the Harvard Open Access Project (HOAP). The OSC aims to provide OA to research done at Harvard University. We implement Harvard's OA policies and maintain its OA repository. We focus on peer-reviewed articles by faculty, but are expanding to other categories of research and researchers. In my HOAP work, I consult pro bono with universities, scholarly societies, publishers, funding agencies, and governments, to help them adopt effective OA policies. HOAP also maintains a guide to good practices for university OA policies, manages the Open Access Tracking Project, writes reference pages on federal OA-related legislation, such as FASTR, and makes regular contributions to the Open Access Directory and the catalog of OA journals from society publishers. </p>
<p>To me success would be making OA the default for new research in every field and language. However, this kind of success more like a new plateau than a finish line. We often focus on the goal of OA itself, or the goal of removing access barriers to knowledge. But that's merely a precondition for an exciting range of new possibilities for making use of that knowledge. In that sense, OA is closer to the minimum than the maximum of how to take advantage of the internet for improving research. Once OA is the default for new research, we can give less energy to attaining it and more energy to reaping the benefits, for example, integrating OA texts with open data, improving the methods of meta-analysis and reproducibility, and building better tools for knowledge extraction, text and data mining, question answering, reference linking, impact measurement, current awareness, search, summary, translation, organization, and recommendation.</p>
<p>From the researcher's side, making OA the new default means that essentially all the new work they write, and essentially all the new work they want to read, will be OA. From the publisher's side, making OA the new default means that sustainability cannot depend on access barriers that subtract value, and must depend on creative ways to add value to research that is already and irrevocably OA. </p>
<p><strong>How do you think the lack of Open Access is currently impacting how science is practiced?</strong></p>
<p><em>Peter</em>:  The lack of OA slows down research. It distorts inquiry by making the retrievability of research a function of publisher prices and library budgets rather than author consent and internet connectivity. It hides results that happen to sit in journals that exceed the affordability threshold for you or your institution. It limits the correction of scientific error by limiting the number of eyeballs that can examine new results. It prevents the use of text and data mining to supplement human analysis with machine analysis. It hinders the reproducibility of research by excluding many who would want to reproduce it. At the same time, and ironically, it increases the inefficient duplication of research by scholars who don't realize that certain experiments have already been done. </p>
<p>It prevents journalists from reading the latest developments, reporting on them, and providing direct, usable links for interested readers. It prevents unaffiliated scholars and the lay public from reading new work in which they may have an interest, especially in the humanities and medicine. It blocks research-driven industries from creating jobs, products, and innovations. It prevents taxpayers from maximizing the return on their enormous investment in publicly-funded research.</p>
<p>I assume we're talking about research that authors publish voluntarily, as opposed to notes, emails, and unfinished manuscripts, and I assume we're talking about research that authors write without expectation of revenue. If so, then the lack of OA harms research and researchers without qualification. The lack of OA benefits no one except conventional publishers who want to own it, sell it, and limit the audience to paying customers. </p>
<p><em>Elizabeth</em>:  There is a prevailing idea that those that need access to the literature already have it; that those that have the ability to understand the content are at institutions that can afford the subscriptions. First, this ignores the needs of physicians, educators, science 
communicators, and smaller institutions and companies. More fundamentally, limiting access to knowledge, so that rests in the hands of an elite 1%, is archaic, backwards, and counterproductive. There has never been a greater urgency to find solutions to problems that fundamentally threaten human existence – climate change, disease transmission, food security – and in the face of this why would we advocate limited dissemination of knowledge? Full adoption of open access has the potential to fundamentally change the pace of scientific progress, as we make this information available to everyone, worldwide.</p>
<p>When it comes to issues of reproducibility, fraud or misreporting, all journals face similar issues regardless of the business model. Researchers design their experiments and collect their data long before they decide the publishing venue, and the quality of the reporting likely won’t change based on whether the venue is OA. I think that these issues are better tackled by requirements for open data and improved reporting. Of course these philosophies are certainly intrinsically linked – improved transparency and access can only improve matters.</p>
<p><strong>What do you think is the biggest reason that people resist Open Access?  Do you think there are good reasons for not making a paper open access?</strong></p>
<p><em>Elizabeth</em>:  Of course there are many publishers who resist open access, which reflects a need to protect established revenue streams. In addition to large commercial publishers, there are a lot of scholarly societies whose primary sources of income are the subscriptions for the journals they publish.</p>
<p>Resistance from authors, in my experience, comes principally in two forms. The first is linked to the impact factor, rather than the business model. Researchers are stuck in a paradigm that requires them to publish as ‘high’ as possible to achieve career advancement. While there are plenty of high impact OA publications with which people choose to publish, it just so happens that the highest are subscription journals. We know that open access increases utility, visibility and impact of individual pieces of research, but the fallacy that a high impact journal is equivalent to high impact research persists.</p>
<p>The second reason cited is that the cost is prohibitory. This is a problem everyone at PLOS can really appreciate, and we very much sympathize with authors who do not have the money in their budget to pay author publication charges (APCs). However, it’s a problem that should really be a lot easier to overcome. If research institutions were to pay publication fees, rather than subscription fees, they would save a fortune; a few institutions have realized this and are paying the APCs for authors who choose to go OA. It would also help if funders could recognize publishing as an intrinsic part of the research, folding the APC into the grant. We are also moving the technology forward in an effort to reduce costs, so that savings can be passed onto authors. PLOS ONE has been around for nearly 7 years, and the fees have not changed. This reflects efforts to keep costs as low as we can. Ironically, the biggest of the pay-walled journals already charge authors to publish: for example, it can be between $500 and $1000 for the first color figure, and a few hundred for each additional one; on top of this there are page charges and reprint costs. Not only is the public paying for the research and the subscription, they are paying for papers that they can’t read.</p>
<p><em>Peter</em>:  There are no good reasons for not making a paper OA, or at least for not wanting to. </p>
<p>There are sometimes reasons not to publish in an OA journal. For example, the best journals in your field may not be OA. Your promotion and tenure committee may give you artificial incentives to limit yourself to a certain list of journals. Or the best OA journals in your field may charge publication fees which your funder or employer will not pay on your behalf. However, in those cases you can publish in a non-OA journal and deposit the peer-reviewed manuscript in an OA repository. </p>
<p>The resistance of non-OA publishers is easier to grasp. But if we're talking about publishing scholars, not publishers, then the largest cause of resistance by far is misunderstanding. Far too many researchers still accept false assumptions about OA, such as these 10:</p>
<p>--that the only way to make an article OA is to publish it in an OA journal
--that all or most OA journals charge publication fees
--that all or most publication fees are paid by authors out of pocket
--that all or most OA journals are not peer reviewed
--that peer-reviewed OA journals cannot use the same standards and even the same people as the best non-OA journals
--that publishing in a non-OA journal closes the door on lawfully making the same article OA
--that making work OA makes it harder rather than easier to find
--that making work OA limits rather than enhances author rights over it
--that OA mandates are about submitting new work to OA journals rather than depositing it in OA repositories, or
--that everyone who needs access already has access. </p>
<p>In a <a href="http://www.theguardian.com/higher-education-network/blog/2013/oct/21/open-access-myths-peter-suber-harvard">recent article</a> in <em>The Guardian</em> I corrected six of the most widespread and harmful myths about OA. In a <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:4322571">2009 article</a>, I corrected 25. And in my 2012 <a href="http://bit.ly/oa-book">book</a>, I tried to take on the whole legendarium. </p>
<p><strong>How has the Open Access movement changed in the last five years?  How do you think it will change in the next five years?</strong></p>
<p><em>Peter</em>:  OA has been making unmistakable progress for more than 20 years. Five years ago we were not in a qualitatively different place. We were just a bit further down the slope from where we are today.</p>
<p>Over the next five years, I expect more than just another five years' worth of progress as usual. I expect five years' worth of progress toward the kind of success I described in my answer to your first question. In fact, insofar as progress tends to add cooperating players and remove or convert resisting players, I expect five years' worth of compound interest and acceleration. </p>
<p>In some fields, like particle physics, OA is already the default. In the next five years we'll see this new reality move at an uneven rate across the research landscape. Every year more and more researchers will be able to stop struggling for access against needless legal, financial, and technical barriers. Every year, those still struggling will have the benefit of a widening circle of precedents, allies, tools, policies, best practices, accommodating publishers, and alternatives to publishers.</p>
<p>Green OA mandates are spreading among universities. They're also spreading among funding agencies, for example, in the US, the EU, and global south. This trend will definitely continue, especially with the support it has received from Global Research Council, Science Europe, the G8 Science Ministers, and the World Bank. </p>
<p>With the exception of the UK and the Netherlands, countries adopting new OA policies are learning from the experience of their predecessors and starting with green. I've argued in many places that mandating gold OA is a mistake. But it's a mistake mainly for historical reasons, and historical circumstances will change. Gold OA mandates are foolish today in part because too few journals are OA, and there's no reason to limit the freedom of authors to publish in the journals of their choice. But the percentage of peer-reviewed journals that are OA is growing and will continue to grow. (Today it's about 30%.) Gold OA mandates are also foolish today because gold OA is much more expensive than green OA, and there's no reason to compromise the public interest in order to guarantee revenue for non-adaptive publishers. But the costs of OA journals will decline, as the growing number of OA journals compete for authors, and the money to pay for OA journals will grow as libraries redirect money from conventional journals to OA. </p>
<p>We'll see a rise in policies linking deposit in repositories with research assessment, promotion, and tenure. These policies were pioneered by the University of Liege, and since adopted at institutions in nine countries, and recommended by the Budapest Open Access Initiative, the UK House of Commons Select Committee on Business, Innovation and Skills, and the Mediterranean Open Access Network. Most recently, this kind of policy has been proposed at the national level by the Higher Education Funding Council for England. If it's adopted, it will mitigate the damage of a gold-first policy in the UK. A similar possibility has been suggested for the Netherlands.</p>
<p>I expect we'll see OA in the humanities start to catch up with OA in the sciences, and OA for books start to catch up with OA for articles. But in both cases, the pace of progress has already picked up significantly, and so has the number of people eager to see these two kinds of progress accelerate.</p>
<p>The recent decision that Google's book scanning is fair use means that a much larger swath of print literature will be digitized, if not in every country, then at least in the US, and if not for OA, then at least for searching. This won't open the doors to vaults that have been closed, but it will open windows to help us see what is inside.</p>
<p>Finally, I expect to see evolution in the genres or containers of research. Like most people, I'm accustomed to the genres I grew up with. I love articles and books, both as a reader and author. But they have limitations that we can overcome, and we don't have to drop them to enhance them or to create post-articles and post-books alongside them. The low barriers to digital experimentation mean that we can try out new breeds until we find some that carry more advantages than disadvantages for specific purposes. Last year I sketched out one idea along these lines, which I call an <a href="http://dash.harvard.edu/bitstream/handle/1/10055732/12-02-12.html?sequence=2#rack">evidence rack</a>, but it's only one in an indefinitely large space constrained only by the limits on our imagination. </p>
<p><em>Elizabeth</em>: It’s starting to feel like universal open access is no longer “if” but “when”. In the next five years we will see funders and institutions recognize the importance of access and adopt policies that mandate and financially support OA; resistance will fade away, and it will simply be the way research is published. As that happens, I think the OA movement will shift towards tackling other issues in research communication: providing better measures of impact in the form of article level metrics, decreasing the time to publication, and improving reproducibility and utility of research.</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/11/27/the-state-of-open-access/#disqus_thread" data-disqus-identifier="2013/11/27/the-state-of-open-access/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/11/27/the-state-of-open-access/">Posted at  2:15 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov 20,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/11/20/theoretical-amnesia/" rel="bookmark" title="Permanent Link to &quot;Theoretical Amnesia&quot;">Theoretical Amnesia</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/denny-borsboom.html" rel="author">Denny Borsboom</a>

   			   <p><img src="images/DennyPortrait-cropped.png" alt="Photo of Denny
Boorsboom" align="left" style="padding-right: 20px;" width="200px" /></p>
<p>In the past few months, the Center for Open Science and its associated enterprises have gathered enormous support in the community of psychological scientists. While these developments are happy ones, in my view, they also cast a shadow over the field of psychology: clearly, many people think that the activities of the Center for Open Science, like organizing massive replication work and promoting preregistration, are <em>necessary</em>. That, in turn, implies that something in the current scientific order is seriously <em>broken</em>. I think that, apart from working towards improvements, it is useful to investigate what that something is. In this post, I want to point towards a factor that I think has received too little attention in the public debate; namely, the near absence of unambiguously formalized scientific theory in psychology.</p>
<p>Scientific theories are perhaps the most bizarre entities that the scientific imagination has produced. They have incredible properties that, if we weren’t so familiar with them, would do pretty well in a <em>Harry Potter</em> novel. For instance, scientific theories allow you to work out, on a piece of paper, what would happen to stuff in conditions that aren’t actually realized. So you can figure out whether an imaginary bridge will stand or collapse in imaginary conditions. You can do this by simply just feeding some imaginary quantities that your imaginary bridge would have (like its mass and dimensions) to a scientific theory (say, Newton’s) and out comes a prediction on what will happen. In the more impressive cases, the predictions are so good that you can actually design the entire bridge on paper, then build it according to specifications (by systematically mapping empirical objects to theoretical terms), and then the bridge will do precisely what the theory says it should do. No surprises.</p>
<p>That’s how they put a man on the moon and that’s how they make the computer screen you’re now looking at. It’s all done in theory before it’s done for real, and that’s what makes it possible to construct complicated but functional pieces of equipment. This is, in effect, why scientific theory makes technology possible, and therefore this is an absolutely central ingredient of the scientific enterprise which, <em>without</em> technology, would be much less impressive than it is.</p>
<p>It’s useful to take stock here, and marvel. A good scientific theory allows you infer what would happen to things in certain situations <em>without creating the situations</em>. Thus, scientific theories are crystal balls that actually work. For this reason, some philosophers of science have suggested that scientific theories should be interpreted as <em>inference tickets</em>. Once you’ve got the ticket, you get to sidestep all the tedious empirical work. Which is great, because empirical work is, well, tedious. Scientific theories are thus exquisitely suited to the needs of lazy people.</p>
<p>My field – psychology – unfortunately does not afford much of a lazy life. We don’t have theories that can offer predictions sufficiently precise to intervene in the world with appreciable certainty. That’s why there exists no such thing as a psychological engineer. And that’s why there are fields of theoretical physics, theoretical biology, and even theoretical economics, while there is no parallel field of theoretical psychology. It is a sad but, in my view, inescapable conclusion: we don’t have much in the way of scientific theory in psychology. For this reason, we have very few inference tickets – let alone inference tickets that work.</p>
<p>And that’s why psychology is so hyper-ultra-mega empirical. We never know how our interventions will pan out, because we have no theory that says how they will pan out (incidentally, that’s also why we need preregistration: in psychology, predictions are made by individual researchers rather than by standing theory, and you can’t trust people the way you can trust theory). The upshot is that, if we want to know what would happen if we did X, we have to actually do X. Because we don’t have inference tickets, we never get to take the shortcut. We always have to wade through the empirical morass. Always.</p>
<p>This has important consequences. For instance, as a field has less theory, it has to leave more to the data. Since you can’t learn anything from data without the armature of statistical analysis, a field without theory tends to grow a thriving statistical community. Thus, the role of statistics grows as soon as the presence of scientific theory wanes. In extreme cases, when statistics has entirely taken over, fields of inquiry can actually develop a kind of philosophical disorder: theoretical amnesia. In fields with this disorder, researchers no longer know what a theory is, which means that they can neither recognize its presence nor its absence. In such fields, for instance, a statistical model – like a factor model – can come to occupy the vacuum created by the absence of theory. I am often afraid that this is precisely what has happened with the advent of “theories” like those of general intelligence (a single factor model) and the so-called “Big Five” of personality (a five-factor model). In fact, I am afraid that this happened in many fields in psychology, where statistical models (which, in their barest null-hypothesis testing form, are misleadingly called “effects”) rule the day.</p>
<p>If your science thrives on experiment and statistics, but lacks the power of theory, you get peculiar problems. Most importantly, you get slow. To see why, it’s interesting to wonder how psychologists would build a bridge, if they were to use their typical methodological strategies. Probably, they would build a thousand bridges, record whether they stand or fall, and then fit a regression equation to figure out which properties are predictive of the outcome. Predictors would be chosen on the basis of statistical significance, which would introduce a multiple testing problem. In response, some of the regressors might be clustered through factor analysis, to handle the overload of predictive variables. Such analyses would probably indicate lots of structure in the data, and psychologists would likely find that the bridges’ weight, size, and elasticity loads on a single latent “strength factor”, producing the “theory” that bridges higher on the “strength factor” are less likely to fall down. Cross validation of the model would be attempted by reproducing the analysis in a new sample of a thousand bridges, to weed out chance findings. It’s likely that, after many years of empirical research, and under a great number of “context-dependent” conditions that would be poorly understood, psychologists would be able to predict a modest but significant proportion of the variance in the outcome variable. Without a doubt, it would ta  ke a thousand years to establish empirically what Newton grasped in a split second, as he wrote down his F=m*a.</p>
<p>Because increased reliance on empirical data makes you so incredibly slow, it also makes you susceptible to fads and frauds. A good theory can be tested in an infinity of ways, many of which are directly available to the interested reader (this is what give classroom demonstrations such enormous evidential force). But if your science is entirely built on generalizations derived from specifics of tediously gathered experimental data, you can’t really test these generalizations without tediously gathering the same, or highly similar, experimental data. That’s not something that people typically like to do, and it’s certainly not what journals want to print. As a result, a field can become dominated by poorly tested generalizations. When that happens, you’re in very big trouble. The reason is that your scientific field becomes susceptible to the equivalent of what evolutionary theorists call free riders: people who capitalize on the invested honest work of others by consistently taking the moral shortcut. Free riders can come to rule a scientific field if two conditions are satisfied: (a) fame is bestowed on whoever dares to make the most <em>adventurous</em> claims (rather than the most <em>defensible</em> ones), and (b) it takes longer to falsify a bogus claim than it takes to become famous. If these conditions are satisfied, you can build your scientific career on a fad and get away with it. By the time they find out your work really doesn’t survive detailed scrutiny, you’re sitting warmly by the fire in the library of your National Academy of Sciences<sub>1</sub>.</p>
<p>Much of our standard methodological teachings in psychology rest on the implicit assumption that scientific fields are similar if not identical in their methodological setup. That simply isn’t true. Without theory, the scientific ball game has to be played by different rules. I think that these new rules are now being invented: without good theory, you need fast acting replication teams, you need a reproducibility project, and you need preregistered hypotheses. Thus, the current period of crisis may lead to extremely important methodological innovations – especially those that are crucial in fields that are low on theory.</p>
<p>Nevertheless, it would be extremely healthy if psychologists received more education in fields which do have some theories, even if they are empirically shaky ones, like you often see in economics or biology. In itself, it’s no shame that we have so little theory: psychology probably has the hardest subject matter ever studied, and to change that may very well take a scientific event of the order of Newton’s discoveries. I don’t know how to do it and I don’t think anybody else knows either. But what we can do is keep in contact with other fields, and at least try to remember what theory is and what it’s good for, so that we don’t fall into theoretical amnesia. As they say, it’s the unknown unknowns that hurt you most.</p>
<p><sub>1  Caveat: I am not saying that people do this on purpose. I believe that free riders are typically unaware of the fact that they are free riders – people are very good at labeling their own actions positively, especially if the rest of the world says that they are brilliant. So, if you think this post isn’t about you, that could be entirely wrong. In fact, I cannot even be   sure that this post isn’t about me.</sub></p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/11/20/theoretical-amnesia/#disqus_thread" data-disqus-identifier="2013/11/20/theoretical-amnesia/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/11/20/theoretical-amnesia/">Posted at  2:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov 13,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/11/13/report-findings-more-transparently/" rel="bookmark" title="Permanent Link to &quot;Let’s Report Our Findings More Transparently – As We Used to Do&quot;">Let’s Report Our Findings More Transparently – As We Used to Do</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/etienne-lebel.html" rel="author">Etienne LeBel</a>

   			   <p><img src="images/etiennelebel.jpg" alt="Photo of Etienne LeBel" align="left" style="padding-right: 20px;" /></p>
<p>In 1959, Festinger and Carlsmith reported the results of an experiment that spawned a voluminous body of research on cognitive dissonance. In that experiment, all subjects performed a boring task. Some participants were paid $1 or $20 to tell the next subject the task was interesting and fun whereas participants in a control condition did no such thing. All participants then indicated how enjoyable they felt the task was, their desire to participate in another similar experiment, and the scientific importance of the experiment. The authors hypothesized that participants paid $1 to tell the next participant that the boring task they just completed was interesting would experience internal dissonance, which could be reduced by altering their attitudes on the three outcomes measures. A little known fact about the results of this experiment, however, is that only on <em>one</em> of these outcome measures did a statistically significant effect emerge across conditions. The authors reported that subjects paid $1 enjoyed the task more than those paid $20 (or the control participants), but no statistically significant differences emerged on the other two measures. </p>
<p>In another highly influential paper, Word, Zanna, and Cooper (1974) documented the self-fulfilling prophecies of racial stereotypes. The researchers had white subjects interview trained white and black applicants and coded for six non-verbal behaviors of immediacy (distance, forward lean, eye contact, shoulder orientation, interview length, and speech error rate). They found that that white interviewers treated black applicants with lower levels of non-verbal immediacy than white applicants. In a follow-up study involving white subjects only, applicants treated with less immediate non-verbal behaviors were judged to perform less well during the interview than applicants treated with more immediate non-verbal behaviors. A fascinating result, however, a little known fact about this paper is that only three of the six measures of non-verbal behaviors assessed in the first study (and subsequently used in the second study) were statistically significant. </p>
<p>What do these two examples make salient in relation to how psychologists report their results nowadays? Regular readers of prominent journals like <em>Psychological Science</em> and <em>Journal of Personality and Social Psychology</em> may see what I’m getting at: It is very rare these days to see articles in these journals wherein half or most of the reported dependent variables (DVs) fail to show statistically significant effects. Rather, one typically sees squeaky-clean looking articles where all of the DVs show statistically significant effects across all of the studies, with an occasional mention of a DV achieving “marginal significance” (Giner-Sorolla, 2012).</p>
<p>In this post, I want us to consider the possibility that psychologists’ reporting practices may have changed in the past 50 years. This then raises the question as to how this came about. One possibility is that as incentives became increasingly more perverse in psychology (Nosek,Spies, &amp; Motyl, 2012), some researchers realized that they could out-compete their peers by reporting “cleaner” looking results which would appear more compelling to editors and reviewers (Giner-Sorolla, 2012). For example, decisions were made to simply not report DVs that failed to show significant differences across conditions or that only achieved “marginal significance”. Indeed, nowadays sometimes even editors or reviewers will demand that such DVs not be reported (see <a href="http://PsychDisclosure.org">PsychDisclosure.org</a>; LeBel et al., 2013). A similar logic may also have contributed to researchers’ deciding not to fully report independent variables that failed to yield statistically significant effects and not fully reporting the exclusion of outlying participants due to fear that this information may raise doubts among the editor/reviewers and hurt their chance of getting their foot in the door (i.e., at least getting a revise-and-resubmit).</p>
<p>An alternative explanation is that new tools and technology have given us the ability to measure a greater number of DVs, which makes it more difficult to report on all them. For example, neuroscience (e.g., EEG, fMRI) and eye-tracking methods yield multitudes of analyzable data that were not previously available. Though this is undeniably true, the internet and online article supplements gives us the ability to fully report our methods and results and use the article to draw attention to the most interesting data.</p>
<p>Considering the possibility that psychologists’ reporting practices have changed in the past 50 years has implications for how to construe recent calls for the need to raise reporting standards in psychology (LeBel et al., 2013; Simmons, Nelson, &amp; Simonsohn, 2011; Simmons, Nelson, &amp; Simonsohn, 2012). Rather than seeing these calls as rigid new requirements that might interfere with exploratory research and stifle our science, one could construe such calls as a plea to revert back to the fuller reporting of results that used to be the norm in our science. From this perspective, it should not be viewed as overly onerous or authoritarian to ask researchers to disclose all excluded observations, all tested experimental conditions, all analyzed measures, and their data collection termination rule (what I’m now calling the BASIC 4 methodological categories covered by PsychDisclosure.org and Simmons et al.’s, 2012 21-word solution). It would simply be the way our forefathers used to do it.</p>
<p><strong>References</strong></p>
<p>Festinger, L. &amp; Carlsmith, J. M. (1959). <a href="http://psychclassics.yorku.ca/Festinger/">Cognitive consequences of forced compliance.</a> <em>The Journal of Abnormal and Social Psychology</em>, 58(2), Mar 1959, 203-210. doi: 10.1037/h0041593</p>
<p>Giner-Sorolla, R. (2012). Science or art? <a href="http://pps.sagepub.com/content/7/6/562.full">How esthetic standards grease the way through the publication bottleneck but undermine science.</a> <em>Perspectives on Psychological Science</em>, 7(6), 562–571. 10.1177/1745691612457576</p>
<p>LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K. R., Ratliff, K. A., &amp; Smith, C. T. (2013). <a href="http://pps.sagepub.com/content/8/4/424.full">PsychDisclosure.org: Grassroots support for reforming reporting standards in psychology.</a> Perspectives on Psychological Science, 8(4), 424-432. doi: 10.1177/1745691613491437</p>
<p>Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012).  <a href="http://pps.sagepub.com/content/7/6/615.full">Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability.</a> Perspectives on Psychological Science, 7, 615-631. doi: 10.1177/1745691612459058</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2011) <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704">False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allow Presenting Anything as Significant.</a> <em>Psychological Science</em>, 22(11), 1359-1366.</p>
<p>Simmons J., Nelson L. &amp; Simonsohn U. (2012) <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588">A 21 Word Solution</a> <em>Dialogue: The Official Newsletter of the Society for Personality and Social Psychology</em>, 26(2), 4-7.</p>
<p>Word, C. O., Zanna, M. P., &amp; Cooper, J. (1974). <a href="https://catalyst.uw.edu/workspace/file/download/72b19d8df8de321d0fed3803109a14aaf7c7b6f3800f40f477d902ab0a5e173b">The nonverbal mediation of self-fulfilling prophecies in interracial interaction.</a> <em>Journal of Experimental Social Psychology</em>, 10(2), 109–120. doi: 10.1016/0022-1031(74)90059-6</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/11/13/report-findings-more-transparently/#disqus_thread" data-disqus-identifier="2013/11/13/report-findings-more-transparently/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/11/13/report-findings-more-transparently/">Posted at 12:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <h4 class="date">Nov  3,  2013</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2013/11/03/Increasing-statistical-power/" rel="bookmark" title="Permanent Link to &quot;Increasing statistical power in psychological research without increasing sample size&quot;">Increasing statistical power in psychological research without increasing sample size</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/sean-mackinnon.html" rel="author">Sean Mackinnon</a>

   			   <p><strong>What is statistical power and precision?</strong></p>
<p>This post is going to give you some practical tips to increase statistical power in your research. Before going there though, let’s make sure everyone is on the same page by starting with some definitions. </p>
<p>Statistical power is the probability that the test will reject the null hypothesis when the null hypothesis
is false. Many authors suggest a statistical power rate of at least .80, which corresponds to an 80% probability of <em>not</em> committing a<a href="http://www.investopedia.com/terms/t/type-ii-error.asp"> Type II error</a>.</p>
<p>Precision refers to the width of the<a href="http://www.psychologicalscience.org/index.php/publications/observer/2010/april-10/understanding-confidence-intervals-cis-and-effect-size-estimation.html"> confidence interval</a> for an <a href="http://www.leeds.ac.uk/educol/documents/00002182.htm">effect size</a>. The smaller this width, the more precise your results are. For 80% power, the confidence interval width will be roughly plus or minus 70% of the population effect size (<a href="http://www.ncbi.nlm.nih.gov/pubmed/8017747">Goodman &amp; Berlin, 1994</a>). Studies that have low precision have a greater probability of both<a href="http://www.investopedia.com/terms/t/type_1_error.asp"> Type I</a> and<a href="http://www.investopedia.com/terms/t/type-ii-error.asp"> Type II</a> errors (<a href="http://dx.doi.org/10.1038/nrn3475">Button et al., 2013</a>).</p>
<p>To get an idea of how this works, here are a few examples of the sample size required to achieve .80 power for small, medium, and large (<a href="http://www2.psych.ubc.ca/~schaller/349and449Readings/Cohen1992.pdf">Cohen, 1992</a>) correlations as well as the expected confidence intervals</p>
<table>
  <tr>
    <td>Population Effect Size</td>
    <td>Sample Size for 80% Power</td>
    <td>Estimated Precision</td>
  </tr>
  <tr>
    <td>r = .10</td>
    <td>782</td>
    <td>95% CI [.03, .17]</td>
  </tr>
  <tr>
    <td>r = .30</td>
    <td>84</td>
    <td>95% CI [.09, .51]</td>
  </tr>
  <tr>
    <td>r = .50</td>
    <td>29</td>
    <td>95% CI [.15, .85]</td>
  </tr>
</table>

<p><strong>Studies in psychology are grossly underpowered</strong></p>
<p>Okay, so now you know what power is. But why should you care? Fifty years ago,<a href="http://dx.doi.org/10.1037/h0045186"> Cohen (1962)</a> estimated the statistical power to detect a medium effect size in abnormal psychology was about .48. That’s a false negative rate of 52%, which is no better than a coin-flip! The situation has improved slightly, but it’s still a serious problem today. For instance, one review suggested only 52% of articles in the applied psychology literature achieved .80 power for a medium effect size (<a href="http://dx.doi.org/10.1111/j.1744-6570.1996.tb01793.x">Mone et al., 1996</a>). This is in part because psychologists are studying small effects. One massive review of 322 meta-analyses including 8 million participants (<a href="http://dx.doi.org/10.1037/1089-2680.7.4.331">Richard et al., 2003</a>) suggested that the average effect size in social psychology is relatively small (<em>r</em> = .21). To put this into perspective, you’d need 175 participants to have .80 power for a simple correlation between two variables at this effect size. This gets even worse when we’re studying interaction effects. One review suggests that the average effect size for interaction effects is even smaller (f2 = .009), which means that sample sizes of around 875 people would be needed to achieve .80 power (<a href="http://dx.doi.org/10.1037/0021-9010.90.1.94">Aguinis et al., 2005</a>). Odds are, if you took the time to design a research study and collect data, you want to find a relationship if one really exists. You don’t want to "miss" something that is really there. More than this, you probably want to have a reasonably precise estimate of the effect size (it’s not that impressive to just say a relationship is positive and probably non-zero). Below, I discuss concrete strategies for improving power and precision.</p>
<p><strong>What can we do to increase power?</strong></p>
<p>It is well-known that increasing sample size increases statistical power and precision. Increasing the population effect size increases statistical power, but has no effect on precision (<a href="http://dx.doi.org/10.1146/annurev.psych.59.103006.093735">Maxwell et al., 2008</a>). Increasing sample size improves power and precision by reducing <a href="http://www.investopedia.com/terms/s/standard-error.asp">standard error</a> of the effect size. Take a look at this formula for the confidence interval of a linear regression coefficient (<a href="http://dx.doi.org/10.1037/1082-989X.2.1.3">McClelland, 2000</a>):</p>
<p><img src="images/power-equation.png" alt="Power Equation"> </p>
<p>MSE is the mean square error, n is the sample size, Vx is the variance of X, and (1-<em>R</em>2) is the proportion of the variance in X not shared by any other variables in the model. Okay, hopefully you didn’t nod off there. There’s a reason I’m showing you this formula. In this formula, decreasing any value in the numerator (MSE) or increasing anything in the denominator (n, Vx, 1-<em>R</em>2) will decrease the standard error of the effect size, and will thus increase power and precision. This formula demonstrates that there are at least three other ways to increase statistical power aside from sample size: (a) Decreasing the mean square error; (b) increasing the variance of x; and (c) increasing the proportion of the variance in X not shared by any other predictors in the model. Below, I’ll give you a few ways to do just that. </p>
<p><strong>Recommendation 1: Decrease the mean square error</strong></p>
<p>Referring to the formula above, you can see that decreasing the mean square error will have about the same impact as increasing sample size. Okay. You’ve probably heard the term "<a href="http://stats.oecd.org/glossary/detail.asp?ID=3716">mean square error</a>" before, but the definition might be kind of fuzzy. Basically, your model makes a prediction for what the outcome variable (Y) should be, given certain values of the predictor (X). Naturally, it’s not a perfect prediction because you have measurement error, and because there are other important variables you probably didn’t measure. The mean square error is the difference between what your model predicts, and what the true values of the data actually are.  So, anything that improves the quality of your measurement or accounts for potential confounding variables will reduce the mean square error, and thus improve statistical power. Let’s make this concrete. Here are three specific techniques you can use:</p>
<p>a)      <em>Reduce measurement error by using more reliable measures</em>(i.e., better internal consistency, test-retest reliability, inter-rater reliability, etc.). You’ve probably read that .70 is the "rule-of-thumb" for acceptable reliability. Okay, sure. That’s publishable. But consider this: Let’s say you want to test a correlation coefficient. Assuming both measures have a reliability of .70, your observed correlation will be about 1.43 times <em>smaller</em> than the true population parameter (I got this using<a href="http://jeromyanglim.blogspot.ca/2009/09/adjusting-correlations-for-reliability.html"> Spearman’s correlation attenuation formula</a>).  Because you have a smaller observed effect size, you end up with less statistical power. Why do this to yourself? Reduce measurement error. If you’re an experimentalist, make sure you execute your experimental manipulations exactly the same way each time, preferably by automating them. Slight variations in the manipulation (e.g., different locations, slight variations in timing) might reduce the reliability of the manipulation, and thus reduce power. </p>
<p>b)      <em>Control for confounding variables.</em> With correlational research, this means including control variables that predict the outcome variable, but are relatively uncorrelated with other predictor variables. In experimental designs, this means taking great care to control for as many possible confounds as possible. In both cases, this reduces the mean square error and improves the overall predictive power of the model – and thus, improves statistical power. Be careful when adding control variables into your models though: There are diminishing returns for adding covariates. Adding a couple of good covariates is bound to improve your model, but you always have to balance predictive power against model complexity. Adding a large number of predictors can sometimes lead to overfitting (i.e., the model is just describing noise or random error) when there are too many predictors in the model relative to the sample size. So, controlling for a couple of good covariates is generally a good idea, but too many covariates will probably make your model worse, not better, especially if the sample is small. </p>
<p>c)      <em>Use repeated-measures designs.</em> Repeated measures designs are where participants are measured multiple times  (e.g., once a day surveys, multiple trials in an experiment, etc.). Repeated measures designs reduce the mean square error by partitioning out the variance due to individual participants. Depending on the kind of analysis you do, it can also increase the degrees of freedom for the analysis substantially. For example, you might only have 100 participants, but if you measured them once a day for 21 days, you’ll actually have 2100 data points to analyze. The data analysis can get tricky and the interpretation of the data may change, but many multilevel and structural equation models can take advantage of these designs by examining each <em>measurement occasion </em>(i.e., each day, each trial, etc.) as the unit of interest, instead of each individual participant. Increasing the degrees of freedom is much like increasing the sample size in terms of increasing statistical power.  I’m a big fan of repeated measures designs, because they allow researchers to collect a lot of data from fewer participants.</p>
<p><strong>Recommendation 2: Increase the variance of your predictor variable</strong></p>
<p>Another less-known way to increase statistical power and precision is to increase the variance of your predictor variables (X). The formula listed above shows that doubling the variance of X is has the same impact on increasing statistical precision as doubling the sample size does! So it’s worth figuring this out. </p>
<p>a)      <em>In correlational research, use more comprehensive continuous measures. </em>That is, there should be a large possible range of values endorsed by participants. However, the measure should also capture many different aspects of the construct of interest; artificially increasing the range of X by adding redundant items (i.e., simply re-phrasing existing items to ask the same question) will actually hurt the validity of the analysis. Also, avoid dichotomizing your measures (e.g., median splits), because this reduces the variance and typically reduces power (<a href="http://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf">MacCallum et al., 2002</a>).</p>
<p>b)      <em>In experimental research, unequally allocating participants to each condition can improve statistical power</em>. For example, if you were designing an experiment with 3 conditions (let’s say low, medium, or high self-esteem). Most of us would equally assign participants to all three groups, right? Well, as it turns out, assigning participants equally across groups usually reduces statistical power. The idea behind assigning participants unequally to conditions is to maximize the variance of X for the particular kind of relationship under study -- which, according the formula I gave earlier, will increase power and precision. For example, the optimal design for a linear relationship would be 50% low, 50% high, and omit the medium condition. The optimal design for a quadratic relationship would be 25% low, 50% medium, and 25% high. The proportions vary widely depending on the design and the kind of relationship you expect, but I recommend you check out<a href="http://dx.doi.org/10.1037/1082-989X.2.1.3"> McClelland (1997)</a> to get more information on efficient experimental designs. You might be surprised.</p>
<p><strong>Recommendation 3: Make sure predictor variables are uncorrelated with each other</strong></p>
<p>A final way to increase statistical power is to <em>increase the proportion of the variance in X not shared with other variables in the model</em>. When predictor variables are correlated with each other, this is known as colinearity. For example, depression and anxiety are positively correlated with each other; including both as simultaneous predictors (say, in multiple regression) means that statistical power will be reduced, especially if one of the two variables actually doesn’t predict the outcome variable. Lots of textbooks suggest that we should only be worried about this when colinearity is extremely high (e.g., correlations around &gt; .70). However, studies have shown that even modest intercorrlations among predictor variables will reduce statistical power (<a href="http://dx.doi.org/10.2307/3172863">Mason et al., 1991</a>). Bottom line: If you can design a model where your predictor variables are relatively uncorrelated with each other, you can improve statistical power.</p>
<p><strong>Conclusion</strong></p>
<p>Increasing statistical power is one of the rare times where what is good for science, and what is good for your career actually coincides. It increases the accuracy and replicability of results, so it’s good for science. It also increases your likelihood of finding a statistically significant result (assuming the effect actually exists), making it more likely to get something published. You don’t need to torture your data with obsessive re-analysis until you get <em>p</em> &lt; .05.  Instead, put more thought into research design in order to maximize statistical power. Everyone wins, and you can use that time you used to spend sweating over p-values to do something more productive. Like volunteering with the<a href="http://openscienceframework.org/"> Open Science Collaboration</a>.</p>
<p><strong>References</strong></p>
<p>Aguinis, H., Beaty, J. C., Boik, R. J., &amp; Pierce, C. A. (2005). Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review. <em>Journal of Applied Psychology, 90,</em> 94-107. doi:10.1037/0021-9010.90.1.94</p>
<p>Button, K. S., Ioannidis, J. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. J., &amp; Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365-376. doi: 10.1038/nrn3475</p>
<p>Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. <em>The Journal of Abnormal and Social Psychology, 65,</em> 145-153. doi:10.1037/h0045186</p>
<p>Cohen, J. (1992). A power primer. <em>Psychological Bulletin, 112,</em> 155-159. doi:10.1037/0033-2909.112.1.155</p>
<p>Goodman, S. N., &amp; Berlin, J. A. (1994). The use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. <em>Annals of Internal Medicine, 121, </em>200-206. </p>
<p>Hansen, W. B., &amp; Collins, L. M. (1994). Seven ways to increase power without increasing N. In L. M. Collins &amp; L. A. Seitz (Eds.), <em>Advances in data analysis for prevention intervention research</em> (NIDA Research Monograph 142, NIH Publication No. 94-3599, pp. 184–195). Rockville, MD: National Institutes of Health.</p>
<p>MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. <em>Psychological Methods, 7,</em> 19-40. doi:10.1037/1082-989X.7.1.19</p>
<p>Mason, C. H., &amp; Perreault, W. D. (1991). Collinearity, power, and interpretation of multiple regression analysis. <em>Journal of Marketing Research, 28,</em> 268-280. doi:10.2307/3172863</p>
<p>Maxwell, S. E., Kelley, K., &amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. <em>Annual Review of Psychology, 59,</em> 537-563. doi:10.1146/annurev.psych.59.103006.093735</p>
<p>McClelland, G. H. (1997). Optimal design in psychological research. <em>Psychological Methods, 2,</em> 3-19. doi:10.1037/1082-989X.2.1.3</p>
<p>McClelland, G. H. (2000). Increasing statistical power without increasing sample size. <em>American Psychologist, 55, </em>963-964. doi:10.1037/0003-066X.55.8.963</p>
<p>Mone, M. A., Mueller, G. C., &amp; Mauland, W. (1996). The perceptions and usage of statistical power in applied psychology and management research. <em>Personnel Psychology, 49,</em> 103-120. doi:10.1111/j.1744-6570.1996.tb01793.x</p>
<p>Open Science Collaboration. (in press). The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility. In V. Stodden, F. Leisch, &amp; R. Peng (Eds.), <em>Implementing Reproducible Computational Research (A Volume in The R Series)</em>.  New York, NY: Taylor &amp; Francis. doi:10.2139/ssrn.2195999</p>
<p>Richard, F. D., Bond, C. r., &amp; Stokes-Zoota, J. J. (2003). One Hundred Years of Social Psychology Quantitatively Described. <em>Review of General Psychology, 7,</em> 331-363. doi:10.1037/1089-2680.7.4.331</p>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2013/11/03/Increasing-statistical-power/#disqus_thread" data-disqus-identifier="2013/11/03/Increasing-statistical-power/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2013/11/03/Increasing-statistical-power/">Posted at 12:00 am</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://centerforopenscience.github.io/osc/index3.html" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="http://centerforopenscience.github.io/osc/index5.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 4 of 5</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>