<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
        <title>A Pelican Blog &middot; Articles by Ryne Sherman</title>
<!--        <link rel="shortcut icon" href="http://centerforopenscience.github.io/osc/favicon.ico" /> -->
		<link rel="shortcut icon" href="http://mcohn.net/rp/favicon.ico" />
<link href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://centerforopenscience.github.io/osc/theme/css/pygments.css" type="text/css" />


    </head>
    <body>
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://centerforopenscience.github.io/osc/author/ryne-sherman.html">Ryne Sherman</a></li>
                <li><a href="http://centerforopenscience.github.io/osc">Home</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/about.html">About</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/authors.html">Authors</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/pages/policy.html">Policy</a></li>
                <li><a href="http://centerforopenscience.github.io/osc/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://centerforopenscience.github.io/osc"><img src="http://centerforopenscience.github.io/osc/images/osc-redblack.png" width="160px" style="margin-top:-60px;" /></a></h1>
            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Jul  2,  2014</h4>
                <div class="post">
<h2 class="title">
                        <a href="http://centerforopenscience.github.io/osc/2014/07/02/phack/" rel="bookmark" title="Permanent Link to &quot;phack - An R Function for Examining the Effects of P-hacking&quot;">phack - An R Function for Examining the Effects of P-hacking</a>
                    </h2>

                    by <a href="http://centerforopenscience.github.io/osc/author/ryne-sherman.html" rel="author">Ryne Sherman</a>

   			   <p><em>This article was <a href="http://rynesherman.com/blog/phack-an-r-function-for-examining-the-effects-of-p-hacking/">originally posted</a> in the author's personal blog.</em></p>
<p>Imagine you have a two group between-S study with N=30 in each group. You compute a two-sample t-test and the result is p = .09, not statistically significant with an effect size r = .17. Unbeknownst to you there is really no relationship between the IV and the DV. But, because you believe there is a relationship (you decided to run the study after all!), you think maybe adding five more subjects to each condition will help clarify things. So now you have N=35 in each group and you compute your t-test again. Now p = .04 with r = .21.</p>
<p>If you are reading this blog you might recognize what happened here as an instance of p-hacking. This particular form (testing periodically as you increase N) of p-hacking was one of the many data analytic flexibility issues exposed by <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704">Simmons, Nelson, and Simonshon (2011)</a>. But what are the real consequences of p-hacking? How often will p-hacking turn a null result into a positive result? What is the impact of p-hacking on effect size?</p>
<p>These were the kinds of questions that I had. So I wrote a little R function that simulates this type of p-hacking. The function – called phack – is designed to be flexible, although right now it only works for two-group between-S designs. The user is allowed to input and manipulate the following factors (argument name in parentheses):</p>
<ul>
<li>Initial Sample Size (initialN): The initial sample size (for each group) one had in mind when beginning the study (default = 30).</li>
<li>Hack Rate (hackrate): The number of subjects to add to each group if the p-value is not statistically significant before testing again (default = 5).</li>
<li>Population Means (grp1M, grp2M): The population means (Mu) for each group (default 0 for both).</li>
<li>Population SDs (grp1SD, grp2SD): The population standard deviations (Sigmas) for each group (default = 1 for both).</li>
<li>Maximum Sample Size (maxN): You weren’t really going to run the study forever right? This is the sample size (for each group) at which you will give up the endeavor and go run another study (default = 200).</li>
<li>Type I Error Rate (alpha): The value (or lower) at which you will declare a result statistically significant (default = .05).</li>
<li>Hypothesis Direction (alternative): Did your study have a directional hypothesis? Two-group studies often do (i.e., this group will have a higher mean than that group). You can choose from “greater” (Group 1 mean is higher), “less” (Group 2 mean is higher), or “two.sided” (any difference at all will work for me, thank you very much!). The default is “greater.”</li>
<li>Display p-curve graph (graph)?: The function will output a figure displaying the p-curve for the results based on the initial study and the results for just those studies that (eventually) reached statistical significance (default = TRUE). More on this below.</li>
<li>How many simulations do you want (sims). The number of times you want to simulate your p-hacking experiment.</li>
</ul>
<p>To make this concrete, consider the following R code:</p>
<div class="highlight"><pre>res <span class="o">&lt;-</span> phack<span class="p">(</span>initialN<span class="o">=</span><span class="m">30</span><span class="p">,</span> hackrate<span class="o">=</span><span class="m">5</span><span class="p">,</span> grp1M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp2M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp1SD<span class="o">=</span><span class="m">1</span><span class="p">,</span> 
  grp2SD<span class="o">=</span><span class="m">1</span><span class="p">,</span> maxN<span class="o">=</span><span class="m">200</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">.05</span><span class="p">,</span> alternative<span class="o">=</span><span class="s">&quot;greater&quot;</span><span class="p">,</span> graph<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span> sims<span class="o">=</span><span class="m">1000</span><span class="p">)</span>
</pre></div>


<p>This says you have planned a two-group study with N=30 (initialN=30) in each group. You are going to compute your t-test on that initial sample. If that is not statistically significant you are going to add 5 more (hackrate=5) to each group and repeat that process until it is statistically significant or you reach 200 subjects in each group (maxN=200). You have set the population Ms to both be 0 (grp1M=0; grp2M=0) with SDs of 1 (grp1SD=1; grp2SD=1). You have set your nominal alpha level to .05 (alpha=.05), specified a direction hypothesis where group 1 should be higher than group 2 (alternative=“greater”), and asked for graphical output (graph=TRUE). Finally, you have requested to run this simulation 1000 times (sims=1000).</p>
<p>So what happens if we run this experiment?<sup>1</sup> So we can get the same thing, I have set the random seed in the code below.</p>
<div class="highlight"><pre>source<span class="p">(</span><span class="s">&quot;http://rynesherman.com/phack.r&quot;</span><span class="p">)</span> <span class="c1"># read in the p-hack function</span>
set.seed<span class="p">(</span><span class="m">3</span><span class="p">)</span>
res <span class="o">&lt;-</span> phack<span class="p">(</span>initialN<span class="o">=</span><span class="m">30</span><span class="p">,</span> hackrate<span class="o">=</span><span class="m">5</span><span class="p">,</span> grp1M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp2M<span class="o">=</span><span class="m">0</span><span class="p">,</span> grp1SD<span class="o">=</span><span class="m">1</span><span class="p">,</span> grp2SD<span class="o">=</span><span class="m">1</span><span class="p">,</span>
   maxN<span class="o">=</span><span class="m">200</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">.05</span><span class="p">,</span> alternative<span class="o">=</span><span class="s">&quot;greater&quot;</span><span class="p">,</span> graph<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span> sims<span class="o">=</span><span class="m">1000</span><span class="p">)</span>
</pre></div>


<p>The following output appears in R:</p>
<div class="highlight"><pre><span class="n">Proportion</span> <span class="n">of</span> <span class="n">Original</span> <span class="n">Samples</span> <span class="n">Statistically</span> <span class="n">Significant</span> <span class="o">=</span> <span class="mf">0.054</span>
<span class="n">Proportion</span> <span class="n">of</span> <span class="n">Samples</span> <span class="n">Statistically</span> <span class="n">Significant</span> <span class="n">After</span> <span class="n">Hacking</span> <span class="o">=</span> <span class="mf">0.196</span>
<span class="n">Probability</span> <span class="n">of</span> <span class="n">Stopping</span> <span class="n">Before</span> <span class="n">Reaching</span> <span class="n">Significance</span> <span class="o">=</span> <span class="mf">0.805</span>
<span class="n">Average</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">Hacks</span> <span class="n">Before</span> <span class="n">Significant</span><span class="o">/</span><span class="n">Stopping</span> <span class="o">=</span> <span class="mf">28.871</span>
<span class="n">Average</span> <span class="n">N</span> <span class="n">Added</span> <span class="n">Before</span> <span class="n">Significant</span><span class="o">/</span><span class="n">Stopping</span> <span class="o">=</span> <span class="mf">144.355</span>
<span class="n">Average</span> <span class="n">Total</span> <span class="n">N</span> <span class="mf">174.355</span>
<span class="n">Estimated</span> <span class="n">r</span> <span class="n">without</span> <span class="n">hacking</span> <span class="mi">0</span>
<span class="n">Estimated</span> <span class="n">r</span> <span class="n">with</span> <span class="n">hacking</span> <span class="mf">0.03</span>
<span class="n">Estimated</span> <span class="n">r</span> <span class="n">with</span> <span class="n">hacking</span> <span class="mf">0.19</span> <span class="p">(</span><span class="n">non</span><span class="o">-</span><span class="n">significant</span> <span class="n">results</span> <span class="n">not</span> <span class="n">included</span><span class="p">)</span>
</pre></div>


<p>The first line tells us how many (out of the 1000 simulations) of the originally planned (N=30 in each group) studies had a p-value that was .05 or less. Because there was no true effect (grp1M = grp2M) this at just about the nominal rate of .05. But what if we had used our p-hacking scheme (testing every 5 subjects per condition until significant or N=200)? That result is in the next line. It shows that just about 20% of the time we would have gotten a statistically significant result. So this type of hacking has inflated our Type I error rate from 5% to 20%. How often would we have given up (i.e., N=200) before reaching statistical significance? That is about 80% of the time. We also averaged 28.87 “hacks” before reaching significance/stopping, averaged having to add N=144 (per condition) before significance/stopping, and had an average total N of 174 (per condition) before significance/stopping.</p>
<p>What about effect sizes? Naturally the estimated effect size (r) was .00 if we just used our original N=30 in each group design. If we include the results of all 1000 completed simulations that effect size averages out to be r = .03. Most importantly, if we exclude those studies that never reached statistical significance, our average effect size r = .19.</p>
<p>This is pretty telling. But there is more. We also get this nice picture:</p>
<p><img src="images/phack.png" alt="Phack" /></p>
<p>It shows the distribution of the p-values below .05 for the initial study (upper panel) and for those p-values below .05 for those reaching statistical significance. The p-curves (see <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2256237">Simonsohn, Nelson, &amp; Simmons, 2013</a>) are also drawn on. If there is really no effect, we should see a flat p-curve (as we do in the upper panel). And if there is no effect and p-hacking has occurred, we should see a p-curve that slopes up towards the critical value (as we do in the lower panel).</p>
<p>Finally, the function provides us with more detailed output that is summarized above. We can get a glimpse of this by running the following code:</p>
<div class="highlight"><pre><span class="n">head</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>


<p>This generates the following output:</p>
<div class="highlight"><pre><span class="n">Initial</span><span class="p">.</span><span class="n">p</span>  <span class="n">Hackcount</span>     <span class="n">Final</span><span class="p">.</span><span class="n">p</span>  <span class="n">NAdded</span>    <span class="n">Initial</span><span class="p">.</span><span class="n">r</span>       <span class="n">Final</span><span class="p">.</span><span class="n">r</span>
<span class="mf">0.86410908</span>         <span class="mi">34</span>  <span class="mf">0.45176972</span>     <span class="mi">170</span>  <span class="o">-</span><span class="mf">0.14422580</span>   <span class="mf">0.006078565</span>
<span class="mf">0.28870264</span>         <span class="mi">34</span>  <span class="mf">0.56397332</span>     <span class="mi">170</span>   <span class="mf">0.07339944</span>  <span class="o">-</span><span class="mf">0.008077691</span>
<span class="mf">0.69915219</span>         <span class="mi">27</span>  <span class="mf">0.04164525</span>     <span class="mi">135</span>  <span class="o">-</span><span class="mf">0.06878039</span>   <span class="mf">0.095492249</span>
<span class="mf">0.84974744</span>         <span class="mi">34</span>  <span class="mf">0.30702946</span>     <span class="mi">170</span>  <span class="o">-</span><span class="mf">0.13594941</span>   <span class="mf">0.025289555</span>
<span class="mf">0.28048754</span>         <span class="mi">34</span>  <span class="mf">0.87849707</span>     <span class="mi">170</span>   <span class="mf">0.07656582</span>  <span class="o">-</span><span class="mf">0.058508736</span>
<span class="mf">0.07712726</span>         <span class="mi">34</span>  <span class="mf">0.58909693</span>     <span class="mi">170</span>   <span class="mf">0.18669338</span>  <span class="o">-</span><span class="mf">0.011296131</span>
</pre></div>


<p>The object res contains the key results from each simulation including the p-value for the initial study (Initial.p), the number of times we had to hack (Hackcount), the p-value for the last study run (Final.p), the total N added to each condition (NAdded), the effect size r for the initial study (Initial.r), and the effect size r for the last study run (Final.r).</p>
<p>So what can we do with this? I see lots of possibilities and quite frankly I don’t have the time or energy to do them. Here are some quick ideas:</p>
<ul>
<li>What would happen if there were a true effect?</li>
<li>What would happen if there were a true (but small) effect?</li>
<li>What would happen if we checked for significance after each subject (hackrate=1)?</li>
<li>What would happen if the maxN were lower?</li>
<li>What would happen if the initial sample size was larger/smaller?</li>
<li>What happens if we set the alpha = .10?</li>
<li>What happens if we try various combinations of these things?</li>
</ul>
<p>I’ll admit I have tried out a few of these ideas myself, but I haven’t really done anything systematic. I just thought other people might find this function interesting and fun to play with.</p>
<p><sup>1</sup> By the way, all of these arguments are set to their default, so you can do the same thing by simply running:</p>
<div class="highlight"><pre>res <span class="o">&lt;-</span> phack<span class="p">()</span>
</pre></div>
                    <div class="clear"></div>
                    <div class="info">
<a href="http://centerforopenscience.github.io/osc/2014/07/02/phack/#disqus_thread" data-disqus-identifier="2014/07/02/phack/" style="text-decoration:underline; color: black">Discuss this post</a> | <a href="http://centerforopenscience.github.io/osc/2014/07/02/phack/">Posted at  3:00 pm</a>&nbsp;&middot;&nbsp;<a href="http://centerforopenscience.github.io/osc/category/content.html" rel="tag">content</a>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    Mockingbird theme by <a href="http://nevanscott.com/">Nevan Scott</a>
                    &middot;
                    <a class="atom" href="http://centerforopenscience.github.io/osc/feeds/all.atom.xml">Feed</a>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44517264-1', 'centerforopenscience.org');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'opensciencecollaboration'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>


    </body>
</html>